{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd362576",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IS_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba132b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    if os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"Repository already exists. Pulling latest changes...\")\n",
    "        %cd /content/aai521_3proj\n",
    "        !git pull\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/swapnilprakashpatil/aai521_3proj.git\n",
    "        %cd aai521_3proj    \n",
    "    %pip install -r requirements.txt\n",
    "    sys.path.append('/content/aai521_3proj/src')\n",
    "    %ls\n",
    "else:\n",
    "    print(\"Running in local environment. Installing packages...\")\n",
    "    %pip install -r ../requirements.txt\n",
    "    sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Google Colab Environment Specifications:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get system info\n",
    "    \n",
    "    print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Architecture: {platform.machine()}\")\n",
    "    print(f\"Python Version: {platform.python_version()}\")\n",
    "    \n",
    "    # Memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "    \n",
    "    # GPU info\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            gpu_info = result.stdout.strip().split('\\n')\n",
    "            for i, gpu in enumerate(gpu_info):\n",
    "                name, memory = gpu.split(', ')\n",
    "                print(f\"GPU {i}: {name}, {memory} MB VRAM\")\n",
    "        else:\n",
    "            print(\"GPU: Not detected or nvidia-smi unavailable\")\n",
    "    except:\n",
    "        print(\"GPU: Not detected\")\n",
    "    \n",
    "    # Disk space\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Space: {disk.free / (1024**3):.1f} GB free / {disk.total / (1024**3):.1f} GB total\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if not os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"WARNING: Cloning project repository required.\")\n",
    "        print(\"=\"*50)\n",
    "else:\n",
    "    print(\"Not running in Google Colab environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a7f68",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67309af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "if 'dataset' in sys.modules:\n",
    "    importlib.reload(sys.modules['dataset'])\n",
    "if 'models' in sys.modules:\n",
    "    importlib.reload(sys.modules['models'])\n",
    "if 'config' in sys.modules:\n",
    "    importlib.reload(sys.modules['config'])\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "from dataset import create_dataloaders, FloodDataset\n",
    "from models import create_model, UNetPlusPlus, DeepLabV3Plus, SegFormer\n",
    "from losses import create_loss_function\n",
    "from metrics import MetricsTracker, SegmentationMetrics\n",
    "from trainer import Trainer\n",
    "from experiment_tracking import ExperimentLogger, ExperimentComparator\n",
    "from gpu_manager import GPUManager\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Initialize GPU manager\n",
    "gpu_mgr = GPUManager()\n",
    "gpu_mgr.setup()\n",
    "gpu_mgr.print_nvidia_smi_info()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {gpu_mgr.is_available()}\")\n",
    "if gpu_mgr.is_available():\n",
    "    print(f\"CUDA device: {gpu_mgr.gpu_name}\")\n",
    "    print(f\"CUDA memory: {gpu_mgr.total_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0cc01",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1dbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print GPU information\n",
    "gpu_mgr.print_info()\n",
    "gpu_mgr.print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f1196",
   "metadata": {},
   "source": [
    "### Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb60715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory first\n",
    "gpu_mgr.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe73b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_dir=config.PROCESSED_TRAIN_DIR,\n",
    "    val_dir=config.PROCESSED_VAL_DIR,\n",
    "    test_dir=config.PROCESSED_TEST_DIR,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Calculate class weights from training data\n",
    "print(\"\\nCalculating class weights from training data...\")\n",
    "class_counts = torch.zeros(config.NUM_CLASSES)\n",
    "\n",
    "for batch in tqdm(train_loader, desc=\"Computing class distribution\"):\n",
    "    masks = batch['mask']\n",
    "    for cls in range(config.NUM_CLASSES):\n",
    "        class_counts[cls] += (masks == cls).sum()\n",
    "\n",
    "# Compute weights (inverse frequency)\n",
    "total_pixels = class_counts.sum()\n",
    "class_weights = total_pixels / (config.NUM_CLASSES * class_counts)\n",
    "class_weights = class_weights / class_weights.sum() * config.NUM_CLASSES  # Normalize\n",
    "\n",
    "print(\"\\nClass distribution and weights:\")\n",
    "for cls, (name, count, weight) in enumerate(zip(config.CLASS_NAMES, class_counts, class_weights)):\n",
    "    pct = (count / total_pixels * 100).item()\n",
    "    print(f\"  {name}: {count:.0f} pixels ({pct:.2f}%), weight: {weight:.4f}\")\n",
    "\n",
    "print(f\"\\nDataloaders created:\")\n",
    "print(f\"  Train: {len(train_loader)} batches ({len(train_loader.dataset)} samples)\")\n",
    "print(f\"  Val: {len(val_loader)} batches ({len(val_loader.dataset)} samples)\")\n",
    "print(f\"  Test: {len(test_loader)} batches ({len(test_loader.dataset)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc0685",
   "metadata": {},
   "source": [
    "### Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "images = batch['image']\n",
    "masks = batch['mask']\n",
    "\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Mask shape: {masks.shape}\")\n",
    "print(f\"Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Mask classes: {masks.unique().tolist()}\")\n",
    "\n",
    "# Visualize samples\n",
    "def visualize_samples(images, masks, num_samples=3):\n",
    "    \"\"\"Visualize pre/post images and masks.\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    \n",
    "    # Color map for masks\n",
    "    cmap = plt.cm.get_cmap('tab10', len(config.CLASS_NAMES))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Pre-event image (first 3 channels)\n",
    "        pre_img = images[i, :3].permute(1, 2, 0).numpy()\n",
    "        pre_img = (pre_img - pre_img.min()) / (pre_img.max() - pre_img.min() + 1e-8)\n",
    "        \n",
    "        # Post-event image (last 3 channels)\n",
    "        post_img = images[i, 3:].permute(1, 2, 0).numpy()\n",
    "        post_img = (post_img - post_img.min()) / (post_img.max() - post_img.min() + 1e-8)\n",
    "        \n",
    "        # Mask\n",
    "        mask = masks[i].numpy()\n",
    "        \n",
    "        # Plot pre-event\n",
    "        axes[i, 0].imshow(pre_img)\n",
    "        axes[i, 0].set_title('Pre-Event Image', fontsize=12, fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot post-event\n",
    "        axes[i, 1].imshow(post_img)\n",
    "        axes[i, 1].set_title('Post-Event Image', fontsize=12, fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Plot mask\n",
    "        mask_plot = axes[i, 2].imshow(mask, cmap=cmap, vmin=0, vmax=len(config.CLASS_NAMES)-1)\n",
    "        axes[i, 2].set_title('Ground Truth Mask', fontsize=12, fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Add colorbar to last mask\n",
    "        if i == num_samples - 1:\n",
    "            cbar = plt.colorbar(mask_plot, ax=axes[i, 2], orientation='horizontal', \n",
    "                              pad=0.05, fraction=0.046)\n",
    "            cbar.set_ticks(range(len(config.CLASS_NAMES)))\n",
    "            cbar.set_ticklabels(config.CLASS_NAMES, rotation=45, ha='right', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(images, masks, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6f041",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for architecture overview\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# List of all models to train\n",
    "ALL_MODELS = ['unet++', 'deeplabv3+', 'segformer', 'fc_siam_diff', 'siamese_unet++', 'stanet']\n",
    "\n",
    "models_info = []\n",
    "\n",
    "for model_name in ALL_MODELS:\n",
    "    model = create_model(\n",
    "        model_name=model_name,\n",
    "        in_channels=6 if 'siamese' not in model_name.lower() else 3,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        **config.MODEL_CONFIGS.get(model_name, {})\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    models_info.append({\n",
    "        'Model': model_name.upper(),\n",
    "        'Total Parameters': f\"{total_params:,}\",\n",
    "        'Trainable Parameters': f\"{trainable_params:,}\",\n",
    "        'Size (MB)': f\"{total_params * 4 / 1e6:.2f}\"\n",
    "    })\n",
    "    \n",
    "    del model\n",
    "\n",
    "# Display as table\n",
    "models_df = pd.DataFrame(models_info)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(models_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15267a92",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bb952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device using GPU manager\n",
    "device = gpu_mgr.get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training configurations\n",
    "LIGHT_CONFIG = {\n",
    "    'num_epochs': 3,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'device': device,\n",
    "    'use_amp': True,\n",
    "    'gradient_clip': 1.0,\n",
    "    'max_batches_per_epoch': 50,\n",
    "    'loss_type': 'combined',\n",
    "    'early_stopping_patience': 5,\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'device': device,\n",
    "    'use_amp': True,\n",
    "    'gradient_clip': 1.0,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'loss_type': 'combined',\n",
    "    'early_stopping_patience': 5,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Light validation: {LIGHT_CONFIG['num_epochs']} epochs, {LIGHT_CONFIG['max_batches_per_epoch']} batches/epoch\")\n",
    "print(f\"  Full training: {TRAINING_CONFIG['num_epochs']} epochs, early stop patience={TRAINING_CONFIG['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b187dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GPU optimizations\n",
    "if gpu_mgr.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_num_threads(8)\n",
    "    \n",
    "    print(f\"GPU: {gpu_mgr.gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_mgr.total_memory_gb:.1f} GB\")\n",
    "    print(\"GPU optimizations enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory and recommend optimal batch size\n",
    "if gpu_mgr.is_available():\n",
    "    recommended_batch = gpu_mgr.recommend_batch_size()\n",
    "    \n",
    "    print(f\"GPU: {gpu_mgr.gpu_name}\")\n",
    "    print(f\"Total Memory: {gpu_mgr.total_memory_gb:.2f} GB\")\n",
    "    print(f\"Recommended batch size: {recommended_batch}\")\n",
    "    print(f\"Current batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "    print(f\"Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "    \n",
    "    # Monitor current GPU state\n",
    "    gpu_mgr.cleanup()\n",
    "    stats = gpu_mgr.get_memory_stats()\n",
    "    print(f\"\\nCurrent GPU Usage: Allocated: {stats['allocated_gb']:.2f} GB, Reserved: {stats['reserved_gb']:.2f} GB, Free: {stats['free_gb']:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available - using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7f702",
   "metadata": {},
   "source": [
    "## 5. Light Pipeline Validation\n",
    "\n",
    "Quickly validate that all models can train without errors before committing to full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import light pipeline\n",
    "from light_pipeline import LightPipeline\n",
    "\n",
    "print(\"Light pipeline class loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run light validation pipeline for all models\n",
    "light_pipeline = LightPipeline(LIGHT_CONFIG, class_weights)\n",
    "\n",
    "# Validate all models\n",
    "validation_results = light_pipeline.validate_all_models(\n",
    "    ALL_MODELS,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")\n",
    "\n",
    "# Access results\n",
    "passed_models = light_pipeline.get_passed_models()\n",
    "failed_models = light_pipeline.get_failed_models()\n",
    "\n",
    "print(f\"\\nReady to proceed with {len(passed_models)} validated models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28668091",
   "metadata": {},
   "source": [
    "### Validation Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization utilities\n",
    "from visualizations import ValidationVisualizer\n",
    "\n",
    "# Create visualizer instance\n",
    "viz = ValidationVisualizer()\n",
    "\n",
    "# Visualize validation results with comprehensive analysis\n",
    "if validation_results:\n",
    "    # Figure 1: Training Speed & Success Rate Overview\n",
    "    viz.plot_validation_overview(validation_results, ALL_MODELS)\n",
    "    \n",
    "    # Figure 2: Learning Progress & Convergence Analysis\n",
    "    viz.plot_learning_analysis(validation_results, ALL_MODELS, len(train_loader))\n",
    "    \n",
    "    # Figure 3: Top Performers Podium\n",
    "    viz.plot_top_performers(validation_results, ALL_MODELS)\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    viz.print_validation_statistics(validation_results, ALL_MODELS, len(train_loader))\n",
    "else:\n",
    "    print(\"No validation results available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3353ee",
   "metadata": {},
   "source": [
    "## 6. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533069bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, config_dict, train_loader, val_loader, class_weights, resume_from_checkpoint=None):\n",
    "    \"\"\"Train a single model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        config_dict: Training configuration dictionary\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        class_weights: Class weights for loss function\n",
    "        resume_from_checkpoint: Path to checkpoint file to resume training from\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # CUDA optimizations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Create output directory or use existing for resume\n",
    "    if resume_from_checkpoint:\n",
    "        # Extract output dir from checkpoint path\n",
    "        checkpoint_path = Path(resume_from_checkpoint)\n",
    "        output_dir = checkpoint_path.parent.parent\n",
    "        checkpoint_dir = checkpoint_path.parent\n",
    "        print(f\"Resuming from checkpoint: {checkpoint_path}\")\n",
    "    else:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_dir = Path('../outputs/training') / f'{model_name}_{timestamp}'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        checkpoint_dir = output_dir / 'checkpoints'\n",
    "        checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Clear GPU memory before creating model\n",
    "    if config_dict['device'] == 'cuda':\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(\n",
    "        model_name=model_name,\n",
    "        in_channels=6 if 'siamese' not in model_name.lower() else 3,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        **config.MODEL_CONFIGS.get(model_name, {})\n",
    "    )\n",
    "    \n",
    "    # Move model to device with proper handling for meta tensors\n",
    "    if config_dict['device'] == 'cuda':\n",
    "        # First move to CPU if needed, then to CUDA to avoid meta tensor issues\n",
    "        model = model.cpu()\n",
    "        \n",
    "        # Clear GPU cache again before moving to CUDA\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        model = model.to(config_dict['device'])\n",
    "    else:\n",
    "        model = model.to(config_dict['device'])\n",
    "    \n",
    "    # Use torch.compile if available (disabled for now to avoid issues)\n",
    "    # if hasattr(torch, 'compile') and config_dict['device'] == 'cuda':\n",
    "    #     try:\n",
    "    #         model = torch.compile(model, mode='default')\n",
    "    #     except Exception as e:\n",
    "    #         pass\n",
    "    \n",
    "    # Create loss function\n",
    "    loss_fn = create_loss_function(\n",
    "        loss_type=config_dict['loss_type'],\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        class_weights=class_weights.to(config_dict['device']),\n",
    "        device=config_dict['device'],\n",
    "        ce_weight=config_dict.get('ce_weight', 0.1),\n",
    "        dice_weight=config_dict.get('dice_weight', 2.0),\n",
    "        focal_weight=config_dict.get('focal_weight', 3.0),\n",
    "        focal_gamma=config_dict.get('focal_gamma', 3.0)\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config_dict['learning_rate'],\n",
    "        weight_decay=config_dict['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Create learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3\n",
    "    )\n",
    "    \n",
    "    # Create experiment logger\n",
    "    from experiment_tracking import ExperimentLogger\n",
    "    logger = ExperimentLogger(experiment_name=f'{model_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "    \n",
    "    # Load checkpoint if resuming\n",
    "    start_epoch = 0\n",
    "    best_val_iou = 0.0\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = torch.load(resume_from_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "        best_val_iou = checkpoint.get('best_val_iou', 0.0)\n",
    "        print(f\"Resuming from epoch {start_epoch}, best IoU: {best_val_iou:.4f}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    experiment_name = f\"{model_name}_{output_dir.name.split('_', 1)[1]}\" if resume_from_checkpoint else f'{model_name}_{timestamp}'\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=loss_fn,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        device=config_dict['device'],\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        experiment_name=experiment_name,\n",
    "        use_amp=config_dict['use_amp'],\n",
    "        gradient_clip_val=config_dict['gradient_clip'],\n",
    "        early_stopping_patience=config_dict['early_stopping_patience'],\n",
    "        gradient_accumulation_steps=config_dict.get('gradient_accumulation_steps', 1),\n",
    "        class_names=config.CLASS_NAMES\n",
    "    )\n",
    "    \n",
    "    # Train (adjust epochs if resuming)\n",
    "    remaining_epochs = config_dict['num_epochs'] - start_epoch\n",
    "    if remaining_epochs > 0:\n",
    "        history = trainer.train(num_epochs=remaining_epochs)\n",
    "    else:\n",
    "        print(f\"Training already completed ({start_epoch} epochs). Skipping.\")\n",
    "        return None, output_dir\n",
    "    \n",
    "    # Print final summary\n",
    "    best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL RESULTS - {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Best epoch: {best_epoch + 1}/{len(history['val_iou'])}\")\n",
    "    print(f\"\\nBest validation metrics:\")\n",
    "    print(f\"  IoU:  {history['val_iou'][best_epoch]:.4f}\")\n",
    "    print(f\"  Dice: {history['val_dice'][best_epoch]:.4f}\")\n",
    "    print(f\"  F1:   {history['val_f1'][best_epoch]:.4f}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    # Per-class metrics if available\n",
    "    if 'val_iou_per_class' in history:\n",
    "        print(f\"\\nPer-class IoU (Best Epoch):\")\n",
    "        for i, (class_name, iou) in enumerate(zip(config.CLASS_NAMES, history['val_iou_per_class'][best_epoch])):\n",
    "            print(f\"  {class_name}: {iou:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    for epoch in range(len(history['train_loss'])):\n",
    "        logger.log_scalar('Loss/train', history['train_loss'][epoch], epoch)\n",
    "        logger.log_scalar('Loss/val', history['val_loss'][epoch], epoch)\n",
    "        logger.log_scalar('IoU/train', history['train_iou'][epoch], epoch)\n",
    "        logger.log_scalar('IoU/val', history['val_iou'][epoch], epoch)\n",
    "    \n",
    "    logger.close()\n",
    "    \n",
    "    # Save history\n",
    "    history_json = {}\n",
    "    for key, values in history.items():\n",
    "        if isinstance(values, list):\n",
    "            history_json[key] = [float(v) if hasattr(v, 'item') else v for v in values]\n",
    "        else:\n",
    "            history_json[key] = values\n",
    "    \n",
    "    with open(output_dir / 'training_history.json', 'w') as f:\n",
    "        json.dump(history_json, f, indent=2)\n",
    "    \n",
    "    print(f\"[SAVED] Checkpoints: {checkpoint_dir}\")\n",
    "    print(f\"[SAVED] Training history: {output_dir / 'training_history.json'}\\n\")\n",
    "    \n",
    "    return history, output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e7cf4",
   "metadata": {},
   "source": [
    "## 6. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training\n",
    "gpu_mgr.cleanup()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "if gpu_mgr.is_available():\n",
    "    stats = gpu_mgr.get_memory_stats()\n",
    "    total_memory_gb = stats['total_gb']\n",
    "    allocated_gb = stats['allocated_gb']\n",
    "    reserved_gb = stats['reserved_gb']\n",
    "    free_gb = stats['free_gb']\n",
    "    \n",
    "    print(f\"GPU Memory Status:\")\n",
    "    print(f\"  Total: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"  Allocated: {allocated_gb:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved_gb:.2f} GB\")\n",
    "    print(f\"  Available: {free_gb:.2f} GB\")\n",
    "    \n",
    "    if free_gb < 2.0:\n",
    "        print(\"\\nWARNING: Less than 2GB free GPU memory!\")\n",
    "        print(\"Recommendation: Restart runtime to clear GPU memory completely.\")\n",
    "    else:\n",
    "        print(f\"\\nGPU memory check passed. Ready for training.\")\n",
    "else:\n",
    "    print(\"No GPU available - training will be slow on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba81b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload trainer module \n",
    "import importlib\n",
    "if 'trainer' in sys.modules:\n",
    "    importlib.reload(sys.modules['trainer'])\n",
    "    print(\"Trainer module reloaded\")\n",
    "else:\n",
    "    from trainer import Trainer\n",
    "    print(\"Trainer module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d25f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "# Global variable to track currently training models\n",
    "current_training_models = []\n",
    "training_lock = threading.Lock()\n",
    "\n",
    "def train_model_parallel(model_name, config_dict, train_loader, val_loader, class_weights, results_queue):\n",
    "    \"\"\"Train a single model in parallel mode.\"\"\"\n",
    "    try:\n",
    "        # Register this model as currently training\n",
    "        with training_lock:\n",
    "            current_training_models.append(model_name)\n",
    "        \n",
    "        # Create separate config for this model\n",
    "        model_config = copy.deepcopy(config_dict)\n",
    "        model_config['batch_size'] = 4  # Reduced to 4 for parallel training\n",
    "        \n",
    "        # Clear GPU cache before training\n",
    "        gpu_mgr.cleanup()\n",
    "        \n",
    "        # Create fresh dataloaders for parallel training\n",
    "        parallel_train_loader, parallel_val_loader, _ = create_dataloaders(\n",
    "            train_dir=config.PROCESSED_TRAIN_DIR,\n",
    "            val_dir=config.PROCESSED_VAL_DIR,\n",
    "            test_dir=config.PROCESSED_TEST_DIR,\n",
    "            batch_size=model_config['batch_size'],\n",
    "            num_workers=1,  # Reduced to 1 for parallel training\n",
    "            pin_memory=False  # Disabled to save GPU memory\n",
    "        )\n",
    "        \n",
    "        # Train model with fresh dataloaders\n",
    "        history, output_dir = train_model(model_name, model_config, parallel_train_loader, parallel_val_loader, class_weights)\n",
    "        \n",
    "        # Store results\n",
    "        results_queue.put({\n",
    "            'model_name': model_name,\n",
    "            'history': history,\n",
    "            'output_dir': output_dir,\n",
    "            'success': True,\n",
    "            'error': None\n",
    "        })\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = f\"{str(e)}\\n{traceback.format_exc()}\"\n",
    "        results_queue.put({\n",
    "            'model_name': model_name,\n",
    "            'history': None,\n",
    "            'output_dir': None,\n",
    "            'success': False,\n",
    "            'error': error_msg\n",
    "        })\n",
    "    finally:\n",
    "        # Unregister this model\n",
    "        with training_lock:\n",
    "            if model_name in current_training_models:\n",
    "                current_training_models.remove(model_name)\n",
    "        \n",
    "        # Clear GPU cache after training\n",
    "        gpu_mgr.cleanup()\n",
    "\n",
    "def train_models_parallel_pairs(model_pairs, config_dict, train_loader, val_loader, class_weights):\n",
    "    \"\"\"Train models in parallel pairs.\"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for pair_idx, pair in enumerate(model_pairs):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PARALLEL TRAINING - PAIR {pair_idx + 1}: {' + '.join([m.upper() for m in pair])}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Clear GPU cache before starting pair\n",
    "        gpu_mgr.cleanup()\n",
    "        \n",
    "        # Create results queue\n",
    "        results_queue = Queue()\n",
    "        \n",
    "        # Create threads for parallel training\n",
    "        threads = []\n",
    "        for model_name in pair:\n",
    "            thread = threading.Thread(\n",
    "                target=train_model_parallel,\n",
    "                args=(model_name, config_dict, train_loader, val_loader, class_weights, results_queue)\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for both to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        \n",
    "        # Collect results\n",
    "        while not results_queue.empty():\n",
    "            result = results_queue.get()\n",
    "            model_name = result['model_name']\n",
    "            \n",
    "            if result['success']:\n",
    "                all_results[model_name] = {\n",
    "                    'history': result['history'],\n",
    "                    'output_dir': result['output_dir']\n",
    "                }\n",
    "                print(f\"\\n{model_name.upper()} completed successfully!\")\n",
    "            else:\n",
    "                print(f\"\\n{model_name.upper()} failed:\")\n",
    "                print(f\"Error: {result['error']}\\n\")\n",
    "        \n",
    "        # Clear GPU cache between pairs\n",
    "        gpu_mgr.cleanup()\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PAIR {pair_idx + 1} COMPLETED\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"Parallel training functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU monitoring using GPUManager\n",
    "print(\"GPU monitoring available via GPUManager.\")\n",
    "print(\"Run gpu_mgr.monitor_memory(interval=30, duration=3600) to start monitoring.\")\n",
    "\n",
    "# Uncomment to run GPU monitor in background\n",
    "import threading\n",
    "monitor_thread = gpu_mgr.monitor_memory(interval=30, duration=3600)\n",
    "print(\"GPU monitor started in background (30s refresh, 1 hour duration).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da915e7e",
   "metadata": {},
   "source": [
    "## 7. Full Training Execution\n",
    "\n",
    "**IMPORTANT:** Run the Light Pipeline Validation (Section 5) first before executing this section!\n",
    "\n",
    "This section trains all models with the optimized configuration:\n",
    "- 20 epochs (reduced from 30)\n",
    "- Early stopping patience: 5 (reduced from 10)\n",
    "- Sequential or parallel mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode: Set to True for parallel, False for sequential\n",
    "USE_PARALLEL_TRAINING = False  # Changed to False for more stable training\n",
    "\n",
    "# Resume from checkpoint (optional)\n",
    "# Set to checkpoint path to resume training, or None to start fresh\n",
    "# Example: RESUME_CHECKPOINTS = {'deeplabv3+': '../outputs/training/deeplabv3+_20251202_123456/checkpoints/best_model.pth'}\n",
    "RESUME_CHECKPOINTS = {}  # Empty dict = start fresh training for all models\n",
    "\n",
    "# Define all models to train\n",
    "# CRITICAL: Other GPU processes are using 18GB of your 42GB GPU!\n",
    "# Latest validation: Only DeepLabV3+ and SegFormer passed\n",
    "# \n",
    "# TO FIX: Kill other GPU processes to free memory\n",
    "# Run in terminal: nvidia-smi\n",
    "# Then kill processes: taskkill /PID 756229 /F (repeat for PIDs 1075108, 1251332)\n",
    "# Or restart your machine to clear all GPU memory\n",
    "#\n",
    "# Current strategy: Train the 2 models that consistently pass validation\n",
    "# ALL_MODELS = ['deeplabv3+', 'segformer']  # Models that passed latest validation\n",
    "# ALL_MODELS = ['unet++', 'deeplabv3+', 'segformer', 'fc_siam_diff']  # Uncomment after freeing GPU memory\n",
    "ALL_MODELS = ['unet++', 'deeplabv3+', 'segformer', 'fc_siam_diff', 'siamese_unet++', 'stanet']  # All 6 models\n",
    "\n",
    "# Model pairs for parallel training (only used if USE_PARALLEL_TRAINING=True)\n",
    "MODEL_PAIRS = [\n",
    "    ['unet++', 'deeplabv3+'],           # Pair 1: Smaller models\n",
    "    ['segformer', 'fc_siam_diff'],      # Pair 2: Medium models\n",
    "    ['siamese_unet++', 'stanet']        # Pair 3: Larger models\n",
    "]\n",
    "\n",
    "# Check validation status\n",
    "if 'validation_results' in globals():\n",
    "    failed_models = [m for m, r in validation_results.items() if r['status'] == 'failed']\n",
    "    if failed_models:\n",
    "        print(f\"\\nWARNING: {len(failed_models)} model(s) failed validation!\")\n",
    "        print(\"Failed models:\", ', '.join([m.upper() for m in failed_models]))\n",
    "        print(\"Recommend fixing validation errors before full training.\\n\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Light validation not run yet!\")\n",
    "    print(\"Recommend running Section 5 (Light Pipeline Validation) first.\\n\")\n",
    "\n",
    "# Execute training\n",
    "if USE_PARALLEL_TRAINING:\n",
    "    print(\"\\nPARALLEL TRAINING MODE\")\n",
    "    print(\"Training 6 models in 3 pairs with batch_size=4 per model\")\n",
    "    print(\"Total effective batch size: 8 (2 models Ã— 4)\\n\")\n",
    "    \n",
    "    results = train_models_parallel_pairs(\n",
    "        MODEL_PAIRS,\n",
    "        TRAINING_CONFIG,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        class_weights\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nSEQUENTIAL TRAINING MODE\")\n",
    "    print(f\"Training {len(ALL_MODELS)} models one by one with batch_size={TRAINING_CONFIG['batch_size']}\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name in ALL_MODELS:\n",
    "        # Check if resume checkpoint exists for this model\n",
    "        resume_checkpoint = RESUME_CHECKPOINTS.get(model_name, None)\n",
    "        if resume_checkpoint:\n",
    "            print(f\"\\nResuming {model_name} from checkpoint...\")\n",
    "        \n",
    "        history, output_dir = train_model(\n",
    "            model_name,\n",
    "            TRAINING_CONFIG,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            class_weights,\n",
    "            resume_from_checkpoint=resume_checkpoint\n",
    "        )\n",
    "        results[model_name] = {\n",
    "            'history': history,\n",
    "            'output_dir': output_dir\n",
    "        }\n",
    "        \n",
    "        # Clear GPU cache between models\n",
    "        gpu_mgr.cleanup()\n",
    "\n",
    "# Extract results (handle missing models gracefully)\n",
    "unet_history = results.get('unet++', {}).get('history')\n",
    "unet_output_dir = results.get('unet++', {}).get('output_dir')\n",
    "deeplab_history = results.get('deeplabv3+', {}).get('history')\n",
    "deeplab_output_dir = results.get('deeplabv3+', {}).get('output_dir')\n",
    "segformer_history = results.get('segformer', {}).get('history')\n",
    "segformer_output_dir = results.get('segformer', {}).get('output_dir')\n",
    "fcsiamdiff_history = results.get('fc_siam_diff', {}).get('history')\n",
    "fcsiamdiff_output_dir = results.get('fc_siam_diff', {}).get('output_dir')\n",
    "siamese_unet_history = results.get('siamese_unet++', {}).get('history')\n",
    "siamese_unet_output_dir = results.get('siamese_unet++', {}).get('output_dir')\n",
    "stanet_history = results.get('stanet', {}).get('history')\n",
    "stanet_output_dir = results.get('stanet', {}).get('output_dir')\n",
    "\n",
    "# Training complete summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mode: {'PARALLEL' if USE_PARALLEL_TRAINING else 'SEQUENTIAL'}\")\n",
    "\n",
    "successful_models = []\n",
    "failed_models = []\n",
    "\n",
    "for model_name in ALL_MODELS:\n",
    "    if model_name in results and results[model_name].get('history') is not None:\n",
    "        successful_models.append(model_name)\n",
    "    else:\n",
    "        failed_models.append(model_name)\n",
    "\n",
    "if successful_models:\n",
    "    print(f\"\\nSuccessfully trained models ({len(successful_models)}/{len(ALL_MODELS)}):\")\n",
    "    for model_name in successful_models:\n",
    "        print(f\"  {model_name.upper()}: {results[model_name]['output_dir']}\")\n",
    "\n",
    "if failed_models:\n",
    "    print(f\"\\nFailed models ({len(failed_models)}/{len(ALL_MODELS)}):\")\n",
    "    for model_name in failed_models:\n",
    "        print(f\"  {model_name.upper()}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d5837",
   "metadata": {},
   "source": [
    "## 8. Training Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef544359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training visualizer\n",
    "from visualizations import TrainingVisualizer\n",
    "\n",
    "# Visualize individual model training histories\n",
    "model_histories = [\n",
    "    (unet_history, 'U-Net++'),\n",
    "    (deeplab_history, 'DeepLabV3+'),\n",
    "    (segformer_history, 'SegFormer'),\n",
    "    (fcsiamdiff_history, 'FC-Siam-Diff'),\n",
    "    (siamese_unet_history, 'Siamese U-Net++'),\n",
    "    (stanet_history, 'STANet')\n",
    "]\n",
    "\n",
    "for history, model_name in model_histories:\n",
    "    if history is not None:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Visualizing {model_name} Training History\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Plot training history\n",
    "        save_path = Path('../outputs/training') / f'{model_name.lower().replace(\" \", \"_\").replace(\"+\", \"plus\")}_history.png'\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        TrainingVisualizer.plot_training_history(history, model_name, save_path)\n",
    "    else:\n",
    "        print(f\"\\nNo training history available for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201d0d",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models using TrainingVisualizer\n",
    "from visualizations import TrainingVisualizer\n",
    "\n",
    "all_histories = [\n",
    "    unet_history, \n",
    "    deeplab_history, \n",
    "    segformer_history, \n",
    "    fcsiamdiff_history, \n",
    "    siamese_unet_history, \n",
    "    stanet_history\n",
    "]\n",
    "\n",
    "all_names = [\n",
    "    'U-Net++', \n",
    "    'DeepLabV3+', \n",
    "    'SegFormer', \n",
    "    'FC-Siam-Diff', \n",
    "    'Siamese U-Net++', \n",
    "    'STANet'\n",
    "]\n",
    "\n",
    "if any(h is not None for h in all_histories):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"COMPARING ALL MODELS\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Generate comparison visualization\n",
    "    save_path = Path('../outputs/training/model_comparison.png')\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    TrainingVisualizer.compare_models(all_histories, all_names, save_path)\n",
    "else:\n",
    "    print(\"No models completed training successfully. Unable to generate comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56349f19",
   "metadata": {},
   "source": [
    "## 10. TensorBoard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard to view training metrics\n",
    "print(\"To view TensorBoard:\")\n",
    "print(\"1. Run in terminal: tensorboard --logdir=../outputs/tensorboard --port=6006\")\n",
    "print(\"2. Open browser: http://localhost:6006\")\n",
    "print(\"\\nTensorBoard shows:\")\n",
    "print(\"  - Training/validation loss curves\")\n",
    "print(\"  - IoU, Dice, F1 metrics over time\")\n",
    "print(\"  - Per-class performance\")\n",
    "print(\"  - Learning rate schedules\")\n",
    "print(\"  - Model graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576440d4",
   "metadata": {},
   "source": [
    "## 11. Commit and Push Checkpoints to Git\n",
    "\n",
    "Save training checkpoints and results to version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33244f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def commit_and_push_checkpoints(commit_message=None):\n",
    "    \"\"\"Commit training outputs and push to git repository.\"\"\"\n",
    "    \n",
    "    # Check if we're in a git repository\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'rev-parse', '--git-dir'], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(\"Git repository detected.\\n\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"ERROR: Not in a git repository. Cannot commit.\")\n",
    "        return False\n",
    "    \n",
    "    # Add outputs directory\n",
    "    outputs_dir = Path('../outputs')\n",
    "    if not outputs_dir.exists():\n",
    "        print(\"No outputs directory found. Nothing to commit.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Adding training outputs to git...\")\n",
    "    \n",
    "    # Add specific files (exclude large model weights if needed)\n",
    "    files_to_add = [\n",
    "        '../outputs/training/*/training_history.json',\n",
    "        '../outputs/training/*/checkpoints/*.pth',\n",
    "        '../outputs/model_comparison.png',\n",
    "        '../outputs/tensorboard'\n",
    "    ]\n",
    "    \n",
    "    for pattern in files_to_add:\n",
    "        try:\n",
    "            subprocess.run(['git', 'add', pattern], check=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not add {pattern}: {e}\")\n",
    "    \n",
    "    # Check if there are changes to commit\n",
    "    result = subprocess.run(['git', 'status', '--porcelain'], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if not result.stdout.strip():\n",
    "        print(\"\\nNo changes to commit.\")\n",
    "        return False\n",
    "    \n",
    "    # Create commit message\n",
    "    if not commit_message:\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        trained_models = ', '.join([m.upper() for m in ALL_MODELS]) if 'ALL_MODELS' in globals() else 'models'\n",
    "        commit_message = f\"Training checkpoint: {trained_models} - {timestamp}\"\n",
    "    \n",
    "    print(f\"\\nCommit message: {commit_message}\")\n",
    "    \n",
    "    # Commit\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'commit', '-m', commit_message],\n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(\"\\nCommit successful!\")\n",
    "        print(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\nCommit failed: {e.stderr}\")\n",
    "        return False\n",
    "    \n",
    "    # Push to remote\n",
    "    print(\"\\nPushing to remote repository...\")\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'push'],\n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(\"Push successful!\")\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\nPush failed: {e.stderr}\")\n",
    "        print(\"You may need to pull changes first or check your remote configuration.\")\n",
    "        return False\n",
    "\n",
    "# Run the commit and push\n",
    "print(\"=\"*80)\n",
    "print(\"COMMITTING TRAINING CHECKPOINTS TO GIT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis will commit:\")\n",
    "print(\"  - Training history JSON files\")\n",
    "print(\"  - Model checkpoints (.pth files)\")\n",
    "print(\"  - Comparison plots\")\n",
    "print(\"  - TensorBoard logs\")\n",
    "print(\"\\nNote: Large checkpoint files may take time to upload.\\n\")\n",
    "\n",
    "# commit_and_push_checkpoints with default message\n",
    "commit_and_push_checkpoints()\n",
    "\n",
    "print(\"COMMITTING TRAINING CHECKPOINTS TO GIT - COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
