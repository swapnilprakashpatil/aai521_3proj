{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd362576",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b052342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IS_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba132b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    if os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"Repository already exists. Pulling latest changes...\")\n",
    "        %cd /content/aai521_3proj\n",
    "        !git pull\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/swapnilprakashpatil/aai521_3proj.git\n",
    "        %cd aai521_3proj    \n",
    "    %pip install -r requirements.txt\n",
    "    sys.path.append('/content/aai521_3proj/src')\n",
    "    %ls\n",
    "else:\n",
    "    print(\"Running in local environment. Installing packages...\")\n",
    "    %pip install -r ../requirements.txt\n",
    "    sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a7f68",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67309af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "if 'dataset' in sys.modules:\n",
    "    importlib.reload(sys.modules['dataset'])\n",
    "if 'models' in sys.modules:\n",
    "    importlib.reload(sys.modules['models'])\n",
    "if 'config' in sys.modules:\n",
    "    importlib.reload(sys.modules['config'])\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "from dataset import create_dataloaders, FloodDataset\n",
    "from models import create_model, UNetPlusPlus, DeepLabV3Plus, SegFormer\n",
    "from losses import create_loss_function\n",
    "from metrics import MetricsTracker, SegmentationMetrics\n",
    "from trainer import Trainer\n",
    "from experiment_tracking import ExperimentLogger, ExperimentComparator\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0cc01",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1dbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_dir=config.PROCESSED_TRAIN_DIR,\n",
    "    val_dir=config.PROCESSED_VAL_DIR,\n",
    "    test_dir=config.PROCESSED_TEST_DIR,\n",
    "    batch_size=8,  # Match training config\n",
    "    num_workers=0,  # CRITICAL FIX: Set to 0 to prevent Windows multiprocessing deadlock\n",
    "    pin_memory=False  # Disable pin_memory when num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training: {len(train_loader.dataset)} patches ({len(train_loader)} batches)\")\n",
    "print(f\"  Validation: {len(val_loader.dataset)} patches ({len(val_loader)} batches)\")\n",
    "print(f\"  Test: {len(test_loader.dataset)} patches ({len(test_loader)} batches)\")\n",
    "\n",
    "# Get class weights\n",
    "class_weights = train_loader.dataset.get_class_weights()\n",
    "print(f\"\\nClass weights: {class_weights}\")\n",
    "\n",
    "# Get class distribution\n",
    "class_dist = train_loader.dataset.get_class_distribution()\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, (class_name, count) in enumerate(zip(config.CLASS_NAMES, class_dist)):\n",
    "    percentage = (count / class_dist.sum()) * 100\n",
    "    print(f\"  {class_name}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f1196",
   "metadata": {},
   "source": [
    "### Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb60715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(config.CLASS_NAMES)))\n",
    "bars = ax1.bar(range(len(config.CLASS_NAMES)), class_dist, color=colors, alpha=0.7)\n",
    "ax1.set_xlabel('Class', fontsize=12)\n",
    "ax1.set_ylabel('Pixel Count', fontsize=12)\n",
    "ax1.set_title('Class Distribution in Training Set', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(range(len(config.CLASS_NAMES)))\n",
    "ax1.set_xticklabels(config.CLASS_NAMES, rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, count) in enumerate(zip(bars, class_dist)):\n",
    "    height = bar.get_height()\n",
    "    percentage = (count / class_dist.sum()) * 100\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{percentage:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(class_dist, labels=config.CLASS_NAMES, autopct='%1.1f%%',\n",
    "        colors=colors, startangle=90)\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc0685",
   "metadata": {},
   "source": [
    "### Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "images = batch['image']\n",
    "masks = batch['mask']\n",
    "\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Mask shape: {masks.shape}\")\n",
    "print(f\"Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Mask classes: {masks.unique().tolist()}\")\n",
    "\n",
    "# Visualize samples\n",
    "def visualize_samples(images, masks, num_samples=3):\n",
    "    \"\"\"Visualize pre/post images and masks.\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    \n",
    "    # Color map for masks\n",
    "    cmap = plt.cm.get_cmap('tab10', len(config.CLASS_NAMES))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Pre-event image (first 3 channels)\n",
    "        pre_img = images[i, :3].permute(1, 2, 0).numpy()\n",
    "        pre_img = (pre_img - pre_img.min()) / (pre_img.max() - pre_img.min() + 1e-8)\n",
    "        \n",
    "        # Post-event image (last 3 channels)\n",
    "        post_img = images[i, 3:].permute(1, 2, 0).numpy()\n",
    "        post_img = (post_img - post_img.min()) / (post_img.max() - post_img.min() + 1e-8)\n",
    "        \n",
    "        # Mask\n",
    "        mask = masks[i].numpy()\n",
    "        \n",
    "        # Plot pre-event\n",
    "        axes[i, 0].imshow(pre_img)\n",
    "        axes[i, 0].set_title('Pre-Event Image', fontsize=12, fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot post-event\n",
    "        axes[i, 1].imshow(post_img)\n",
    "        axes[i, 1].set_title('Post-Event Image', fontsize=12, fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Plot mask\n",
    "        mask_plot = axes[i, 2].imshow(mask, cmap=cmap, vmin=0, vmax=len(config.CLASS_NAMES)-1)\n",
    "        axes[i, 2].set_title('Ground Truth Mask', fontsize=12, fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Add colorbar to last mask\n",
    "        if i == num_samples - 1:\n",
    "            cbar = plt.colorbar(mask_plot, ax=axes[i, 2], orientation='horizontal', \n",
    "                              pad=0.05, fraction=0.046)\n",
    "            cbar.set_ticks(range(len(config.CLASS_NAMES)))\n",
    "            cbar.set_ticklabels(config.CLASS_NAMES, rotation=45, ha='right', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(images, masks, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6f041",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for architecture overview\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# List of all models to train\n",
    "ALL_MODELS = ['unet++', 'deeplabv3+', 'segformer', 'fc_siam_diff', 'siamese_unet++', 'stanet']\n",
    "\n",
    "models_info = []\n",
    "\n",
    "for model_name in ALL_MODELS:\n",
    "    model = create_model(\n",
    "        model_name=model_name,\n",
    "        in_channels=6 if 'siamese' not in model_name.lower() else 3,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        **config.MODEL_CONFIGS.get(model_name, {})\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    models_info.append({\n",
    "        'Model': model_name.upper(),\n",
    "        'Total Parameters': f\"{total_params:,}\",\n",
    "        'Trainable Parameters': f\"{trainable_params:,}\",\n",
    "        'Size (MB)': f\"{total_params * 4 / 1e6:.2f}\"\n",
    "    })\n",
    "    \n",
    "    del model\n",
    "\n",
    "# Display as table\n",
    "models_df = pd.DataFrame(models_info)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(models_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15267a92",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bb952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for all models\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 8,  # Reduced to safe size for stability\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 1e-4,\n",
    "    'use_amp': True,\n",
    "    'gradient_clip': 1.0,\n",
    "    'early_stopping_patience': 15,\n",
    "    'loss_type': 'combined',\n",
    "    'scheduler_type': 'plateau',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'gradient_accumulation_steps': 4,  # Effective batch size = 32\n",
    "    'print_every_n_epochs': 5,\n",
    "    # Loss weights for class imbalance\n",
    "    'ce_weight': 0.1,\n",
    "    'dice_weight': 2.0,\n",
    "    'focal_weight': 3.0,\n",
    "    'focal_gamma': 3.0\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\n  Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory and recommend optimal batch size\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory_gb = gpu_props.total_memory / 1e9\n",
    "    \n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total Memory: {total_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Rough estimation for batch size recommendation\n",
    "    if total_memory_gb >= 24:\n",
    "        recommended_batch = \"24-32 (you have plenty of VRAM)\"\n",
    "    elif total_memory_gb >= 16:\n",
    "        recommended_batch = \"16-24 (current: 16 is good)\"\n",
    "    elif total_memory_gb >= 12:\n",
    "        recommended_batch = \"12-16 (current: 16 might be tight)\"\n",
    "    elif total_memory_gb >= 8:\n",
    "        recommended_batch = \"8-12 (reduce to 12 if OOM)\"\n",
    "    else:\n",
    "        recommended_batch = \"4-8 (reduce to 8 and increase grad accumulation)\"\n",
    "    \n",
    "    print(f\"Recommended batch size: {recommended_batch}\")\n",
    "    print(f\"Current batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "    print(f\"Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "    \n",
    "    # Monitor current GPU state\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"\\nCurrent GPU Usage:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Free: {total_memory_gb - reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available - using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7f702",
   "metadata": {},
   "source": [
    "## 5. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533069bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, config_dict, train_loader, val_loader, class_weights):\n",
    "    \"\"\"Train a single model and return training history.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # CUDA optimizations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Create output directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = Path('../outputs/training') / f'{model_name}_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_dir = output_dir / 'checkpoints'\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(\n",
    "        model_name=model_name,\n",
    "        in_channels=6 if 'siamese' not in model_name.lower() else 3,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        **config.MODEL_CONFIGS.get(model_name, {})\n",
    "    )\n",
    "    model = model.to(config_dict['device'])\n",
    "    \n",
    "    # Use torch.compile if available\n",
    "    if hasattr(torch, 'compile') and config_dict['device'] == 'cuda':\n",
    "        try:\n",
    "            model = torch.compile(model, mode='default')\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Create loss function\n",
    "    loss_fn = create_loss_function(\n",
    "        loss_type=config_dict['loss_type'],\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        class_weights=class_weights.to(config_dict['device']),\n",
    "        device=config_dict['device'],\n",
    "        ce_weight=config_dict.get('ce_weight', 0.1),\n",
    "        dice_weight=config_dict.get('dice_weight', 2.0),\n",
    "        focal_weight=config_dict.get('focal_weight', 3.0),\n",
    "        focal_gamma=config_dict.get('focal_gamma', 3.0)\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config_dict['learning_rate'],\n",
    "        weight_decay=config_dict['weight_decay'],\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Create experiment logger\n",
    "    logger = ExperimentLogger(\n",
    "        log_dir=Path('../outputs/tensorboard'),\n",
    "        experiment_name=f'{model_name}_{timestamp}'\n",
    "    )\n",
    "    logger.log_hyperparameters(config_dict)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=loss_fn,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        device=config_dict['device'],\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        experiment_name=f'{model_name}_{timestamp}',\n",
    "        use_amp=config_dict['use_amp'],\n",
    "        gradient_clip_val=config_dict['gradient_clip'],\n",
    "        early_stopping_patience=config_dict['early_stopping_patience'],\n",
    "        gradient_accumulation_steps=config_dict.get('gradient_accumulation_steps', 1),\n",
    "        class_names=config.CLASS_NAMES\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.train(num_epochs=config_dict['num_epochs'])\n",
    "    \n",
    "    # Print final summary\n",
    "    best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL RESULTS - {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Best epoch: {best_epoch + 1}/{len(history['val_iou'])}\")\n",
    "    print(f\"\\nBest validation metrics:\")\n",
    "    print(f\"  IoU:  {history['val_iou'][best_epoch]:.4f}\")\n",
    "    print(f\"  Dice: {history['val_dice'][best_epoch]:.4f}\")\n",
    "    print(f\"  F1:   {history['val_f1'][best_epoch]:.4f}\")\n",
    "    \n",
    "    # Per-class metrics if available\n",
    "    if 'val_iou_per_class' in history:\n",
    "        print(f\"\\nPer-class IoU (Best Epoch):\")\n",
    "        for i, (class_name, iou) in enumerate(zip(config.CLASS_NAMES, history['val_iou_per_class'][best_epoch])):\n",
    "            print(f\"  {class_name}: {iou:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    for epoch in range(len(history['train_loss'])):\n",
    "        logger.log_scalar('Loss/train', history['train_loss'][epoch], epoch)\n",
    "        logger.log_scalar('Loss/val', history['val_loss'][epoch], epoch)\n",
    "        logger.log_scalar('IoU/train', history['train_iou'][epoch], epoch)\n",
    "        logger.log_scalar('IoU/val', history['val_iou'][epoch], epoch)\n",
    "    \n",
    "    logger.close()\n",
    "    \n",
    "    # Save history\n",
    "    history_json = {}\n",
    "    for key, values in history.items():\n",
    "        if isinstance(values, list):\n",
    "            history_json[key] = [float(v) if hasattr(v, 'item') else v for v in values]\n",
    "        else:\n",
    "            history_json[key] = values\n",
    "    \n",
    "    with open(output_dir / 'training_history.json', 'w') as f:\n",
    "        json.dump(history_json, f, indent=2)\n",
    "    \n",
    "    print(f\"[SAVED] Checkpoints: {checkpoint_dir}\")\n",
    "    print(f\"[SAVED] Training history: {output_dir / 'training_history.json'}\\n\")\n",
    "    \n",
    "    return history, output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e7cf4",
   "metadata": {},
   "source": [
    "## 6. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check available memory\n",
    "    free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "    free_gb = free_memory / 1e9\n",
    "    print(f\"GPU Memory Available: {free_gb:.2f} GB\")\n",
    "    \n",
    "    if free_gb < 2.0:\n",
    "        print(\"WARNING: Less than 2GB free GPU memory!\")\n",
    "        print(\"Please restart kernel and re-run from the beginning.\")\n",
    "    else:\n",
    "        print(\"GPU memory check passed. Starting training...\")\n",
    "else:\n",
    "    print(\"No GPU available - training will be slow on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba81b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload trainer module with error fix\n",
    "import importlib\n",
    "if 'trainer' in sys.modules:\n",
    "    importlib.reload(sys.modules['trainer'])\n",
    "    print(\"Trainer module reloaded successfully\")\n",
    "else:\n",
    "    from trainer import Trainer\n",
    "    print(\"Trainer module loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02822b6",
   "metadata": {},
   "source": [
    "### 6.1 U-Net++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc31f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train U-Net++\n",
    "unet_history, unet_output_dir = train_model(\n",
    "    'unet++',\n",
    "    TRAINING_CONFIG,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62447322",
   "metadata": {},
   "source": [
    "### 6.2 DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DeepLabV3+\n",
    "deeplab_history, deeplab_output_dir = train_model(\n",
    "    'deeplabv3+',\n",
    "    TRAINING_CONFIG,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6278095",
   "metadata": {},
   "source": [
    "### 6.3 SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe20705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SegFormer\n",
    "segformer_history, segformer_output_dir = train_model(\n",
    "    'segformer',\n",
    "    TRAINING_CONFIG,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576629fa",
   "metadata": {},
   "source": [
    "### 6.4 FC-Siam-Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FC-Siam-Diff\n",
    "fcsiamdiff_history, fcsiamdiff_output_dir = train_model(\n",
    "    'fc_siam_diff',\n",
    "    TRAINING_CONFIG,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1c292",
   "metadata": {},
   "source": [
    "### 6.5 Siamese U-Net++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Siamese U-Net++\n",
    "siamese_unet_history, siamese_unet_output_dir = train_model(\n",
    "    'siamese_unet++',\n",
    "    TRAINING_CONFIG,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81f7f7",
   "metadata": {},
   "source": [
    "### 6.6 STANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train STANet\n",
    "stanet_history, stanet_output_dir = train_model(\n",
    "    'stanet',\n",
    "    TRAINING_CONFIG,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d5837",
   "metadata": {},
   "source": [
    "## 7. Training Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef544359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, save_path=None):\n",
    "    \"\"\"Plot training history for a single model.\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title(f'{model_name} - Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # IoU plot\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax2.plot(epochs, history['train_iou'], 'b-', label='Train IoU', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_iou'], 'r-', label='Val IoU', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=11)\n",
    "    ax2.set_ylabel('IoU', fontsize=11)\n",
    "    ax2.set_title('Mean IoU', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Dice plot\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax3.plot(epochs, history['train_dice'], 'b-', label='Train Dice', linewidth=2)\n",
    "    ax3.plot(epochs, history['val_dice'], 'r-', label='Val Dice', linewidth=2)\n",
    "    ax3.set_xlabel('Epoch', fontsize=11)\n",
    "    ax3.set_ylabel('Dice', fontsize=11)\n",
    "    ax3.set_title('Mean Dice Coefficient', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} Training Metrics', fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print best metrics\n",
    "    best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name} - Best Validation Metrics (Epoch {best_epoch + 1})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Mean IoU: {history['val_iou'][best_epoch]:.4f}\")\n",
    "    print(f\"Mean Dice: {history['val_dice'][best_epoch]:.4f}\")\n",
    "    print(f\"Mean F1: {history['val_f1'][best_epoch]:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201d0d",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "def compare_models(histories, model_names):\n",
    "    \"\"\"Compare multiple models.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    # Plot 1: Validation Loss\n",
    "    for history, name, color in zip(histories, model_names, colors):\n",
    "        epochs = range(1, len(history['val_loss']) + 1)\n",
    "        axes[0, 0].plot(epochs, history['val_loss'], label=name, linewidth=2, color=color)\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 0].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation IoU\n",
    "    for history, name, color in zip(histories, model_names, colors):\n",
    "        epochs = range(1, len(history['val_iou']) + 1)\n",
    "        axes[0, 1].plot(epochs, history['val_iou'], label=name, linewidth=2, color=color)\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('IoU', fontsize=12)\n",
    "    axes[0, 1].set_title('Validation Mean IoU Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Validation Dice\n",
    "    for history, name, color in zip(histories, model_names, colors):\n",
    "        epochs = range(1, len(history['val_dice']) + 1)\n",
    "        axes[1, 0].plot(epochs, history['val_dice'], label=name, linewidth=2, color=color)\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Dice', fontsize=12)\n",
    "    axes[1, 0].set_title('Validation Mean Dice Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Best Metrics Bar Chart\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Get best metrics for each model\n",
    "    iou_values = []\n",
    "    dice_values = []\n",
    "    f1_values = []\n",
    "    \n",
    "    for history in histories:\n",
    "        best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "        iou_values.append(history['val_iou'][best_epoch])\n",
    "        dice_values.append(history['val_dice'][best_epoch])\n",
    "        f1_values.append(history['val_f1'][best_epoch])\n",
    "    \n",
    "    axes[1, 1].bar(x - width, iou_values, width, label='IoU', alpha=0.8)\n",
    "    axes[1, 1].bar(x, dice_values, width, label='Dice', alpha=0.8)\n",
    "    axes[1, 1].bar(x + width, f1_values, width, label='F1', alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Model', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "    axes[1, 1].set_title('Best Validation Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"MODEL COMPARISON - BEST VALIDATION METRICS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"{'Model':<20} {'Best Epoch':<12} {'Mean IoU':<12} {'Mean Dice':<12} {'Mean F1':<12}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    for name, history in zip(model_names, histories):\n",
    "        best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "        \n",
    "        print(f\"{name:<20} {best_epoch+1:<12} {history['val_iou'][best_epoch]:<12.4f} \"\n",
    "              f\"{history['val_dice'][best_epoch]:<12.4f} {history['val_f1'][best_epoch]:<12.4f}\")\n",
    "    \n",
    "    print(f\"{'='*100}\\n\")\n",
    "\n",
    "# Compare all models\n",
    "compare_models(\n",
    "    [unet_history, deeplab_history, segformer_history, fcsiamdiff_history, siamese_unet_history, stanet_history],\n",
    "    ['U-Net++', 'DeepLabV3+', 'SegFormer', 'FC-Siam-Diff', 'Siamese U-Net++', 'STANet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56349f19",
   "metadata": {},
   "source": [
    "## 9. TensorBoard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard to view training metrics\n",
    "print(\"To view TensorBoard:\")\n",
    "print(\"1. Run in terminal: tensorboard --logdir=../outputs/tensorboard --port=6006\")\n",
    "print(\"2. Open browser: http://localhost:6006\")\n",
    "print(\"\\nTensorBoard shows:\")\n",
    "print(\"  - Training/validation loss curves\")\n",
    "print(\"  - IoU, Dice, F1 metrics over time\")\n",
    "print(\"  - Per-class performance\")\n",
    "print(\"  - Learning rate schedules\")\n",
    "print(\"  - Model graphs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
