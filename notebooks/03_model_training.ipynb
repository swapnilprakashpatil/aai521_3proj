{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd362576",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b052342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IS_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Google Colab Environment Specifications:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get system info\n",
    "    \n",
    "    print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Architecture: {platform.machine()}\")\n",
    "    print(f\"Python Version: {platform.python_version()}\")\n",
    "    \n",
    "    # Memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "    \n",
    "    # GPU info\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            gpu_info = result.stdout.strip().split('\\n')\n",
    "            for i, gpu in enumerate(gpu_info):\n",
    "                name, memory = gpu.split(', ')\n",
    "                print(f\"GPU {i}: {name}, {memory} MB VRAM\")\n",
    "        else:\n",
    "            print(\"GPU: Not detected or nvidia-smi unavailable\")\n",
    "    except:\n",
    "        print(\"GPU: Not detected\")\n",
    "    \n",
    "    # Disk space\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Space: {disk.free / (1024**3):.1f} GB free / {disk.total / (1024**3):.1f} GB total\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"Not running in Google Colab environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba132b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    if os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"Repository already exists. Pulling latest changes...\")\n",
    "        %cd /content/aai521_3proj\n",
    "        !git pull\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/swapnilprakashpatil/aai521_3proj.git\n",
    "        %cd aai521_3proj    \n",
    "    %pip install -r requirements.txt\n",
    "    sys.path.append('/content/aai521_3proj/src')\n",
    "    %ls\n",
    "else:\n",
    "    print(\"Running in local environment. Installing packages...\")\n",
    "    %pip install -r ../requirements.txt\n",
    "    sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a7f68",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67309af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "if 'dataset' in sys.modules:\n",
    "    importlib.reload(sys.modules['dataset'])\n",
    "if 'models' in sys.modules:\n",
    "    importlib.reload(sys.modules['models'])\n",
    "if 'config' in sys.modules:\n",
    "    importlib.reload(sys.modules['config'])\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "from dataset import create_dataloaders, FloodDataset\n",
    "from models import create_model, UNetPlusPlus, DeepLabV3Plus, SegFormer\n",
    "from losses import create_loss_function\n",
    "from metrics import MetricsTracker, SegmentationMetrics\n",
    "from trainer import Trainer\n",
    "from experiment_tracking import ExperimentLogger, ExperimentComparator\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0cc01",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1dbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_dir=config.PROCESSED_TRAIN_DIR,\n",
    "    val_dir=config.PROCESSED_VAL_DIR,\n",
    "    test_dir=config.PROCESSED_TEST_DIR,\n",
    "    batch_size=64,  # A100 can handle 64 easily with 40GB VRAM\n",
    "    num_workers=4,  # Linux supports more workers without issues\n",
    "    pin_memory=True  # Faster GPU transfer\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training: {len(train_loader.dataset)} patches ({len(train_loader)} batches)\")\n",
    "print(f\"  Validation: {len(val_loader.dataset)} patches ({len(val_loader)} batches)\")\n",
    "print(f\"  Test: {len(test_loader.dataset)} patches ({len(test_loader)} batches)\")\n",
    "\n",
    "# Get class weights\n",
    "class_weights = train_loader.dataset.get_class_weights()\n",
    "print(f\"\\nClass weights: {class_weights}\")\n",
    "\n",
    "# Get class distribution\n",
    "class_dist = train_loader.dataset.get_class_distribution()\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, (class_name, count) in enumerate(zip(config.CLASS_NAMES, class_dist)):\n",
    "    percentage = (count / class_dist.sum()) * 100\n",
    "    print(f\"  {class_name}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f1196",
   "metadata": {},
   "source": [
    "### Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb60715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(config.CLASS_NAMES)))\n",
    "bars = ax1.bar(range(len(config.CLASS_NAMES)), class_dist, color=colors, alpha=0.7)\n",
    "ax1.set_xlabel('Class', fontsize=12)\n",
    "ax1.set_ylabel('Pixel Count', fontsize=12)\n",
    "ax1.set_title('Class Distribution in Training Set', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(range(len(config.CLASS_NAMES)))\n",
    "ax1.set_xticklabels(config.CLASS_NAMES, rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, count) in enumerate(zip(bars, class_dist)):\n",
    "    height = bar.get_height()\n",
    "    percentage = (count / class_dist.sum()) * 100\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{percentage:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(class_dist, labels=config.CLASS_NAMES, autopct='%1.1f%%',\n",
    "        colors=colors, startangle=90)\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc0685",
   "metadata": {},
   "source": [
    "### Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "images = batch['image']\n",
    "masks = batch['mask']\n",
    "\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Mask shape: {masks.shape}\")\n",
    "print(f\"Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Mask classes: {masks.unique().tolist()}\")\n",
    "\n",
    "# Visualize samples\n",
    "def visualize_samples(images, masks, num_samples=3):\n",
    "    \"\"\"Visualize pre/post images and masks.\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    \n",
    "    # Color map for masks\n",
    "    cmap = plt.cm.get_cmap('tab10', len(config.CLASS_NAMES))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Pre-event image (first 3 channels)\n",
    "        pre_img = images[i, :3].permute(1, 2, 0).numpy()\n",
    "        pre_img = (pre_img - pre_img.min()) / (pre_img.max() - pre_img.min() + 1e-8)\n",
    "        \n",
    "        # Post-event image (last 3 channels)\n",
    "        post_img = images[i, 3:].permute(1, 2, 0).numpy()\n",
    "        post_img = (post_img - post_img.min()) / (post_img.max() - post_img.min() + 1e-8)\n",
    "        \n",
    "        # Mask\n",
    "        mask = masks[i].numpy()\n",
    "        \n",
    "        # Plot pre-event\n",
    "        axes[i, 0].imshow(pre_img)\n",
    "        axes[i, 0].set_title('Pre-Event Image', fontsize=12, fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot post-event\n",
    "        axes[i, 1].imshow(post_img)\n",
    "        axes[i, 1].set_title('Post-Event Image', fontsize=12, fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Plot mask\n",
    "        mask_plot = axes[i, 2].imshow(mask, cmap=cmap, vmin=0, vmax=len(config.CLASS_NAMES)-1)\n",
    "        axes[i, 2].set_title('Ground Truth Mask', fontsize=12, fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Add colorbar to last mask\n",
    "        if i == num_samples - 1:\n",
    "            cbar = plt.colorbar(mask_plot, ax=axes[i, 2], orientation='horizontal', \n",
    "                              pad=0.05, fraction=0.046)\n",
    "            cbar.set_ticks(range(len(config.CLASS_NAMES)))\n",
    "            cbar.set_ticklabels(config.CLASS_NAMES, rotation=45, ha='right', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(images, masks, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6f041",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for architecture overview\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# List of all models to train\n",
    "ALL_MODELS = ['unet++', 'deeplabv3+', 'segformer', 'fc_siam_diff', 'siamese_unet++', 'stanet']\n",
    "\n",
    "models_info = []\n",
    "\n",
    "for model_name in ALL_MODELS:\n",
    "    model = create_model(\n",
    "        model_name=model_name,\n",
    "        in_channels=6 if 'siamese' not in model_name.lower() else 3,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        **config.MODEL_CONFIGS.get(model_name, {})\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    models_info.append({\n",
    "        'Model': model_name.upper(),\n",
    "        'Total Parameters': f\"{total_params:,}\",\n",
    "        'Trainable Parameters': f\"{trainable_params:,}\",\n",
    "        'Size (MB)': f\"{total_params * 4 / 1e6:.2f}\"\n",
    "    })\n",
    "    \n",
    "    del model\n",
    "\n",
    "# Display as table\n",
    "models_df = pd.DataFrame(models_info)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(models_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15267a92",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bb952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - OPTIMIZED FOR A100 GPU (40GB VRAM)\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 64,  # A100 can handle large batches with 40GB VRAM\n",
    "    'num_epochs': 30,  # Balanced for quality and speed\n",
    "    'learning_rate': 1e-4,  # Slightly higher LR for larger batch\n",
    "    'weight_decay': 1e-4,\n",
    "    'use_amp': True,  # Critical for A100 - uses Tensor Cores\n",
    "    'gradient_clip': 1.0,\n",
    "    'early_stopping_patience': 10,\n",
    "    'loss_type': 'combined',\n",
    "    'scheduler_type': 'plateau',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'gradient_accumulation_steps': 1,  # No accumulation needed with batch=64\n",
    "    'print_every_n_epochs': 5,\n",
    "    # Loss weights for class imbalance\n",
    "    'ce_weight': 0.1,\n",
    "    'dice_weight': 2.0,\n",
    "    'focal_weight': 3.0,\n",
    "    'focal_gamma': 3.0\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CONFIGURATION - A100 GPU OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\n  ðŸš€ A100 GPU OPTIMIZATIONS:\")\n",
    "print(f\"  âœ“ Batch size: 64 (8x original - maximum GPU utilization)\")\n",
    "print(f\"  âœ“ Data workers: 4 (Linux + persistent_workers)\")\n",
    "print(f\"  âœ“ Pin memory: enabled\")\n",
    "print(f\"  âœ“ Prefetch factor: 2\")\n",
    "print(f\"  âœ“ Mixed precision (AMP): Tensor Cores enabled\")\n",
    "print(f\"  âœ“ TF32: ON for faster matrix operations\")\n",
    "print(f\"\\n  âš¡ EXPECTED PERFORMANCE ON A100:\")\n",
    "print(f\"  â€¢ Speed: ~30-40 sec/epoch (10x faster than Windows!)\")\n",
    "print(f\"  â€¢ Time per model: ~20-25 minutes\")\n",
    "print(f\"  â€¢ Total for 6 models: ~2-2.5 hours\")\n",
    "print(f\"  â€¢ GPU utilization: 30-40% (optimal for this model size)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b187dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply A100 GPU-specific optimizations\n",
    "if torch.cuda.is_available():\n",
    "    # Enable cuDNN auto-tuner to find best algorithms\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Enable TF32 for A100 (faster computation on Ampere GPUs)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Set optimal number of threads for Colab\n",
    "    torch.set_num_threads(8)\n",
    "    \n",
    "    # Check if A100 is detected\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸš€ A100 GPU OPTIMIZATIONS ENABLED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  GPU: {gpu_name}\")\n",
    "    print(f\"  âœ“ cuDNN benchmark mode: ON (auto-tune algorithms)\")\n",
    "    print(f\"  âœ“ TF32 tensor cores: ON (A100 specific - 8x faster!)\")\n",
    "    print(f\"  âœ“ Mixed Precision (AMP): Enabled in config\")\n",
    "    print(f\"  âœ“ CPU threads: 8 (Colab optimized)\")\n",
    "    print(f\"  âœ“ Persistent workers: ON (prefetching enabled)\")\n",
    "    \n",
    "    # Display memory info\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\n  ðŸ’¾ GPU Memory: {total_memory:.1f} GB\")\n",
    "    print(f\"  ðŸ“Š Expected usage with batch=64: ~15-20 GB (50% utilization)\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68758ad9",
   "metadata": {},
   "source": [
    "### ðŸš€ A100 GPU Performance Optimizations\n",
    "\n",
    "**Optimized for Google Colab A100 (40GB VRAM) - 10x faster than local!**\n",
    "\n",
    "#### 1. **Hardware Advantages**:\n",
    "   - **A100 GPU**: 40GB VRAM (vs 4-8GB consumer GPUs)\n",
    "   - **Tensor Cores**: Specialized hardware for mixed precision\n",
    "   - **High bandwidth**: 1.5TB/s memory bandwidth\n",
    "   - **Linux environment**: Better multiprocessing support\n",
    "\n",
    "#### 2. **Configuration Optimizations**:\n",
    "   - **Batch size: 64** (8x larger than Windows - 50% GPU utilization)\n",
    "   - **Workers: 4** with persistent_workers (no Windows limitations)\n",
    "   - **Prefetch factor: 2** (continuous data streaming)\n",
    "   - **Mixed Precision (AMP)**: Leverages A100 Tensor Cores for 2-3x speedup\n",
    "\n",
    "#### 3. **PyTorch Optimizations**:\n",
    "   - **TF32 mode**: A100-specific acceleration (8x faster matmul)\n",
    "   - **cuDNN benchmark**: Auto-tunes convolution algorithms\n",
    "   - **No gradient accumulation**: Direct large batches\n",
    "\n",
    "#### 4. **Expected Performance** âš¡:\n",
    "   - **Speed**: ~30-40 sec/epoch (was ~377 sec on Windows)\n",
    "   - **Per model**: ~20-25 minutes (was ~5 hours)\n",
    "   - **Total (6 models)**: ~2-2.5 hours (was ~30 hours)\n",
    "   - **Overall speedup**: **10-12x faster than original setup!**\n",
    "\n",
    "#### 5. **Memory Usage**:\n",
    "   - Batch=64: ~15-20 GB / 40 GB (~50% utilization)\n",
    "   - Leaves plenty of headroom for model optimization\n",
    "   - Can potentially increase to batch=96 if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory and recommend optimal batch size\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory_gb = gpu_props.total_memory / 1e9\n",
    "    \n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total Memory: {total_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Rough estimation for batch size recommendation\n",
    "    if total_memory_gb >= 24:\n",
    "        recommended_batch = \"24-32 (you have plenty of VRAM)\"\n",
    "    elif total_memory_gb >= 16:\n",
    "        recommended_batch = \"16-24 (current: 16 is good)\"\n",
    "    elif total_memory_gb >= 12:\n",
    "        recommended_batch = \"12-16 (current: 16 might be tight)\"\n",
    "    elif total_memory_gb >= 8:\n",
    "        recommended_batch = \"8-12 (reduce to 12 if OOM)\"\n",
    "    else:\n",
    "        recommended_batch = \"4-8 (reduce to 8 and increase grad accumulation)\"\n",
    "    \n",
    "    print(f\"Recommended batch size: {recommended_batch}\")\n",
    "    print(f\"Current batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "    print(f\"Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "    \n",
    "    # Monitor current GPU state\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"\\nCurrent GPU Usage:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Free: {total_memory_gb - reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available - using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7f702",
   "metadata": {},
   "source": [
    "## 5. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533069bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, config_dict, train_loader, val_loader, class_weights):\n",
    "    \"\"\"Train a single model and return training history.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # CUDA optimizations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Create output directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = Path('../outputs/training') / f'{model_name}_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_dir = output_dir / 'checkpoints'\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(\n",
    "        model_name=model_name,\n",
    "        in_channels=6 if 'siamese' not in model_name.lower() else 3,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        **config.MODEL_CONFIGS.get(model_name, {})\n",
    "    )\n",
    "    model = model.to(config_dict['device'])\n",
    "    \n",
    "    # Use torch.compile if available\n",
    "    if hasattr(torch, 'compile') and config_dict['device'] == 'cuda':\n",
    "        try:\n",
    "            model = torch.compile(model, mode='default')\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Create loss function\n",
    "    loss_fn = create_loss_function(\n",
    "        loss_type=config_dict['loss_type'],\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        class_weights=class_weights.to(config_dict['device']),\n",
    "        device=config_dict['device'],\n",
    "        ce_weight=config_dict.get('ce_weight', 0.1),\n",
    "        dice_weight=config_dict.get('dice_weight', 2.0),\n",
    "        focal_weight=config_dict.get('focal_weight', 3.0),\n",
    "        focal_gamma=config_dict.get('focal_gamma', 3.0)\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config_dict['learning_rate'],\n",
    "        weight_decay=config_dict['weight_decay'],\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Create experiment logger\n",
    "    logger = ExperimentLogger(\n",
    "        log_dir=Path('../outputs/tensorboard'),\n",
    "        experiment_name=f'{model_name}_{timestamp}'\n",
    "    )\n",
    "    logger.log_hyperparameters(config_dict)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=loss_fn,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        device=config_dict['device'],\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        experiment_name=f'{model_name}_{timestamp}',\n",
    "        use_amp=config_dict['use_amp'],\n",
    "        gradient_clip_val=config_dict['gradient_clip'],\n",
    "        early_stopping_patience=config_dict['early_stopping_patience'],\n",
    "        gradient_accumulation_steps=config_dict.get('gradient_accumulation_steps', 1),\n",
    "        class_names=config.CLASS_NAMES\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.train(num_epochs=config_dict['num_epochs'])\n",
    "    \n",
    "    # Print final summary\n",
    "    best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL RESULTS - {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Best epoch: {best_epoch + 1}/{len(history['val_iou'])}\")\n",
    "    print(f\"\\nBest validation metrics:\")\n",
    "    print(f\"  IoU:  {history['val_iou'][best_epoch]:.4f}\")\n",
    "    print(f\"  Dice: {history['val_dice'][best_epoch]:.4f}\")\n",
    "    print(f\"  F1:   {history['val_f1'][best_epoch]:.4f}\")\n",
    "    \n",
    "    # Per-class metrics if available\n",
    "    if 'val_iou_per_class' in history:\n",
    "        print(f\"\\nPer-class IoU (Best Epoch):\")\n",
    "        for i, (class_name, iou) in enumerate(zip(config.CLASS_NAMES, history['val_iou_per_class'][best_epoch])):\n",
    "            print(f\"  {class_name}: {iou:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    for epoch in range(len(history['train_loss'])):\n",
    "        logger.log_scalar('Loss/train', history['train_loss'][epoch], epoch)\n",
    "        logger.log_scalar('Loss/val', history['val_loss'][epoch], epoch)\n",
    "        logger.log_scalar('IoU/train', history['train_iou'][epoch], epoch)\n",
    "        logger.log_scalar('IoU/val', history['val_iou'][epoch], epoch)\n",
    "    \n",
    "    logger.close()\n",
    "    \n",
    "    # Save history\n",
    "    history_json = {}\n",
    "    for key, values in history.items():\n",
    "        if isinstance(values, list):\n",
    "            history_json[key] = [float(v) if hasattr(v, 'item') else v for v in values]\n",
    "        else:\n",
    "            history_json[key] = values\n",
    "    \n",
    "    with open(output_dir / 'training_history.json', 'w') as f:\n",
    "        json.dump(history_json, f, indent=2)\n",
    "    \n",
    "    print(f\"[SAVED] Checkpoints: {checkpoint_dir}\")\n",
    "    print(f\"[SAVED] Training history: {output_dir / 'training_history.json'}\\n\")\n",
    "    \n",
    "    return history, output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e7cf4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Train All Models\n",
    "\n",
    "**Choose ONE of the following training modes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check available memory\n",
    "    free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "    free_gb = free_memory / 1e9\n",
    "    print(f\"GPU Memory Available: {free_gb:.2f} GB\")\n",
    "    \n",
    "    if free_gb < 2.0:\n",
    "        print(\"WARNING: Less than 2GB free GPU memory!\")\n",
    "        print(\"Please restart kernel and re-run from the beginning.\")\n",
    "    else:\n",
    "        print(\"GPU memory check passed. Starting training...\")\n",
    "else:\n",
    "    print(\"No GPU available - training will be slow on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba81b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload trainer module \n",
    "import importlib\n",
    "if 'trainer' in sys.modules:\n",
    "    importlib.reload(sys.modules['trainer'])\n",
    "    print(\"Trainer module reloaded\")\n",
    "else:\n",
    "    from trainer import Trainer\n",
    "    print(\"Trainer module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d25f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import copy\n",
    "\n",
    "# Global variable to track currently training models\n",
    "current_training_models = []\n",
    "training_lock = threading.Lock()\n",
    "\n",
    "def train_model_parallel(model_name, config_dict, train_loader, val_loader, class_weights, results_queue):\n",
    "    \"\"\"Train a single model in parallel mode.\"\"\"\n",
    "    try:\n",
    "        # Register this model as currently training\n",
    "        with training_lock:\n",
    "            current_training_models.append(model_name)\n",
    "        \n",
    "        # Create separate config for this model\n",
    "        model_config = copy.deepcopy(config_dict)\n",
    "        model_config['batch_size'] = 32  # Reduced for parallel training\n",
    "        \n",
    "        # Train model\n",
    "        history, output_dir = train_model(model_name, model_config, train_loader, val_loader, class_weights)\n",
    "        \n",
    "        # Store results\n",
    "        results_queue.put({\n",
    "            'model_name': model_name,\n",
    "            'history': history,\n",
    "            'output_dir': output_dir,\n",
    "            'success': True,\n",
    "            'error': None\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results_queue.put({\n",
    "            'model_name': model_name,\n",
    "            'history': None,\n",
    "            'output_dir': None,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    finally:\n",
    "        # Unregister this model\n",
    "        with training_lock:\n",
    "            if model_name in current_training_models:\n",
    "                current_training_models.remove(model_name)\n",
    "\n",
    "def train_models_parallel_pairs(model_pairs, config_dict, train_loader, val_loader, class_weights):\n",
    "    \"\"\"Train models in parallel pairs.\"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for pair_idx, pair in enumerate(model_pairs):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PARALLEL TRAINING - PAIR {pair_idx + 1}: {' + '.join([m.upper() for m in pair])}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Create results queue\n",
    "        results_queue = Queue()\n",
    "        \n",
    "        # Create threads for parallel training\n",
    "        threads = []\n",
    "        for model_name in pair:\n",
    "            thread = threading.Thread(\n",
    "                target=train_model_parallel,\n",
    "                args=(model_name, config_dict, train_loader, val_loader, class_weights, results_queue)\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for both to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        \n",
    "        # Collect results\n",
    "        while not results_queue.empty():\n",
    "            result = results_queue.get()\n",
    "            model_name = result['model_name']\n",
    "            \n",
    "            if result['success']:\n",
    "                all_results[model_name] = {\n",
    "                    'history': result['history'],\n",
    "                    'output_dir': result['output_dir']\n",
    "                }\n",
    "                print(f\"\\n{model_name.upper()} completed successfully!\")\n",
    "            else:\n",
    "                print(f\"\\n{model_name.upper()} failed: {result['error']}\")\n",
    "        \n",
    "        # Clear GPU cache between pairs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PAIR {pair_idx + 1} COMPLETED\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"Parallel training functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage during parallel training\n",
    "def monitor_gpu_usage(interval=10, duration=None):\n",
    "    \"\"\"Monitor GPU memory usage in real-time with model names.\"\"\"\n",
    "    import time\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            if duration and (time.time() - start_time) > duration:\n",
    "                break\n",
    "                \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "                \n",
    "                # Get currently training models\n",
    "                with training_lock:\n",
    "                    training_models = current_training_models.copy()\n",
    "                \n",
    "                print(f\"{'='*70}\")\n",
    "                print(f\"GPU Memory Monitor (refreshes every {interval}s)\")\n",
    "                print(f\"{'='*70}\")\n",
    "                \n",
    "                # Show currently training models\n",
    "                if training_models:\n",
    "                    models_str = ' + '.join([m.upper() for m in training_models])\n",
    "                    print(f\"  Currently Training: {models_str}\")\n",
    "                    print(f\"{'-'*70}\")\n",
    "                else:\n",
    "                    print(f\"  Currently Training: (idle)\")\n",
    "                    print(f\"{'-'*70}\")\n",
    "                \n",
    "                print(f\"  Allocated: {allocated:.2f} GB / {total:.1f} GB ({allocated/total*100:.1f}%)\")\n",
    "                print(f\"  Reserved:  {reserved:.2f} GB / {total:.1f} GB ({reserved/total*100:.1f}%)\")\n",
    "                print(f\"  Free:      {total - reserved:.2f} GB ({(total-reserved)/total*100:.1f}%)\")\n",
    "                print(f\"\\n  Press Interrupt (â– ) to stop monitoring\")\n",
    "                print(f\"{'='*70}\")\n",
    "            else:\n",
    "                print(\"No GPU available\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(interval)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped.\")\n",
    "\n",
    "print(\"GPU monitoring function ready.\")\n",
    "\n",
    "# run GPU monitor in background (useful during parallel training)\n",
    "import threading\n",
    "monitor_thread = threading.Thread(target=monitor_gpu_usage, args=(10, 3600))\n",
    "monitor_thread.daemon = True\n",
    "monitor_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da915e7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training Mode Configuration\n",
    "\n",
    "**Set the training mode below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING MODE CONFIGURATION\n",
    "# ============================================================================\n",
    "# Set USE_PARALLEL_TRAINING to True for parallel training (faster on A100)\n",
    "# Set to False for sequential training (more stable, easier to debug)\n",
    "\n",
    "USE_PARALLEL_TRAINING = True  # Change to False for sequential training\n",
    "\n",
    "# Define all models to train\n",
    "ALL_MODELS = ['unet++', 'deeplabv3+', 'segformer', 'fc_siam_diff', 'siamese_unet++', 'stanet']\n",
    "\n",
    "# Define model pairs for parallel training (only used if USE_PARALLEL_TRAINING=True)\n",
    "MODEL_PAIRS = [\n",
    "    ['unet++', 'deeplabv3+'],           # Pair 1: ~25 minutes\n",
    "    ['segformer', 'fc_siam_diff'],      # Pair 2: ~25 minutes  \n",
    "    ['siamese_unet++', 'stanet']        # Pair 3: ~25 minutes\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if USE_PARALLEL_TRAINING:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PARALLEL TRAINING MODE - Training 6 models in 3 pairs\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Train all pairs\n",
    "    results = train_models_parallel_pairs(\n",
    "        MODEL_PAIRS,\n",
    "        TRAINING_CONFIG,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        class_weights\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEQUENTIAL TRAINING MODE - Training 6 models one by one\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Train models sequentially\n",
    "    results = {}\n",
    "    for model_name in ALL_MODELS:\n",
    "        history, output_dir = train_model(\n",
    "            model_name,\n",
    "            TRAINING_CONFIG,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            class_weights\n",
    "        )\n",
    "        results[model_name] = {\n",
    "            'history': history,\n",
    "            'output_dir': output_dir\n",
    "        }\n",
    "        \n",
    "        # Clear GPU cache between models\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT RESULTS FOR ALL MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Extract individual results (works for both parallel and sequential)\n",
    "unet_history = results['unet++']['history']\n",
    "unet_output_dir = results['unet++']['output_dir']\n",
    "\n",
    "deeplab_history = results['deeplabv3+']['history']\n",
    "deeplab_output_dir = results['deeplabv3+']['output_dir']\n",
    "\n",
    "segformer_history = results['segformer']['history']\n",
    "segformer_output_dir = results['segformer']['output_dir']\n",
    "\n",
    "fcsiamdiff_history = results['fc_siam_diff']['history']\n",
    "fcsiamdiff_output_dir = results['fc_siam_diff']['output_dir']\n",
    "\n",
    "siamese_unet_history = results['siamese_unet++']['history']\n",
    "siamese_unet_output_dir = results['siamese_unet++']['output_dir']\n",
    "\n",
    "stanet_history = results['stanet']['history']\n",
    "stanet_output_dir = results['stanet']['output_dir']\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING COMPLETE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL 6 MODELS TRAINED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training mode: {'PARALLEL' if USE_PARALLEL_TRAINING else 'SEQUENTIAL'}\")\n",
    "print(f\"\\nResults stored:\")\n",
    "for model_name, result in results.items():\n",
    "    print(f\"  {model_name.upper()}: {result['output_dir']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d5837",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef544359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, save_path=None):\n",
    "    \"\"\"Plot training history for a single model.\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title(f'{model_name} - Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # IoU plot\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax2.plot(epochs, history['train_iou'], 'b-', label='Train IoU', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_iou'], 'r-', label='Val IoU', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=11)\n",
    "    ax2.set_ylabel('IoU', fontsize=11)\n",
    "    ax2.set_title('Mean IoU', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Dice plot\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax3.plot(epochs, history['train_dice'], 'b-', label='Train Dice', linewidth=2)\n",
    "    ax3.plot(epochs, history['val_dice'], 'r-', label='Val Dice', linewidth=2)\n",
    "    ax3.set_xlabel('Epoch', fontsize=11)\n",
    "    ax3.set_ylabel('Dice', fontsize=11)\n",
    "    ax3.set_title('Mean Dice Coefficient', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} Training Metrics', fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print best metrics\n",
    "    best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name} - Best Validation Metrics (Epoch {best_epoch + 1})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Mean IoU: {history['val_iou'][best_epoch]:.4f}\")\n",
    "    print(f\"Mean Dice: {history['val_dice'][best_epoch]:.4f}\")\n",
    "    print(f\"Mean F1: {history['val_f1'][best_epoch]:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201d0d",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "def compare_models(histories, model_names):\n",
    "    \"\"\"Compare multiple models.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    # Plot 1: Validation Loss\n",
    "    for history, name, color in zip(histories, model_names, colors):\n",
    "        epochs = range(1, len(history['val_loss']) + 1)\n",
    "        axes[0, 0].plot(epochs, history['val_loss'], label=name, linewidth=2, color=color)\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 0].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation IoU\n",
    "    for history, name, color in zip(histories, model_names, colors):\n",
    "        epochs = range(1, len(history['val_iou']) + 1)\n",
    "        axes[0, 1].plot(epochs, history['val_iou'], label=name, linewidth=2, color=color)\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('IoU', fontsize=12)\n",
    "    axes[0, 1].set_title('Validation Mean IoU Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Validation Dice\n",
    "    for history, name, color in zip(histories, model_names, colors):\n",
    "        epochs = range(1, len(history['val_dice']) + 1)\n",
    "        axes[1, 0].plot(epochs, history['val_dice'], label=name, linewidth=2, color=color)\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Dice', fontsize=12)\n",
    "    axes[1, 0].set_title('Validation Mean Dice Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Best Metrics Bar Chart\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Get best metrics for each model\n",
    "    iou_values = []\n",
    "    dice_values = []\n",
    "    f1_values = []\n",
    "    \n",
    "    for history in histories:\n",
    "        best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "        iou_values.append(history['val_iou'][best_epoch])\n",
    "        dice_values.append(history['val_dice'][best_epoch])\n",
    "        f1_values.append(history['val_f1'][best_epoch])\n",
    "    \n",
    "    axes[1, 1].bar(x - width, iou_values, width, label='IoU', alpha=0.8)\n",
    "    axes[1, 1].bar(x, dice_values, width, label='Dice', alpha=0.8)\n",
    "    axes[1, 1].bar(x + width, f1_values, width, label='F1', alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Model', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "    axes[1, 1].set_title('Best Validation Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"MODEL COMPARISON - BEST VALIDATION METRICS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"{'Model':<20} {'Best Epoch':<12} {'Mean IoU':<12} {'Mean Dice':<12} {'Mean F1':<12}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    for name, history in zip(model_names, histories):\n",
    "        best_epoch = max(range(len(history['val_iou'])), key=lambda i: history['val_iou'][i])\n",
    "        \n",
    "        print(f\"{name:<20} {best_epoch+1:<12} {history['val_iou'][best_epoch]:<12.4f} \"\n",
    "              f\"{history['val_dice'][best_epoch]:<12.4f} {history['val_f1'][best_epoch]:<12.4f}\")\n",
    "    \n",
    "    print(f\"{'='*100}\\n\")\n",
    "\n",
    "# Compare all models\n",
    "compare_models(\n",
    "    [unet_history, deeplab_history, segformer_history, fcsiamdiff_history, siamese_unet_history, stanet_history],\n",
    "    ['U-Net++', 'DeepLabV3+', 'SegFormer', 'FC-Siam-Diff', 'Siamese U-Net++', 'STANet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56349f19",
   "metadata": {},
   "source": [
    "## 9. TensorBoard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard to view training metrics\n",
    "print(\"To view TensorBoard:\")\n",
    "print(\"1. Run in terminal: tensorboard --logdir=../outputs/tensorboard --port=6006\")\n",
    "print(\"2. Open browser: http://localhost:6006\")\n",
    "print(\"\\nTensorBoard shows:\")\n",
    "print(\"  - Training/validation loss curves\")\n",
    "print(\"  - IoU, Dice, F1 metrics over time\")\n",
    "print(\"  - Per-class performance\")\n",
    "print(\"  - Learning rate schedules\")\n",
    "print(\"  - Model graphs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
