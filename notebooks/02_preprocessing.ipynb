{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dacffbc",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IS_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Google Colab Environment Specifications:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get system info\n",
    "    \n",
    "    print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Architecture: {platform.machine()}\")\n",
    "    print(f\"Python Version: {platform.python_version()}\")\n",
    "    \n",
    "    # Memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "    \n",
    "    # GPU info\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            gpu_info = result.stdout.strip().split('\\n')\n",
    "            for i, gpu in enumerate(gpu_info):\n",
    "                name, memory = gpu.split(', ')\n",
    "                print(f\"GPU {i}: {name}, {memory} MB VRAM\")\n",
    "        else:\n",
    "            print(\"GPU: Not detected or nvidia-smi unavailable\")\n",
    "    except:\n",
    "        print(\"GPU: Not detected\")\n",
    "    \n",
    "    # Disk space\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Space: {disk.free / (1024**3):.1f} GB free / {disk.total / (1024**3):.1f} GB total\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if not os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"WARNING: Cloning project repository required.\")\n",
    "        print(\"=\"*50)\n",
    "else:\n",
    "    print(\"Not running in Google Colab environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    if os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"Repository already exists. Pulling latest changes...\")\n",
    "        %cd /content/aai521_3proj\n",
    "        !git pull\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/swapnilprakashpatil/aai521_3proj.git\n",
    "        %cd aai521_3proj    \n",
    "    %pip install -r requirements.txt --quiet\n",
    "    sys.path.append('/content/aai521_3proj/src')\n",
    "    %ls\n",
    "else:\n",
    "    print(\"Running in local environment. Installing packages...\")\n",
    "    %pip install -r ../requirements.txt --quiet\n",
    "    sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79760ae5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "if 'config' in sys.modules:\n",
    "    importlib.reload(sys.modules['config'])\n",
    "if 'data_loader' in sys.modules:\n",
    "    importlib.reload(sys.modules['data_loader'])\n",
    "if 'preprocessing' in sys.modules:\n",
    "    importlib.reload(sys.modules['preprocessing'])\n",
    "if 'augmentation' in sys.modules:\n",
    "    importlib.reload(sys.modules['augmentation'])\n",
    "\n",
    "# Import custom modules\n",
    "from config import (\n",
    "    GERMANY_TRAIN, LOUISIANA_EAST_TRAIN,\n",
    "    PROCESSED_TRAIN_DIR, CLASS_NAMES, CLASS_COLORS,\n",
    "    PATCH_SIZE, PATCH_OVERLAP, MIN_FLOOD_PIXELS\n",
    ")\n",
    "\n",
    "from data_loader import DatasetLoader, load_tile_data\n",
    "from preprocessing import ImagePreprocessor, PatchExtractor\n",
    "from augmentation import get_training_augmentation, DualImageAugmentation\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Configuration loaded: MIN_FLOOD_PIXELS = {MIN_FLOOD_PIXELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984c033",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset loaders\n",
    "germany_loader = DatasetLoader(GERMANY_TRAIN, 'Germany')\n",
    "louisiana_loader = DatasetLoader(LOUISIANA_EAST_TRAIN, 'Louisiana-East')\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"  Germany tiles: {len(germany_loader.get_tile_list())}\")\n",
    "print(f\"  Louisiana-East tiles: {len(louisiana_loader.get_tile_list())}\")\n",
    "print(f\"  Total tiles: {len(germany_loader.get_tile_list()) + len(louisiana_loader.get_tile_list())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flood statistics\n",
    "germany_stats = germany_loader.get_flood_statistics()\n",
    "louisiana_stats = louisiana_loader.get_flood_statistics()\n",
    "\n",
    "# Create comparison dataframe\n",
    "stats_df = pd.DataFrame({\n",
    "    'Germany': germany_stats,\n",
    "    'Louisiana-East': louisiana_stats\n",
    "}).T\n",
    "\n",
    "print(\"\\nFlood Statistics by Region:\")\n",
    "print(stats_df)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Flooded vs Non-flooded counts\n",
    "regions = ['Germany', 'Louisiana-East']\n",
    "flooded = [germany_stats['flooded_count'], louisiana_stats['flooded_count']]\n",
    "non_flooded = [germany_stats['non_flooded_count'], louisiana_stats['non_flooded_count']]\n",
    "\n",
    "x = np.arange(len(regions))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, flooded, width, label='Flooded', color='#e74c3c')\n",
    "axes[0].bar(x + width/2, non_flooded, width, label='Non-flooded', color='#2ecc71')\n",
    "axes[0].set_xlabel('Region')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Flooded vs Non-flooded Road Segments')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(regions)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentage breakdown\n",
    "flooded_pct = [germany_stats['flooded_pct'], louisiana_stats['flooded_pct']]\n",
    "non_flooded_pct = [germany_stats['non_flooded_pct'], louisiana_stats['non_flooded_pct']]\n",
    "\n",
    "axes[1].bar(x, flooded_pct, width, label='Flooded %', color='#e74c3c')\n",
    "axes[1].bar(x, non_flooded_pct, width, bottom=flooded_pct, label='Non-flooded %', color='#2ecc71')\n",
    "axes[1].set_xlabel('Region')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].set_title('Class Distribution (Percentage)')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(regions)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Imbalance Detected:\")\n",
    "print(f\"   Average flooded ratio: {np.mean(flooded_pct):.1f}%\")\n",
    "print(f\"   Addressed through:\")\n",
    "print(f\"     - Oversampling flood-positive patches\")\n",
    "print(f\"     - Class-weighted loss functions\")\n",
    "print(f\"     - Focused augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37123a7f",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Sample Tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample tile from Germany\n",
    "sample_tile_name = germany_loader.get_tile_list()[0]\n",
    "print(f\"Loading sample tile: {sample_tile_name}\")\n",
    "\n",
    "sample_data = load_tile_data(GERMANY_TRAIN, sample_tile_name, 'Germany')\n",
    "\n",
    "print(f\"\\nTile information:\")\n",
    "print(f\"  Pre-image shape: {sample_data['pre_image'].shape}\")\n",
    "print(f\"  Post-image shape: {sample_data['post_image'].shape}\")\n",
    "print(f\"  Mask shape: {sample_data['mask'].shape}\")\n",
    "print(f\"  Pre-image dtype: {sample_data['pre_metadata']['dtype']}\")\n",
    "print(f\"  Pre-image range: [{sample_data['pre_image'].min():.3f}, {sample_data['pre_image'].max():.3f}]\")\n",
    "\n",
    "# Check mask classes\n",
    "unique_classes = np.unique(sample_data['mask'])\n",
    "print(f\"\\nMask classes present: {unique_classes}\")\n",
    "for cls in unique_classes:\n",
    "    count = np.sum(sample_data['mask'] == cls)\n",
    "    pct = (count / sample_data['mask'].size) * 100\n",
    "    print(f\"  Class {cls} ({CLASS_NAMES.get(cls, 'unknown')}): {count} pixels ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769beb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original tile\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "\n",
    "# Pre-event\n",
    "axes[0, 0].imshow(sample_data['pre_image'])\n",
    "axes[0, 0].set_title('Pre-Event Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Post-event\n",
    "axes[0, 1].imshow(sample_data['post_image'])\n",
    "axes[0, 1].set_title('Post-Event Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Mask\n",
    "axes[1, 0].imshow(sample_data['mask'], cmap='tab10')\n",
    "axes[1, 0].set_title('Segmentation Mask', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Difference\n",
    "diff = np.abs(sample_data['post_image'] - sample_data['pre_image'])\n",
    "axes[1, 1].imshow(diff)\n",
    "axes[1, 1].set_title('Temporal Difference (|Post - Pre|)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44339f",
   "metadata": {},
   "source": [
    "## 4. Image Enhancement with CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = ImagePreprocessor(\n",
    "    apply_clahe=True,\n",
    "    clahe_clip_limit=2.0,\n",
    "    clahe_tile_grid_size=(8, 8)\n",
    ")\n",
    "\n",
    "# Apply enhancement to pre-image\n",
    "pre_enhanced = preprocessor.apply_clahe_enhancement(sample_data['pre_image'])\n",
    "post_enhanced = preprocessor.apply_clahe_enhancement(sample_data['post_image'])\n",
    "\n",
    "# Compare original vs enhanced\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Pre-event comparison\n",
    "axes[0, 0].imshow(sample_data['pre_image'])\n",
    "axes[0, 0].set_title('Pre-Event Original', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(pre_enhanced)\n",
    "axes[0, 1].set_title('Pre-Event Enhanced (CLAHE)', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Histogram comparison for pre-event\n",
    "for i in range(3):\n",
    "    axes[0, 2].hist(sample_data['pre_image'][:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Orig')\n",
    "    axes[0, 2].hist(pre_enhanced[:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Enh', linestyle='--')\n",
    "axes[0, 2].set_title('Pre-Event Histogram', fontsize=12)\n",
    "axes[0, 2].set_xlabel('Intensity')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend(fontsize=8)\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# Post-event comparison\n",
    "axes[1, 0].imshow(sample_data['post_image'])\n",
    "axes[1, 0].set_title('Post-Event Original', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(post_enhanced)\n",
    "axes[1, 1].set_title('Post-Event Enhanced (CLAHE)', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Histogram comparison for post-event\n",
    "for i in range(3):\n",
    "    axes[1, 2].hist(sample_data['post_image'][:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Orig')\n",
    "    axes[1, 2].hist(post_enhanced[:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Enh', linestyle='--')\n",
    "axes[1, 2].set_title('Post-Event Histogram', fontsize=12)\n",
    "axes[1, 2].set_xlabel('Intensity')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].legend(fontsize=8)\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCLAHE Enhancement Applied:\")\n",
    "print(\"  - Improves local contrast\")\n",
    "print(\"  - Better visibility of flood boundaries\")\n",
    "print(\"  - Histogram equalization in tiles (8x8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f3e9d",
   "metadata": {},
   "source": [
    "## 5. Advanced Preprocessing: Cloud Removal, Deblurring & Geometric Correction\n",
    "\n",
    "Apply advanced preprocessing techniques to handle common satellite imagery issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced image processing libraries\n",
    "import sys\n",
    "try:\n",
    "    from skimage import morphology, filters, exposure, restoration, transform\n",
    "    from skimage.filters import rank, gaussian\n",
    "    from skimage.morphology import disk, remove_small_objects, remove_small_holes\n",
    "    from scipy import ndimage\n",
    "    from scipy.signal import convolve2d\n",
    "    print(\"Advanced image processing libraries loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Installing required libraries: {e}\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"scikit-image\", \"scipy\"])\n",
    "    from skimage import morphology, filters, exposure, restoration, transform\n",
    "    from skimage.filters import rank, gaussian\n",
    "    from skimage.morphology import disk, remove_small_objects, remove_small_holes\n",
    "    from scipy import ndimage\n",
    "    from scipy.signal import convolve2d\n",
    "    print(\"Libraries installed and loaded\")\n",
    "\n",
    "print(\"\\nAdvanced preprocessing methods:\")\n",
    "print(\"  1. Multi-stage cloud detection (brightness + texture + saturation)\")\n",
    "print(\"  2. Morphological cloud refinement\")\n",
    "print(\"  3. Advanced inpainting (Navier-Stokes + Telea)\")\n",
    "print(\"  4. Wiener deconvolution for deblurring\")\n",
    "print(\"  5. Richardson-Lucy deconvolution\")\n",
    "print(\"  6. Unsharp masking with adaptive strength\")\n",
    "print(\"  7. CLAHE enhancement per channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_degraded_image(clean_image):\n",
    "    \"\"\"\n",
    "    Create a moderately degraded version to demonstrate preprocessing capabilities\n",
    "    Adds realistic clouds, haze, and blur while preserving some edge structure\n",
    "    \"\"\"\n",
    "    degraded = clean_image.copy()\n",
    "    h, w = degraded.shape[:2]\n",
    "    \n",
    "    # 1. Add atmospheric haze (reduces contrast and adds blue tint) - REDUCED\n",
    "    haze_strength = 0.3  # Reduced from 0.5 to preserve more edges\n",
    "    haze_color = np.array([0.7, 0.75, 0.85])  # Blueish-white\n",
    "    degraded = degraded * (1 - haze_strength) + haze_color * haze_strength\n",
    "    \n",
    "    # 2. Add realistic cloud patches - FEWER AND LIGHTER\n",
    "    num_clouds = np.random.randint(5, 10)  # Reduced from 8-15\n",
    "    for _ in range(num_clouds):\n",
    "        # Random cloud center\n",
    "        cx, cy = np.random.randint(0, w), np.random.randint(0, h)\n",
    "        \n",
    "        # Cloud size - SMALLER\n",
    "        cloud_w = np.random.randint(60, 150)  # Reduced from 80-200\n",
    "        cloud_h = np.random.randint(40, 120)  # Reduced from 60-150\n",
    "        \n",
    "        # Create cloud mask with soft edges (Gaussian falloff)\n",
    "        y_coords, x_coords = np.ogrid[:h, :w]\n",
    "        cloud_mask = np.exp(-((x_coords - cx)**2 / (2 * cloud_w**2) + \n",
    "                             (y_coords - cy)**2 / (2 * cloud_h**2)))\n",
    "        \n",
    "        # Cloud color (bright white with slight variation)\n",
    "        cloud_color = np.array([0.85, 0.88, 0.95]) + np.random.uniform(-0.05, 0.05, 3)\n",
    "        cloud_opacity = np.random.uniform(0.3, 0.6)  # Reduced from 0.5-0.9 for lighter clouds\n",
    "        \n",
    "        # Blend cloud\n",
    "        for c in range(3):\n",
    "            degraded[:, :, c] = (degraded[:, :, c] * (1 - cloud_mask * cloud_opacity) + \n",
    "                                cloud_color[c] * cloud_mask * cloud_opacity)\n",
    "    \n",
    "    # 3. Add motion blur (simulating camera/satellite motion) - REDUCED\n",
    "    kernel_size = 9  # Reduced from 15\n",
    "    motion_kernel = np.zeros((kernel_size, kernel_size))\n",
    "    motion_kernel[kernel_size // 2, :] = 1.0 / kernel_size\n",
    "    \n",
    "    blurred = np.zeros_like(degraded)\n",
    "    for c in range(3):\n",
    "        blurred[:, :, c] = convolve2d(degraded[:, :, c], motion_kernel, mode='same', boundary='symm')\n",
    "    degraded = blurred\n",
    "    \n",
    "    # 4. Add Gaussian noise - REDUCED\n",
    "    noise = np.random.normal(0, 0.02, degraded.shape)  # Reduced from 0.03\n",
    "    degraded = degraded + noise\n",
    "    \n",
    "    # 5. Reduce overall sharpness - LESS AGGRESSIVE\n",
    "    degraded = gaussian(degraded, sigma=1.0, channel_axis=2)  # Reduced from 1.5\n",
    "    \n",
    "    return np.clip(degraded, 0, 1)\n",
    "\n",
    "\n",
    "def advanced_cloud_removal(image, aggressive=True):\n",
    "    \"\"\"\n",
    "    State-of-the-art cloud detection and removal\n",
    "    \"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    h, w = img_uint8.shape[:2]\n",
    "    \n",
    "    # === MULTI-STAGE CLOUD DETECTION ===\n",
    "    \n",
    "    # Stage 1: Brightness analysis\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    if aggressive:\n",
    "        bright_mask = gray > 160  # Lower threshold for more detection\n",
    "    else:\n",
    "        bright_mask = gray > 180\n",
    "    \n",
    "    # Stage 2: Blue channel analysis (clouds are blue-white)\n",
    "    blue_excess = img_uint8[:, :, 2].astype(float) - (img_uint8[:, :, 0].astype(float) + img_uint8[:, :, 1].astype(float)) / 2\n",
    "    blue_mask = blue_excess > 10\n",
    "    \n",
    "    # Stage 3: Texture analysis (clouds have uniform texture)\n",
    "    selem = disk(7)\n",
    "    entropy_img = rank.entropy(gray, selem)\n",
    "    texture_mask = entropy_img < np.percentile(entropy_img, 25)\n",
    "    \n",
    "    # Stage 4: Saturation analysis (clouds have low saturation)\n",
    "    hsv = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2HSV)\n",
    "    low_sat_mask = hsv[:, :, 1] < 40\n",
    "    \n",
    "    # Stage 5: Value analysis (clouds are bright in HSV)\n",
    "    high_value_mask = hsv[:, :, 2] > 200\n",
    "    \n",
    "    # Combine all stages\n",
    "    cloud_mask = (bright_mask & blue_mask) | (bright_mask & low_sat_mask & texture_mask) | (high_value_mask & low_sat_mask)\n",
    "    cloud_mask = cloud_mask.astype(np.uint8) * 255\n",
    "    \n",
    "    # === MORPHOLOGICAL REFINEMENT ===\n",
    "    \n",
    "    # Remove small false positives\n",
    "    cloud_mask_binary = cloud_mask > 0\n",
    "    cloud_mask_binary = remove_small_objects(cloud_mask_binary, min_size=100, connectivity=2)\n",
    "    cloud_mask_binary = remove_small_holes(cloud_mask_binary, area_threshold=200)\n",
    "    \n",
    "    # Dilate to ensure full cloud coverage\n",
    "    selem_dilate = disk(5 if aggressive else 3)\n",
    "    cloud_mask_binary = morphology.dilation(cloud_mask_binary, selem_dilate)\n",
    "    \n",
    "    cloud_mask_final = (cloud_mask_binary * 255).astype(np.uint8)\n",
    "    \n",
    "    # === ADVANCED INPAINTING ===\n",
    "    \n",
    "    if np.sum(cloud_mask_final > 0) > 200:\n",
    "        # Method 1: Navier-Stokes (better for texture preservation)\n",
    "        inpainted_ns = cv2.inpaint(img_uint8, cloud_mask_final, 10, cv2.INPAINT_NS)\n",
    "        \n",
    "        # Method 2: Fast Marching (better for structure)\n",
    "        inpainted_fm = cv2.inpaint(img_uint8, cloud_mask_final, 7, cv2.INPAINT_TELEA)\n",
    "        \n",
    "        # Blend both methods\n",
    "        result = cv2.addWeighted(inpainted_ns, 0.6, inpainted_fm, 0.4, 0)\n",
    "        \n",
    "        # Apply bilateral filter for smooth transitions\n",
    "        result = cv2.bilateralFilter(result, 7, 75, 75)\n",
    "    else:\n",
    "        result = img_uint8\n",
    "    \n",
    "    return result.astype(np.float32) / 255.0, cloud_mask_final.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def advanced_deblurring(image, strength='high'):\n",
    "    \"\"\"\n",
    "    Advanced deblurring using multiple state-of-the-art methods\n",
    "    Proper weight normalization to preserve contrast\n",
    "    \"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # === METHOD 1: WIENER DECONVOLUTION ===\n",
    "    try:\n",
    "        # Create motion blur PSF\n",
    "        kernel_size = 11\n",
    "        psf = np.zeros((kernel_size, kernel_size))\n",
    "        psf[kernel_size // 2, :] = 1.0\n",
    "        psf = psf / psf.sum()\n",
    "        \n",
    "        # Apply Wiener deconvolution\n",
    "        deconvolved = np.zeros_like(image)\n",
    "        for c in range(3):\n",
    "            deconv_channel = restoration.wiener(image[:, :, c], psf, balance=0.05)\n",
    "            deconvolved[:, :, c] = np.clip(deconv_channel, 0, 1)\n",
    "        \n",
    "        deconv_uint8 = (deconvolved * 255).astype(np.uint8)\n",
    "    except:\n",
    "        deconv_uint8 = img_uint8\n",
    "    \n",
    "    # === METHOD 2: RICHARDSON-LUCY DECONVOLUTION ===\n",
    "    try:\n",
    "        rl_deconvolved = np.zeros_like(image)\n",
    "        for c in range(3):\n",
    "            rl_channel = restoration.richardson_lucy(image[:, :, c], psf, num_iter=15)\n",
    "            rl_deconvolved[:, :, c] = np.clip(rl_channel, 0, 1)\n",
    "        \n",
    "        rl_uint8 = (rl_deconvolved * 255).astype(np.uint8)\n",
    "    except:\n",
    "        rl_uint8 = img_uint8\n",
    "    \n",
    "    # === METHOD 3: ENHANCED UNSHARP MASKING ===\n",
    "    gaussian_blur = cv2.GaussianBlur(img_uint8, (9, 9), 2.0)\n",
    "    if strength == 'high':\n",
    "        unsharp = cv2.addWeighted(img_uint8, 2.0, gaussian_blur, -1.0, 0)\n",
    "    else:\n",
    "        unsharp = cv2.addWeighted(img_uint8, 1.8, gaussian_blur, -0.8, 0)\n",
    "    unsharp = np.clip(unsharp, 0, 255)\n",
    "    \n",
    "    # === METHOD 4: EDGE ENHANCEMENT ===\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Sobel edge detection\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    edges = np.sqrt(sobelx**2 + sobely**2)\n",
    "    edges = np.clip(edges, 0, 255).astype(np.uint8)\n",
    "    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    edge_enhanced = cv2.addWeighted(img_uint8, 1.0, edges_colored, 0.3, 0)\n",
    "    edge_enhanced = np.clip(edge_enhanced, 0, 255)\n",
    "    \n",
    "    # === METHOD 5: ADAPTIVE CLAHE ===\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    clahe_enhanced = np.zeros_like(img_uint8)\n",
    "    for c in range(3):\n",
    "        clahe_enhanced[:, :, c] = clahe.apply(img_uint8[:, :, c])\n",
    "    \n",
    "    # === BLEND ALL METHODS WITH NORMALIZED WEIGHTS ===\n",
    "    # Weights: Wiener (20%) + RL (15%) + Unsharp (35%) + Edge (15%) + CLAHE (15%) = 100%\n",
    "    result = (deconv_uint8.astype(np.float32) * 0.20 + \n",
    "              rl_uint8.astype(np.float32) * 0.15 + \n",
    "              unsharp.astype(np.float32) * 0.35 + \n",
    "              edge_enhanced.astype(np.float32) * 0.15 + \n",
    "              clahe_enhanced.astype(np.float32) * 0.15)\n",
    "    \n",
    "    result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # === FINAL CONTRAST ENHANCEMENT ===\n",
    "    # Apply adaptive histogram equalization to boost contrast\n",
    "    final_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    result_enhanced = np.zeros_like(result)\n",
    "    for c in range(3):\n",
    "        result_enhanced[:, :, c] = final_clahe.apply(result[:, :, c])\n",
    "    \n",
    "    return result_enhanced.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def calculate_quality_metrics(image):\n",
    "    \"\"\"Calculate image quality metrics with safe division\"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Sharpness (Laplacian variance)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    \n",
    "    # Contrast (standard deviation)\n",
    "    contrast = np.std(image)\n",
    "    \n",
    "    # Brightness\n",
    "    brightness = np.mean(image)\n",
    "    \n",
    "    # Edge density with minimum threshold to prevent divide by zero\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = max(np.sum(edges > 0) / edges.size, 1e-6)  # Minimum 1e-6 to prevent inf\n",
    "    \n",
    "    return {\n",
    "        'sharpness': laplacian_var,\n",
    "        'contrast': contrast,\n",
    "        'brightness': brightness,\n",
    "        'edge_density': edge_density\n",
    "    }\n",
    "\n",
    "print(\"Advanced preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply advanced preprocessing to both pre-event and post-event images\n",
    "print(\"Applying advanced preprocessing to BOTH pre-event and post-event images...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pre-event processing\n",
    "print(\"\\n[PRE-EVENT IMAGE]\")\n",
    "pre_degraded = create_synthetic_degraded_image(sample_data['pre_image'])\n",
    "pre_cloud_removed, pre_cloud_mask = advanced_cloud_removal(pre_degraded, aggressive=True)\n",
    "pre_enhanced = advanced_deblurring(pre_cloud_removed, strength='high')\n",
    "pre_cloud_cov = np.mean(pre_cloud_mask) * 100\n",
    "\n",
    "# Calculate metrics\n",
    "pre_orig_metrics = calculate_quality_metrics(sample_data['pre_image'])\n",
    "pre_deg_metrics = calculate_quality_metrics(pre_degraded)\n",
    "pre_enh_metrics = calculate_quality_metrics(pre_enhanced)\n",
    "\n",
    "print(f\"  Cloud coverage detected: {pre_cloud_cov:.1f}%\")\n",
    "print(f\"  Sharpness improvement: {((pre_enh_metrics['sharpness']/pre_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"  Contrast improvement:  {((pre_enh_metrics['contrast']/pre_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "\n",
    "# Post-event processing\n",
    "print(\"\\n[POST-EVENT IMAGE]\")\n",
    "post_degraded = create_synthetic_degraded_image(sample_data['post_image'])\n",
    "post_cloud_removed, post_cloud_mask = advanced_cloud_removal(post_degraded, aggressive=True)\n",
    "post_enhanced = advanced_deblurring(post_cloud_removed, strength='high')\n",
    "post_cloud_cov = np.mean(post_cloud_mask) * 100\n",
    "\n",
    "# Calculate metrics\n",
    "post_orig_metrics = calculate_quality_metrics(sample_data['post_image'])\n",
    "post_deg_metrics = calculate_quality_metrics(post_degraded)\n",
    "post_enh_metrics = calculate_quality_metrics(post_enhanced)\n",
    "\n",
    "print(f\"  Cloud coverage detected: {post_cloud_cov:.1f}%\")\n",
    "print(f\"  Sharpness improvement: {((post_enh_metrics['sharpness']/post_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"  Contrast improvement:  {((post_enh_metrics['contrast']/post_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Preprocessing complete for both images\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization: Pre-event & Post-event (Original -> Degraded -> Enhanced)\n",
    "fig, axes = plt.subplots(5, 3, figsize=(18, 30))\n",
    "\n",
    "# Column headers (Row 0)\n",
    "for ax in axes[0, :]:\n",
    "    ax.axis('off')\n",
    "\n",
    "axes[0, 0].text(0.5, 0.5, 'ORIGINAL\\n(Clean)', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='green',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "\n",
    "axes[0, 1].text(0.5, 0.5, 'DEGRADED\\n(Clouds + Blur)', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='red',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))\n",
    "\n",
    "axes[0, 2].text(0.5, 0.5, 'ENHANCED\\n(Preprocessed)', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='darkgreen',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "# === PRE-EVENT IMAGE (Row 1) ===\n",
    "axes[1, 0].imshow(sample_data['pre_image'])\n",
    "axes[1, 0].set_title(f'Pre-Event Original\\nSharpness: {pre_orig_metrics[\"sharpness\"]:.1f} | Contrast: {pre_orig_metrics[\"contrast\"]:.3f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(pre_degraded)\n",
    "axes[1, 1].set_title(f'Pre-Event Degraded\\nSharpness: {pre_deg_metrics[\"sharpness\"]:.1f} | Contrast: {pre_deg_metrics[\"contrast\"]:.3f}', \n",
    "                     fontsize=11, color='darkred', fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(pre_enhanced)\n",
    "axes[1, 2].set_title(f'Pre-Event Enhanced\\nSharpness: {pre_enh_metrics[\"sharpness\"]:.1f} (+{((pre_enh_metrics[\"sharpness\"]/pre_deg_metrics[\"sharpness\"]-1)*100):.0f}%) | Contrast: {pre_enh_metrics[\"contrast\"]:.3f} (+{((pre_enh_metrics[\"contrast\"]/pre_deg_metrics[\"contrast\"]-1)*100):.0f}%)', \n",
    "                     fontsize=11, color='darkgreen', fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# === PRE-EVENT DETAILS (Row 2) ===\n",
    "# Show cloud mask\n",
    "axes[2, 0].imshow(pre_cloud_mask, cmap='Reds', vmin=0, vmax=1)\n",
    "axes[2, 0].set_title(f'Pre-Event Cloud Mask\\n{pre_cloud_cov:.1f}% coverage detected', \n",
    "                     fontsize=11, color='red')\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "# Show cloud removed\n",
    "axes[2, 1].imshow(pre_cloud_removed)\n",
    "axes[2, 1].set_title('Pre-Event Cloud Removed\\n(Before deblurring)', fontsize=11)\n",
    "axes[2, 1].axis('off')\n",
    "\n",
    "# Show difference map\n",
    "diff_pre = np.abs(pre_degraded - pre_enhanced)\n",
    "axes[2, 2].imshow(diff_pre)\n",
    "axes[2, 2].set_title('Pre-Event Changes\\n(Difference Map)', fontsize=11)\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "# === POST-EVENT IMAGE (Row 3) ===\n",
    "axes[3, 0].imshow(sample_data['post_image'])\n",
    "axes[3, 0].set_title(f'Post-Event Original\\nSharpness: {post_orig_metrics[\"sharpness\"]:.1f} | Contrast: {post_orig_metrics[\"contrast\"]:.3f}', \n",
    "                     fontsize=11, fontweight='bold')\n",
    "axes[3, 0].axis('off')\n",
    "\n",
    "axes[3, 1].imshow(post_degraded)\n",
    "axes[3, 1].set_title(f'Post-Event Degraded\\nSharpness: {post_deg_metrics[\"sharpness\"]:.1f} | Contrast: {post_deg_metrics[\"contrast\"]:.3f}', \n",
    "                     fontsize=11, color='darkred', fontweight='bold')\n",
    "axes[3, 1].axis('off')\n",
    "\n",
    "axes[3, 2].imshow(post_enhanced)\n",
    "axes[3, 2].set_title(f'Post-Event Enhanced\\nSharpness: {post_enh_metrics[\"sharpness\"]:.1f} (+{((post_enh_metrics[\"sharpness\"]/post_deg_metrics[\"sharpness\"]-1)*100):.0f}%) | Contrast: {post_enh_metrics[\"contrast\"]:.3f} (+{((post_enh_metrics[\"contrast\"]/post_deg_metrics[\"contrast\"]-1)*100):.0f}%)', \n",
    "                     fontsize=11, color='darkgreen', fontweight='bold')\n",
    "axes[3, 2].axis('off')\n",
    "\n",
    "# === POST-EVENT DETAILS (Row 4) ===\n",
    "# Show cloud mask\n",
    "axes[4, 0].imshow(post_cloud_mask, cmap='Reds', vmin=0, vmax=1)\n",
    "axes[4, 0].set_title(f'Post-Event Cloud Mask\\n{post_cloud_cov:.1f}% coverage detected', \n",
    "                     fontsize=11, color='red')\n",
    "axes[4, 0].axis('off')\n",
    "\n",
    "# Show cloud removed\n",
    "axes[4, 1].imshow(post_cloud_removed)\n",
    "axes[4, 1].set_title('Post-Event Cloud Removed\\n(Before deblurring)', fontsize=11)\n",
    "axes[4, 1].axis('off')\n",
    "\n",
    "# Show difference map\n",
    "diff_post = np.abs(post_degraded - post_enhanced)\n",
    "axes[4, 2].imshow(diff_post)\n",
    "axes[4, 2].set_title('Post-Event Changes\\n(Difference Map)', fontsize=11)\n",
    "axes[4, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Advanced Preprocessing: Cloud Removal + Deblurring\\nOriginal → Synthetic Degradation → Enhanced (Pre & Post Event)', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary with safe percentage calculations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING EFFECTIVENESS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Safe percentage calculation function\n",
    "def safe_improvement(enhanced, degraded):\n",
    "    \"\"\"Calculate percentage improvement, handling zero/near-zero denominators\"\"\"\n",
    "    if degraded < 1e-6:\n",
    "        return 0.0\n",
    "    return ((enhanced / degraded - 1) * 100)\n",
    "\n",
    "print(\"\\nPRE-EVENT IMAGE:\")\n",
    "print(f\"    Sharpness improvement:    {safe_improvement(pre_enh_metrics['sharpness'], pre_deg_metrics['sharpness']):+.1f}%\")\n",
    "print(f\"    Contrast improvement:     {safe_improvement(pre_enh_metrics['contrast'], pre_deg_metrics['contrast']):+.1f}%\")\n",
    "print(f\"    Edge density improvement: {safe_improvement(pre_enh_metrics['edge_density'], pre_deg_metrics['edge_density']):+.1f}%\")\n",
    "print(f\"    Cloud coverage removed:   {pre_cloud_cov:.1f}%\")\n",
    "\n",
    "print(\"\\nPOST-EVENT IMAGE:\")\n",
    "print(f\"    Sharpness improvement:    {safe_improvement(post_enh_metrics['sharpness'], post_deg_metrics['sharpness']):+.1f}%\")\n",
    "print(f\"    Contrast improvement:     {safe_improvement(post_enh_metrics['contrast'], post_deg_metrics['contrast']):+.1f}%\")\n",
    "print(f\"    Edge density improvement: {safe_improvement(post_enh_metrics['edge_density'], post_deg_metrics['edge_density']):+.1f}%\")\n",
    "print(f\"    Cloud coverage removed:   {post_cloud_cov:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TECHNIQUES APPLIED:\")\n",
    "print(\"  - Multi-stage cloud detection (brightness + blue excess + texture + saturation + value)\")\n",
    "print(\"  - Morphological refinement (remove_small_objects + remove_small_holes + dilation)\")\n",
    "print(\"  - Dual inpainting (Navier-Stokes 60% + Telea 40%)\")\n",
    "print(\"  - Wiener deconvolution (20% weight)\")\n",
    "print(\"  - Richardson-Lucy deconvolution (15% weight)\")\n",
    "print(\"  - Enhanced unsharp masking (35% weight)\")\n",
    "print(\"  - Sobel edge enhancement (15% weight)\")\n",
    "print(\"  - Adaptive CLAHE (15% weight, clip=3.0)\")\n",
    "print(\"  - Final CLAHE enhancement (clip=2.0)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b31550",
   "metadata": {},
   "source": [
    "## 5. Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image quality\n",
    "quality_pre = preprocessor.check_image_quality(sample_data['pre_image'])\n",
    "quality_post = preprocessor.check_image_quality(sample_data['post_image'])\n",
    "\n",
    "print(\"Pre-Event Quality Metrics:\")\n",
    "for key, value in quality_pre.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nPost-Event Quality Metrics:\")\n",
    "for key, value in quality_post.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Visualize quality metrics\n",
    "metrics = ['valid_ratio', 'cloud_ratio', 'dark_ratio', 'mean_intensity', 'std_intensity']\n",
    "pre_values = [quality_pre[m] for m in metrics]\n",
    "post_values = [quality_post[m] for m in metrics]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, pre_values, width, label='Pre-Event', color='#3498db')\n",
    "ax.bar(x + width/2, post_values, width, label='Post-Event', color='#e74c3c')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Image Quality Metrics Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f0b77",
   "metadata": {},
   "source": [
    "## 6. Patch Extraction with Smart Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize patch extractor with updated threshold\n",
    "patch_extractor = PatchExtractor(\n",
    "    patch_size=PATCH_SIZE,\n",
    "    overlap=PATCH_OVERLAP,\n",
    "    min_flood_pixels=MIN_FLOOD_PIXELS  # Now 2621 pixels (~1% of patch)\n",
    ")\n",
    "\n",
    "print(f\"Patch extractor configuration:\")\n",
    "print(f\"  Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"  Overlap: {PATCH_OVERLAP}\")\n",
    "print(f\"  Min flood pixels: {MIN_FLOOD_PIXELS} ({(MIN_FLOOD_PIXELS/(PATCH_SIZE**2))*100:.2f}% of patch)\")\n",
    "\n",
    "# Concatenate pre and post images\n",
    "combined_image = np.concatenate([pre_enhanced, post_enhanced], axis=2)\n",
    "print(f\"\\nCombined image shape: {combined_image.shape} (6 channels: 3 pre + 3 post)\")\n",
    "\n",
    "# Extract patches (without oversampling for demonstration)\n",
    "patches = patch_extractor.extract_patches(\n",
    "    combined_image,\n",
    "    mask=sample_data['mask'],\n",
    "    oversample_flood=False  # Disabled to show true distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(patches)} patches\")\n",
    "\n",
    "# Count flood-positive patches\n",
    "flood_positive = [p for p in patches if p['is_flood_positive']]\n",
    "print(f\"  Flood-positive patches: {len(flood_positive)}\")\n",
    "print(f\"  Non-flood patches: {len(patches) - len(flood_positive)}\")\n",
    "print(f\"  Flood ratio: {len(flood_positive)/len(patches)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6371619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mask shape:\", sample_data['mask'].shape)\n",
    "print(\"Unique classes in mask:\", np.unique(sample_data['mask']))\n",
    "print(\"Mask value counts:\")\n",
    "unique, counts = np.unique(sample_data['mask'], return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    pct = (count / sample_data['mask'].size) * 100\n",
    "    print(f\"  Class {cls} ({CLASS_NAMES.get(cls, 'unknown')}): {count:,} pixels ({pct:.2f}%)\")\n",
    "\n",
    "# Check if mask has any flood-related classes (2, 3, 4, 5)\n",
    "flood_related_pixels = np.sum(sample_data['mask'] > 1)\n",
    "print(f\"\\nTotal flood-related pixels (class > 1): {flood_related_pixels:,}\")\n",
    "print(f\"Percentage: {(flood_related_pixels / sample_data['mask'].size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading different tiles to find one with mixed flood/non-flood patches\n",
    "print(\"Testing different tiles to find varied flood distribution:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx in range(min(5, len(germany_loader.get_tile_list()))):\n",
    "    tile_name = germany_loader.get_tile_list()[idx]\n",
    "    tile_data = load_tile_data(GERMANY_TRAIN, tile_name, 'Germany')\n",
    "    \n",
    "    # Calculate flood percentage\n",
    "    flood_px = np.sum((tile_data['mask'] == 2) | (tile_data['mask'] == 3) | (tile_data['mask'] == 4))\n",
    "    total_px = tile_data['mask'].size\n",
    "    flood_pct = (flood_px / total_px) * 100\n",
    "    \n",
    "    print(f\"\\nTile {idx}: {tile_name}\")\n",
    "    print(f\"  Flood pixels: {flood_px:,} ({flood_pct:.2f}%)\")\n",
    "    print(f\"  Classes present: {np.unique(tile_data['mask'])}\")\n",
    "    \n",
    "    # If this tile has moderate flooding (2-15%), use it for demo\n",
    "    if 2.0 <= flood_pct <= 15.0:\n",
    "        print(f\" Good candidate for mixed flood/non-flood patches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673aeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing patch extraction with different thresholds:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for threshold in [100, 2621, 5000, 10000, 20000]:\n",
    "    test_extractor = PatchExtractor(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        overlap=PATCH_OVERLAP,\n",
    "        min_flood_pixels=threshold\n",
    "    )\n",
    "    \n",
    "    test_patches = test_extractor.extract_patches(\n",
    "        combined_image,\n",
    "        mask=sample_data['mask'],\n",
    "        oversample_flood=True  # ENABLED: Production setting for balanced training\n",
    "    )\n",
    "    \n",
    "    flood_count = sum(1 for p in test_patches if p['is_flood_positive'])\n",
    "    non_flood_count = len(test_patches) - flood_count\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold} pixels ({(threshold/(PATCH_SIZE**2))*100:.2f}% of patch)\")\n",
    "    print(f\"  Total patches: {len(test_patches)}\")\n",
    "    print(f\"  Flood-positive: {flood_count}\")\n",
    "    print(f\"  Non-flood: {non_flood_count}\")\n",
    "    print(f\"  Flood ratio: {(flood_count/len(test_patches))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124835c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPatch-level flood analysis:\")\n",
    "for i, patch in enumerate(patches):\n",
    "    flood_px = patch['flood_pixels']\n",
    "    total_px = PATCH_SIZE * PATCH_SIZE\n",
    "    flood_pct = (flood_px / total_px) * 100\n",
    "    print(f\"  Patch {i}: {flood_px} flood pixels ({flood_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nPatch size: {PATCH_SIZE}x{PATCH_SIZE} = {PATCH_SIZE*PATCH_SIZE:,} pixels\")\n",
    "print(f\"Min flood pixels threshold: {patch_extractor.min_flood_pixels}\")\n",
    "print(f\"Min flood percentage needed: {(patch_extractor.min_flood_pixels / (PATCH_SIZE*PATCH_SIZE)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample patches\n",
    "n_samples = min(8, len(patches))\n",
    "sample_patches = np.random.choice(patches, n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(4, n_samples, figsize=(n_samples*3, 12))\n",
    "\n",
    "for i, patch in enumerate(sample_patches):\n",
    "    # Pre-event (first 3 channels)\n",
    "    pre_patch = patch['image'][:, :, :3]\n",
    "    axes[0, i].imshow(pre_patch)\n",
    "    axes[0, i].set_title(f\"Patch {i}\\nPre-Event\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Post-event (last 3 channels)\n",
    "    post_patch = patch['image'][:, :, 3:6]\n",
    "    axes[1, i].imshow(post_patch)\n",
    "    axes[1, i].set_title('Post-Event', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Mask\n",
    "    axes[2, i].imshow(patch['mask'], cmap='tab10', vmin=0, vmax=5)\n",
    "    axes[2, i].set_title('Mask', fontsize=10)\n",
    "    axes[2, i].axis('off')\n",
    "    \n",
    "    # Difference\n",
    "    diff_patch = np.abs(post_patch - pre_patch)\n",
    "    axes[3, i].imshow(diff_patch)\n",
    "    flood_status = 'FLOOD' if patch['is_flood_positive'] else 'OK'\n",
    "    axes[3, i].set_title(f\"Difference\\n{flood_status}\", fontsize=10)\n",
    "    axes[3, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feef5e5",
   "metadata": {},
   "source": [
    "## 7. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3743bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution across all patches\n",
    "class_totals = {i: 0 for i in range(7)}  # Classes 0-6\n",
    "\n",
    "for patch in patches:\n",
    "    for cls, count in patch.get('class_distribution', {}).items():\n",
    "        class_totals[int(cls)] += count\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "classes = list(class_totals.keys())\n",
    "counts = list(class_totals.values())\n",
    "labels = [f\"{CLASS_NAMES.get(c, f'Class {c}')}\\n{counts[i]:,}\" for i, c in enumerate(classes)]\n",
    "\n",
    "# Convert RGB colors (0-255) to hex format, normalizing to 0-1 range\n",
    "colors = []\n",
    "for cls in classes:\n",
    "    rgb = CLASS_COLORS.get(cls, [128, 128, 128])\n",
    "    # Normalize RGB values from 0-255 to 0-1\n",
    "    r, g, b = [val/255.0 for val in rgb]\n",
    "    colors.append((r, g, b))\n",
    "\n",
    "axes[0].pie(counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Class Distribution (Pixel Count)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "axes[1].bar(classes, counts, color=colors)\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Pixel Count', fontsize=12)\n",
    "axes[1].set_title('Class Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(classes)\n",
    "axes[1].set_xticklabels([CLASS_NAMES.get(c, f'C{c}') for c in classes], rotation=45, ha='right')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add counts on bars\n",
    "for i, (cls, count) in enumerate(zip(classes, counts)):\n",
    "    axes[1].text(cls, count, f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass Imbalance Summary:\")\n",
    "total_pixels = sum(counts)\n",
    "for cls, count in zip(classes, counts):\n",
    "    pct = (count / total_pixels) * 100\n",
    "    print(f\"  {CLASS_NAMES.get(cls, f'Class {cls}')}: {count:,} pixels ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12897b97",
   "metadata": {},
   "source": [
    "## 8. Data Augmentation Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training augmentation\n",
    "train_aug = get_training_augmentation(image_size=PATCH_SIZE)\n",
    "\n",
    "# Select a flood-positive patch for demonstration\n",
    "demo_patch = flood_positive[0] if len(flood_positive) > 0 else patches[0]\n",
    "demo_image = demo_patch['image'][:, :, :3]  # Use pre-event for demo\n",
    "demo_mask = demo_patch['mask']\n",
    "\n",
    "# Apply augmentation multiple times\n",
    "n_aug = 6\n",
    "fig, axes = plt.subplots(2, n_aug, figsize=(n_aug*3, 6))\n",
    "\n",
    "for i in range(n_aug):\n",
    "    augmented = train_aug(image=demo_image, mask=demo_mask)\n",
    "    \n",
    "    axes[0, i].imshow(augmented['image'])\n",
    "    axes[0, i].set_title(f'Aug {i+1} - Image', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(augmented['mask'], cmap='tab10', vmin=0, vmax=5)\n",
    "    axes[1, i].set_title(f'Aug {i+1} - Mask', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAugmentations include:\")\n",
    "print(\"  - Horizontal/Vertical flips\")\n",
    "print(\"  - Random rotations (90 degrees)\")\n",
    "print(\"  - Shift, scale, rotate\")\n",
    "print(\"  - Brightness/contrast adjustments\")\n",
    "print(\"  - Gaussian noise and blur\")\n",
    "print(\"  - Random fog and shadows\")\n",
    "print(\"  - Grid dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6f2a8",
   "metadata": {},
   "source": [
    "## 9. Run Full Preprocessing Pipeline\n",
    "\n",
    "This section runs the complete preprocessing pipeline on both regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ebe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full preprocessing pipeline\n",
    "print(\"Starting preprocessing pipeline...\")\n",
    "print(\"This may take 30-60 minutes depending on dataset size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77977da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous preprocessing output before running\n",
    "from preprocessing import cleanup_processed_data\n",
    "import config\n",
    "\n",
    "cleanup_processed_data(config.PROCESSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the preprocessing script\n",
    "# This will now:\n",
    "# 1. Process training data (Germany + Louisiana-East)\n",
    "# 2. Create train/val split (85%/15%)\n",
    "# 3. Process test data (Louisiana-West_Test_Public)\n",
    "\n",
    "if IS_COLAB:\n",
    "    %run src/run_preprocessing.py\n",
    "else:    \n",
    "    %run ../src/run_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88614c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data\n",
    "if IS_COLAB:\n",
    "    %run src/process_test_data.py\n",
    "else:    \n",
    "    %run ../src/process_test_data.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate class balance after preprocessing\n",
    "from preprocessing import validate_class_balance\n",
    "import config\n",
    "\n",
    "validate_class_balance(config.PROCESSED_DIR, config.NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813473ec",
   "metadata": {},
   "source": [
    "## 10. Verify Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f416b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check 1: Val folder has full-resolution processed images\n",
    "print(\"\\n1. Checking Val folder processed_images...\")\n",
    "val_processed = Path('../dataset/processed/val/processed_images')\n",
    "if val_processed.exists():\n",
    "    regions = [d.name for d in val_processed.iterdir() if d.is_dir()]\n",
    "    print(f\"   Found {len(regions)} region(s): {regions}\")\n",
    "    \n",
    "    for region in regions:\n",
    "        pre_dir = val_processed / region / 'PRE-event'\n",
    "        post_dir = val_processed / region / 'POST-event'\n",
    "        \n",
    "        if pre_dir.exists() and post_dir.exists():\n",
    "            pre_count = len(list(pre_dir.glob('*.tif')))\n",
    "            post_count = len(list(post_dir.glob('*.tif')))\n",
    "            print(f\"   {region}:\")\n",
    "            print(f\"      - PRE-event: {pre_count} images\")\n",
    "            print(f\"      - POST-event: {post_count} images\")\n",
    "            \n",
    "            if pre_count > 0:\n",
    "                # Show sample filenames\n",
    "                sample = list(pre_dir.glob('*.tif'))[0].name\n",
    "                print(f\"      - Sample: {sample}\")\n",
    "        else:\n",
    "            print(f\"   {region}: Missing PRE/POST directories\")\n",
    "else:\n",
    "    print(\"   Val processed_images directory not found!\")\n",
    "\n",
    "# Check 2: Test folder has Louisiana-West data\n",
    "print(\"\\n2. Checking Test folder for Louisiana-West data...\")\n",
    "test_processed = Path('../dataset/processed/test/processed_images')\n",
    "if test_processed.exists():\n",
    "    regions = [d.name for d in test_processed.iterdir() if d.is_dir()]\n",
    "    print(f\"   Found {len(regions)} region(s): {regions}\")\n",
    "    \n",
    "    for region in regions:\n",
    "        pre_dir = test_processed / region / 'PRE-event'\n",
    "        post_dir = test_processed / region / 'POST-event'\n",
    "        \n",
    "        if pre_dir.exists() and post_dir.exists():\n",
    "            pre_count = len(list(pre_dir.glob('*.tif')))\n",
    "            post_count = len(list(post_dir.glob('*.tif')))\n",
    "            print(f\"   {region}:\")\n",
    "            print(f\"      - PRE-event: {pre_count} images\")\n",
    "            print(f\"      - POST-event: {post_count} images\")\n",
    "            \n",
    "            if pre_count > 0:\n",
    "                # Show sample filenames\n",
    "                sample = list(pre_dir.glob('*.tif'))[0].name\n",
    "                print(f\"      - Sample: {sample}\")\n",
    "                \n",
    "                # Verify it's Louisiana-West data (different sensor IDs)\n",
    "                if 'Louisiana-West' in region:\n",
    "                    print(f\"      Correctly using Louisiana-West test data!\")\n",
    "                elif 'Germany' in region or 'Louisiana-East' in region:\n",
    "                    print(f\"      WARNING: Test should not contain training regions!\")\n",
    "        else:\n",
    "            print(f\"   {region}: Missing PRE/POST directories\")\n",
    "else:\n",
    "    print(\"   Test processed_images directory not found!\")\n",
    "\n",
    "# Check 3: Verify filenames match CSV\n",
    "print(\"\\n3. Verifying filenames match CSV mappings...\")\n",
    "import pandas as pd\n",
    "\n",
    "# Check Germany training\n",
    "csv_path = Path('../dataset/raw/train/Germany_Training_Public/Germany_Training_Public_label_image_mapping.csv')\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    sample_pre = df.iloc[0]['pre-event image']\n",
    "    sample_post = df.iloc[0]['post-event image 1']\n",
    "    \n",
    "    # Check if processed file exists with same name\n",
    "    processed_pre = Path('../dataset/processed/train/processed_images/Germany_Training_Public/PRE-event') / sample_pre\n",
    "    processed_post = Path('../dataset/processed/train/processed_images/Germany_Training_Public/POST-event') / sample_post\n",
    "    \n",
    "    print(f\"Germany sample from CSV:\")\n",
    "    print(f\" - PRE:  {sample_pre}\")\n",
    "    print(f\" - POST: {sample_post}\")\n",
    "    \n",
    "    if processed_pre.exists():\n",
    "        print(f\"Pre-event file found with exact name!\")\n",
    "    else:\n",
    "        print(f\"Pre-event file NOT found!\")\n",
    "    \n",
    "    if processed_post.exists():\n",
    "        print(f\"Post-event file found with exact name!\")\n",
    "    else:\n",
    "        print(f\"Post-event file NOT found!\")\n",
    "\n",
    "# Check Louisiana-West test\n",
    "csv_path_test = Path('../dataset/raw/test/Louisiana-West_Test_Public/Louisiana-West_Test_Public_label_image_mapping.csv')\n",
    "if csv_path_test.exists():\n",
    "    df_test = pd.read_csv(csv_path_test)\n",
    "    sample_pre_test = df_test.iloc[0]['pre-event image']\n",
    "    sample_post_test = df_test.iloc[0]['post-event image 1']\n",
    "    \n",
    "    # Check if processed file exists with same name\n",
    "    processed_pre_test = Path('../dataset/processed/test/processed_images/Louisiana-West_Test_Public/PRE-event') / sample_pre_test\n",
    "    processed_post_test = Path('../dataset/processed/test/processed_images/Louisiana-West_Test_Public/POST-event') / sample_post_test\n",
    "    \n",
    "    print(f\"\\nLouisiana-West sample from CSV:\")\n",
    "    print(f\" - PRE:  {sample_pre_test}\")\n",
    "    print(f\" - POST: {sample_post_test}\")\n",
    "    \n",
    "    if processed_pre_test.exists():\n",
    "        print(f\"Pre-event file found with exact name!\")\n",
    "    else:\n",
    "        print(f\"Pre-event file NOT found!\")\n",
    "    \n",
    "    if processed_post_test.exists():\n",
    "        print(f\"Post-event file found with exact name!\")\n",
    "    else:\n",
    "        print(f\"Post-event file NOT found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc6258",
   "metadata": {},
   "source": [
    "## 11. Export Metadata Files\n",
    "\n",
    "After preprocessing is complete, generate metadata files (JSON, Pickle, CSV) for all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metadata export script\n",
    "if IS_COLAB:\n",
    "    %run src/export_metadata.py\n",
    "else:    \n",
    "    %run ../src/export_metadata.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c8465",
   "metadata": {},
   "source": [
    "## 12. Validate Processed Data\n",
    "\n",
    "After preprocessing and metadata export are complete, validate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if processed data exists\n",
    "if PROCESSED_TRAIN_DIR.exists():\n",
    "    print(\"Processed data directory exists\")\n",
    "    \n",
    "    # Count files\n",
    "    train_images = list((PROCESSED_TRAIN_DIR / 'images').glob('*.npy'))\n",
    "    train_masks = list((PROCESSED_TRAIN_DIR / 'masks').glob('*.npy'))\n",
    "    \n",
    "    print(f\"\\nTraining set:\")\n",
    "    print(f\"  Images: {len(train_images)}\")\n",
    "    print(f\"  Masks: {len(train_masks)}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_path = PROCESSED_TRAIN_DIR / 'metadata' / 'train_metadata.json'\n",
    "    if metadata_path.exists():\n",
    "        import json\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"  Metadata entries: {len(metadata)}\")\n",
    "        \n",
    "        # Count flood-positive\n",
    "        flood_count = sum(1 for m in metadata if m['is_flood_positive'])\n",
    "        print(f\"  Flood-positive patches: {flood_count} ({flood_count/len(metadata)*100:.1f}%)\")\n",
    "    \n",
    "    # Load and display a sample\n",
    "    if len(train_images) > 0:\n",
    "        sample_img = np.load(train_images[0])\n",
    "        sample_mask = np.load(train_masks[0])\n",
    "        \n",
    "        print(f\"\\nSample patch:\")\n",
    "        print(f\"  Image shape: {sample_img.shape}\")\n",
    "        print(f\"  Mask shape: {sample_mask.shape}\")\n",
    "        print(f\"  Image range: [{sample_img.min():.3f}, {sample_img.max():.3f}]\")\n",
    "        print(f\"  Mask classes: {np.unique(sample_mask)}\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(sample_img[:, :, :3])  # Pre-event\n",
    "        axes[0].set_title('Pre-Event (Processed)', fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(sample_img[:, :, 3:6])  # Post-event\n",
    "        axes[1].set_title('Post-Event (Processed)', fontsize=12)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(sample_mask, cmap='tab10')\n",
    "        axes[2].set_title('Mask', fontsize=12)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Processed data not found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321313ec",
   "metadata": {},
   "source": [
    "## 13. Compare Raw vs Processed Full-Resolution Images\n",
    "\n",
    "Compare original raw images with processed full-resolution TIF images for both Germany and Louisiana datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Check if processed full-resolution images exist\n",
    "if IS_COLAB:\n",
    "    processed_base = Path('dataset/processed')\n",
    "else:\n",
    "    processed_base = Path('../dataset/processed')\n",
    "train_processed_images = processed_base / 'train' / 'processed_images'\n",
    "\n",
    "if not train_processed_images.exists():\n",
    "    print(\"Processed full-resolution images not found.\")\n",
    "    print(\"Run preprocessing pipeline first to generate processed images.\")\n",
    "else:\n",
    "    print(\"Processed images directory found\")\n",
    "    \n",
    "    # Get available regions\n",
    "    available_regions = [d.name for d in train_processed_images.iterdir() if d.is_dir()]\n",
    "    print(f\"Available regions: {available_regions}\")\n",
    "    \n",
    "    # Map to raw data directories (use actual directory names)\n",
    "    region_mapping = {\n",
    "        'Germany_Training_Public': GERMANY_TRAIN,\n",
    "        'Louisiana-East_Training_Public': LOUISIANA_EAST_TRAIN\n",
    "    }\n",
    "    \n",
    "    # Select 2 random tiles from each region\n",
    "    comparison_samples = []\n",
    "    \n",
    "    for region in available_regions:\n",
    "        if region in region_mapping:\n",
    "            raw_dir = region_mapping[region]\n",
    "            \n",
    "            # Load CSV mapping file\n",
    "            csv_name = f\"{region}_label_image_mapping.csv\"\n",
    "            csv_path = raw_dir / csv_name\n",
    "            \n",
    "            if not csv_path.exists():\n",
    "                print(f\"CSV mapping not found: {csv_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Read CSV to get pre/post image mappings\n",
    "            mapping_df = pd.read_csv(csv_path)\n",
    "            print(f\"\\nLoaded {len(mapping_df)} mappings from {csv_name}\")\n",
    "            \n",
    "            # Get list of processed PRE-event images\n",
    "            pre_processed_dir = train_processed_images / region / 'PRE-event'\n",
    "            if not pre_processed_dir.exists():\n",
    "                print(f\"No PRE-event processed images for {region}\")\n",
    "                continue\n",
    "            \n",
    "            processed_pre_files = list(pre_processed_dir.glob('*.tif'))\n",
    "            \n",
    "            if len(processed_pre_files) == 0:\n",
    "                print(f\"No TIF files found for {region}\")\n",
    "                continue\n",
    "            \n",
    "            # Select 2 random samples\n",
    "            n_samples = min(2, len(processed_pre_files))\n",
    "            selected_files = random.sample(processed_pre_files, n_samples)\n",
    "            \n",
    "            for pre_tif in selected_files:\n",
    "                # Processed files are named after the pre-event image\n",
    "                pre_image_name = pre_tif.name  # e.g., \"10500500C4DD7000_0_41_59.tif\"\n",
    "                \n",
    "                # Find matching row in CSV\n",
    "                matching_row = mapping_df[mapping_df['pre-event image'] == pre_image_name]\n",
    "                \n",
    "                if matching_row.empty:\n",
    "                    print(f\"No CSV mapping found for: {pre_image_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get post-event image name from CSV\n",
    "                post_image_name = matching_row.iloc[0]['post-event image 1']\n",
    "                \n",
    "                # Paths\n",
    "                raw_pre_path = raw_dir / 'PRE-event' / pre_image_name\n",
    "                raw_post_path = raw_dir / 'POST-event' / post_image_name\n",
    "                # Processed POST-event images are saved with their original POST-event filenames\n",
    "                processed_post_path = train_processed_images / region / 'POST-event' / post_image_name\n",
    "                \n",
    "                if all([p.exists() for p in [raw_pre_path, raw_post_path, processed_post_path]]):\n",
    "                    comparison_samples.append({\n",
    "                        'region': region,\n",
    "                        'tile': pre_tif.stem,\n",
    "                        'raw_pre': raw_pre_path,\n",
    "                        'raw_post': raw_post_path,\n",
    "                        'processed_pre': pre_tif,\n",
    "                        'processed_post': processed_post_path\n",
    "                    })\n",
    "                    print(f\"Found complete set: {pre_tif.stem}\")\n",
    "                else:\n",
    "                    missing = []\n",
    "                    if not raw_pre_path.exists(): missing.append(\"raw_pre\")\n",
    "                    if not raw_post_path.exists(): missing.append(\"raw_post\")\n",
    "                    if not processed_post_path.exists(): missing.append(\"processed_post\")\n",
    "                    print(f\"Missing files for {pre_tif.stem}: {', '.join(missing)}\")\n",
    "\n",
    "    print(f\"\\nFound {len(comparison_samples)} complete sample sets for comparison\")\n",
    "    for sample in comparison_samples:\n",
    "        print(f\"  - {sample['region']}: {sample['tile']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc682c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tif_image(path):\n",
    "    \"\"\"Load TIF image and convert from uint16 to float32\"\"\"\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {path}\")\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert uint16 [0, 65535] to float32 [0, 1]\n",
    "    img_float = img.astype(np.float32) / 65535.0\n",
    "    return img_float\n",
    "\n",
    "\n",
    "def load_raw_png_image(path):\n",
    "    \"\"\"Load raw PNG image\"\"\"\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {path}\")\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to float32 [0, 1]\n",
    "    img_float = img.astype(np.float32) / 255.0\n",
    "    return img_float\n",
    "\n",
    "\n",
    "# Visualize all comparison samples\n",
    "if len(comparison_samples) > 0:\n",
    "    n_samples = len(comparison_samples)\n",
    "    \n",
    "    # Create figure with subplots: 4 columns (Raw Pre, Processed Pre, Raw Post, Processed Post) x n_samples rows\n",
    "    fig, axes = plt.subplots(n_samples, 4, figsize=(20, 5*n_samples))\n",
    "    \n",
    "    # Handle single sample case (axes won't be 2D)\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, sample in enumerate(comparison_samples):\n",
    "        try:\n",
    "            # Load images\n",
    "            raw_pre = load_raw_png_image(sample['raw_pre'])\n",
    "            raw_post = load_raw_png_image(sample['raw_post'])\n",
    "            processed_pre = load_tif_image(sample['processed_pre'])\n",
    "            processed_post = load_tif_image(sample['processed_post'])\n",
    "            \n",
    "            # Display images\n",
    "            axes[idx, 0].imshow(raw_pre)\n",
    "            axes[idx, 0].set_title(f\"{sample['region']}\\n{sample['tile']}\\nRaw PRE-event\", \n",
    "                                   fontsize=11, fontweight='bold')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            axes[idx, 1].imshow(processed_pre)\n",
    "            axes[idx, 1].set_title(f\"Processed PRE-event\\n(CLAHE + Cloud Removal + Deblur)\", \n",
    "                                   fontsize=11, fontweight='bold', color='darkgreen')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            axes[idx, 2].imshow(raw_post)\n",
    "            axes[idx, 2].set_title(f\"Raw POST-event\", \n",
    "                                   fontsize=11, fontweight='bold')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            axes[idx, 3].imshow(processed_post)\n",
    "            axes[idx, 3].set_title(f\"Processed POST-event\\n(CLAHE + Cloud Removal + Deblur)\", \n",
    "                                   fontsize=11, fontweight='bold', color='darkgreen')\n",
    "            axes[idx, 3].axis('off')\n",
    "            \n",
    "            # Calculate quality improvements\n",
    "            pre_raw_metrics = calculate_quality_metrics(raw_pre)\n",
    "            pre_proc_metrics = calculate_quality_metrics(processed_pre)\n",
    "            post_raw_metrics = calculate_quality_metrics(raw_post)\n",
    "            post_proc_metrics = calculate_quality_metrics(processed_post)\n",
    "            \n",
    "            print(f\"\\n{sample['region']} - {sample['tile']}:\")\n",
    "            print(f\"  PRE-event improvements:\")\n",
    "            print(f\"    Sharpness: {pre_raw_metrics['sharpness']:.1f} → {pre_proc_metrics['sharpness']:.1f} \"\n",
    "                  f\"({((pre_proc_metrics['sharpness']/pre_raw_metrics['sharpness']-1)*100):+.1f}%)\")\n",
    "            print(f\"    Contrast:  {pre_raw_metrics['contrast']:.3f} → {pre_proc_metrics['contrast']:.3f} \"\n",
    "                  f\"({((pre_proc_metrics['contrast']/pre_raw_metrics['contrast']-1)*100):+.1f}%)\")\n",
    "            \n",
    "            print(f\"  POST-event improvements:\")\n",
    "            print(f\"    Sharpness: {post_raw_metrics['sharpness']:.1f} → {post_proc_metrics['sharpness']:.1f} \"\n",
    "                  f\"({((post_proc_metrics['sharpness']/post_raw_metrics['sharpness']-1)*100):+.1f}%)\")\n",
    "            print(f\"    Contrast:  {post_raw_metrics['contrast']:.3f} → {post_proc_metrics['contrast']:.3f} \"\n",
    "                  f\"({((post_proc_metrics['contrast']/post_raw_metrics['contrast']-1)*100):+.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {sample['tile']}: {e}\")\n",
    "            for col in range(4):\n",
    "                axes[idx, col].text(0.5, 0.5, 'Error loading image', \n",
    "                                    ha='center', va='center', color='red')\n",
    "                axes[idx, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('Raw vs Processed Full-Resolution Images Comparison\\n(Germany & Louisiana-East Datasets)', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPROCESSING EFFECTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"CLAHE Enhancement: Improved local contrast and visibility\")\n",
    "    print(\"Cloud Removal: Multi-stage detection and advanced inpainting\")\n",
    "    print(\"Deblurring: Wiener + Richardson-Lucy + Unsharp masking + Edge enhancement\")\n",
    "    print(\"Format: Saved as TIF (uint16) for quality preservation\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n No comparison samples available. Run preprocessing pipeline first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
