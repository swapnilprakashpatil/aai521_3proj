{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IS_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Google Colab Environment Specifications:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get system info\n",
    "    \n",
    "    print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Architecture: {platform.machine()}\")\n",
    "    print(f\"Python Version: {platform.python_version()}\")\n",
    "    \n",
    "    # Memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "    \n",
    "    # GPU info\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            gpu_info = result.stdout.strip().split('\\n')\n",
    "            for i, gpu in enumerate(gpu_info):\n",
    "                name, memory = gpu.split(', ')\n",
    "                print(f\"GPU {i}: {name}, {memory} MB VRAM\")\n",
    "        else:\n",
    "            print(\"GPU: Not detected or nvidia-smi unavailable\")\n",
    "    except:\n",
    "        print(\"GPU: Not detected\")\n",
    "    \n",
    "    # Disk space\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Space: {disk.free / (1024**3):.1f} GB free / {disk.total / (1024**3):.1f} GB total\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if not os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"WARNING: Cloning project repository required.\")\n",
    "        print(\"=\"*50)\n",
    "else:\n",
    "    print(\"Not running in Google Colab environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    if os.path.exists('/content/aai521_3proj'):\n",
    "        print(\"Repository already exists. Pulling latest changes...\")\n",
    "        %cd /content/aai521_3proj\n",
    "        !git pull\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/swapnilprakashpatil/aai521_3proj.git\n",
    "        %cd aai521_3proj    \n",
    "    %pip install -r requirements.txt --quiet\n",
    "    sys.path.append('/content/aai521_3proj/src')\n",
    "    %ls\n",
    "else:\n",
    "    print(\"Running in local environment. Installing packages...\")\n",
    "    %pip install -r ../requirements.txt --quiet\n",
    "    sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "if 'config' in sys.modules:\n",
    "    importlib.reload(sys.modules['config'])\n",
    "if 'data_loader' in sys.modules:\n",
    "    importlib.reload(sys.modules['data_loader'])\n",
    "if 'preprocessing' in sys.modules:\n",
    "    importlib.reload(sys.modules['preprocessing'])\n",
    "if 'augmentation' in sys.modules:\n",
    "    importlib.reload(sys.modules['augmentation'])\n",
    "if 'visualizations' in sys.modules:\n",
    "    importlib.reload(sys.modules['visualizations'])\n",
    "\n",
    "# Import custom modules\n",
    "from config import (\n",
    "    GERMANY_TRAIN, LOUISIANA_EAST_TRAIN,\n",
    "    PROCESSED_TRAIN_DIR, CLASS_NAMES, CLASS_COLORS,\n",
    "    PATCH_SIZE, PATCH_OVERLAP, MIN_FLOOD_PIXELS, SELECTED_REGIONS\n",
    ")\n",
    "\n",
    "from data_loader import DatasetLoader, load_tile_data\n",
    "from preprocessing import ImagePreprocessor, PatchExtractor\n",
    "from augmentation import get_training_augmentation, DualImageAugmentation\n",
    "from visualizations import (\n",
    "    plot_flood_statistics,\n",
    "    plot_class_distribution,\n",
    "    plot_augmentation_samples,\n",
    "    plot_tile_overview,\n",
    "    plot_clahe_comparison,\n",
    "    plot_advanced_preprocessing,\n",
    "    plot_patch_samples,\n",
    "    plot_processed_sample\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Configuration loaded: MIN_FLOOD_PIXELS = {MIN_FLOOD_PIXELS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "germany_loader = DatasetLoader(GERMANY_TRAIN, 'Germany')\n",
    "louisiana_loader = DatasetLoader(LOUISIANA_EAST_TRAIN, 'Louisiana-East')\n",
    "\n",
    "print(f\"Germany tiles: {len(germany_loader.get_tile_list())}\")\n",
    "print(f\"Louisiana-East tiles: {len(louisiana_loader.get_tile_list())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "germany_stats = germany_loader.get_flood_statistics()\n",
    "louisiana_stats = louisiana_loader.get_flood_statistics()\n",
    "\n",
    "stats_df = pd.DataFrame({\n",
    "    'Germany': germany_stats,\n",
    "    'Louisiana-East': louisiana_stats\n",
    "}).T\n",
    "\n",
    "print(stats_df)\n",
    "\n",
    "fig = plot_flood_statistics(germany_stats, louisiana_stats)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample tile from Germany\n",
    "sample_tile_name = germany_loader.get_tile_list()[0]\n",
    "print(f\"Loading sample tile: {sample_tile_name}\")\n",
    "\n",
    "sample_data = load_tile_data(GERMANY_TRAIN, sample_tile_name, 'Germany')\n",
    "\n",
    "print(f\"\\nTile information:\")\n",
    "print(f\"  Pre-image shape: {sample_data['pre_image'].shape}\")\n",
    "print(f\"  Post-image shape: {sample_data['post_image'].shape}\")\n",
    "print(f\"  Mask shape: {sample_data['mask'].shape}\")\n",
    "print(f\"  Pre-image dtype: {sample_data['pre_metadata']['dtype']}\")\n",
    "print(f\"  Pre-image range: [{sample_data['pre_image'].min():.3f}, {sample_data['pre_image'].max():.3f}]\")\n",
    "\n",
    "# Check mask classes\n",
    "unique_classes = np.unique(sample_data['mask'])\n",
    "print(f\"\\nMask classes present: {unique_classes}\")\n",
    "for cls in unique_classes:\n",
    "    count = np.sum(sample_data['mask'] == cls)\n",
    "    pct = (count / sample_data['mask'].size) * 100\n",
    "    print(f\"  Class {cls} ({CLASS_NAMES.get(cls, 'unknown')}): {count} pixels ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769beb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tile_overview(sample_data, CLASS_NAMES)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ImagePreprocessor(\n",
    "    apply_clahe=True,\n",
    "    clahe_clip_limit=2.0,\n",
    "    clahe_tile_grid_size=(8, 8)\n",
    ")\n",
    "\n",
    "pre_enhanced = preprocessor.apply_clahe_enhancement(sample_data['pre_image'])\n",
    "post_enhanced = preprocessor.apply_clahe_enhancement(sample_data['post_image'])\n",
    "\n",
    "fig = plot_clahe_comparison(sample_data, pre_enhanced, post_enhanced)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCLAHE Enhancement Applied:\")\n",
    "print(\"  - Improves local contrast\")\n",
    "print(\"  - Better visibility of flood boundaries\")\n",
    "print(\"  - Histogram equalization in tiles (8x8)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced image processing libraries\n",
    "import sys\n",
    "try:\n",
    "    from skimage import morphology, filters, exposure, restoration, transform\n",
    "    from skimage.filters import rank, gaussian\n",
    "    from skimage.morphology import disk, remove_small_objects, remove_small_holes\n",
    "    from scipy import ndimage\n",
    "    from scipy.signal import convolve2d\n",
    "    print(\"Advanced image processing libraries loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Installing required libraries: {e}\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"scikit-image\", \"scipy\"])\n",
    "    from skimage import morphology, filters, exposure, restoration, transform\n",
    "    from skimage.filters import rank, gaussian\n",
    "    from skimage.morphology import disk, remove_small_objects, remove_small_holes\n",
    "    from scipy import ndimage\n",
    "    from scipy.signal import convolve2d\n",
    "    print(\"Libraries installed and loaded\")\n",
    "\n",
    "print(\"\\nAdvanced preprocessing methods:\")\n",
    "print(\"  1. Multi-stage cloud detection (brightness + texture + saturation)\")\n",
    "print(\"  2. Morphological cloud refinement\")\n",
    "print(\"  3. Advanced inpainting (Navier-Stokes + Telea)\")\n",
    "print(\"  4. Wiener deconvolution for deblurring\")\n",
    "print(\"  5. Richardson-Lucy deconvolution\")\n",
    "print(\"  6. Unsharp masking with adaptive strength\")\n",
    "print(\"  7. CLAHE enhancement per channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_degraded_image(clean_image):\n",
    "    \"\"\"\n",
    "    Create a moderately degraded version to demonstrate preprocessing capabilities\n",
    "    Adds realistic clouds, haze, and blur while preserving some edge structure\n",
    "    \"\"\"\n",
    "    degraded = clean_image.copy()\n",
    "    h, w = degraded.shape[:2]\n",
    "    \n",
    "    # 1. Add atmospheric haze (reduces contrast and adds blue tint) - REDUCED\n",
    "    haze_strength = 0.3  # Reduced from 0.5 to preserve more edges\n",
    "    haze_color = np.array([0.7, 0.75, 0.85])  # Blueish-white\n",
    "    degraded = degraded * (1 - haze_strength) + haze_color * haze_strength\n",
    "    \n",
    "    # 2. Add realistic cloud patches - FEWER AND LIGHTER\n",
    "    num_clouds = np.random.randint(5, 10)  # Reduced from 8-15\n",
    "    for _ in range(num_clouds):\n",
    "        # Random cloud center\n",
    "        cx, cy = np.random.randint(0, w), np.random.randint(0, h)\n",
    "        \n",
    "        # Cloud size - SMALLER\n",
    "        cloud_w = np.random.randint(60, 150)  # Reduced from 80-200\n",
    "        cloud_h = np.random.randint(40, 120)  # Reduced from 60-150\n",
    "        \n",
    "        # Create cloud mask with soft edges (Gaussian falloff)\n",
    "        y_coords, x_coords = np.ogrid[:h, :w]\n",
    "        cloud_mask = np.exp(-((x_coords - cx)**2 / (2 * cloud_w**2) + \n",
    "                             (y_coords - cy)**2 / (2 * cloud_h**2)))\n",
    "        \n",
    "        # Cloud color (bright white with slight variation)\n",
    "        cloud_color = np.array([0.85, 0.88, 0.95]) + np.random.uniform(-0.05, 0.05, 3)\n",
    "        cloud_opacity = np.random.uniform(0.3, 0.6)  # Reduced from 0.5-0.9 for lighter clouds\n",
    "        \n",
    "        # Blend cloud\n",
    "        for c in range(3):\n",
    "            degraded[:, :, c] = (degraded[:, :, c] * (1 - cloud_mask * cloud_opacity) + \n",
    "                                cloud_color[c] * cloud_mask * cloud_opacity)\n",
    "    \n",
    "    # 3. Add motion blur (simulating camera/satellite motion) - REDUCED\n",
    "    kernel_size = 9  # Reduced from 15\n",
    "    motion_kernel = np.zeros((kernel_size, kernel_size))\n",
    "    motion_kernel[kernel_size // 2, :] = 1.0 / kernel_size\n",
    "    \n",
    "    blurred = np.zeros_like(degraded)\n",
    "    for c in range(3):\n",
    "        blurred[:, :, c] = convolve2d(degraded[:, :, c], motion_kernel, mode='same', boundary='symm')\n",
    "    degraded = blurred\n",
    "    \n",
    "    # 4. Add Gaussian noise - REDUCED\n",
    "    noise = np.random.normal(0, 0.02, degraded.shape)  # Reduced from 0.03\n",
    "    degraded = degraded + noise\n",
    "    \n",
    "    # 5. Reduce overall sharpness - LESS AGGRESSIVE\n",
    "    degraded = gaussian(degraded, sigma=1.0, channel_axis=2)  # Reduced from 1.5\n",
    "    \n",
    "    return np.clip(degraded, 0, 1)\n",
    "\n",
    "\n",
    "def advanced_cloud_removal(image, aggressive=True):\n",
    "    \"\"\"\n",
    "    State-of-the-art cloud detection and removal\n",
    "    \"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    h, w = img_uint8.shape[:2]\n",
    "    \n",
    "    # === MULTI-STAGE CLOUD DETECTION ===\n",
    "    \n",
    "    # Stage 1: Brightness analysis\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    if aggressive:\n",
    "        bright_mask = gray > 160  # Lower threshold for more detection\n",
    "    else:\n",
    "        bright_mask = gray > 180\n",
    "    \n",
    "    # Stage 2: Blue channel analysis (clouds are blue-white)\n",
    "    blue_excess = img_uint8[:, :, 2].astype(float) - (img_uint8[:, :, 0].astype(float) + img_uint8[:, :, 1].astype(float)) / 2\n",
    "    blue_mask = blue_excess > 10\n",
    "    \n",
    "    # Stage 3: Texture analysis (clouds have uniform texture)\n",
    "    selem = disk(7)\n",
    "    entropy_img = rank.entropy(gray, selem)\n",
    "    texture_mask = entropy_img < np.percentile(entropy_img, 25)\n",
    "    \n",
    "    # Stage 4: Saturation analysis (clouds have low saturation)\n",
    "    hsv = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2HSV)\n",
    "    low_sat_mask = hsv[:, :, 1] < 40\n",
    "    \n",
    "    # Stage 5: Value analysis (clouds are bright in HSV)\n",
    "    high_value_mask = hsv[:, :, 2] > 200\n",
    "    \n",
    "    # Combine all stages\n",
    "    cloud_mask = (bright_mask & blue_mask) | (bright_mask & low_sat_mask & texture_mask) | (high_value_mask & low_sat_mask)\n",
    "    cloud_mask = cloud_mask.astype(np.uint8) * 255\n",
    "    \n",
    "    # === MORPHOLOGICAL REFINEMENT ===\n",
    "    \n",
    "    # Remove small false positives\n",
    "    cloud_mask_binary = cloud_mask > 0\n",
    "    cloud_mask_binary = remove_small_objects(cloud_mask_binary, min_size=100, connectivity=2)\n",
    "    cloud_mask_binary = remove_small_holes(cloud_mask_binary, area_threshold=200)\n",
    "    \n",
    "    # Dilate to ensure full cloud coverage\n",
    "    selem_dilate = disk(5 if aggressive else 3)\n",
    "    cloud_mask_binary = morphology.dilation(cloud_mask_binary, selem_dilate)\n",
    "    \n",
    "    cloud_mask_final = (cloud_mask_binary * 255).astype(np.uint8)\n",
    "    \n",
    "    # === ADVANCED INPAINTING ===\n",
    "    \n",
    "    if np.sum(cloud_mask_final > 0) > 200:\n",
    "        # Method 1: Navier-Stokes (better for texture preservation)\n",
    "        inpainted_ns = cv2.inpaint(img_uint8, cloud_mask_final, 10, cv2.INPAINT_NS)\n",
    "        \n",
    "        # Method 2: Fast Marching (better for structure)\n",
    "        inpainted_fm = cv2.inpaint(img_uint8, cloud_mask_final, 7, cv2.INPAINT_TELEA)\n",
    "        \n",
    "        # Blend both methods\n",
    "        result = cv2.addWeighted(inpainted_ns, 0.6, inpainted_fm, 0.4, 0)\n",
    "        \n",
    "        # Apply bilateral filter for smooth transitions\n",
    "        result = cv2.bilateralFilter(result, 7, 75, 75)\n",
    "    else:\n",
    "        result = img_uint8\n",
    "    \n",
    "    return result.astype(np.float32) / 255.0, cloud_mask_final.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def advanced_deblurring(image, strength='high'):\n",
    "    \"\"\"\n",
    "    Advanced deblurring using multiple state-of-the-art methods\n",
    "    Proper weight normalization to preserve contrast\n",
    "    \"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # === METHOD 1: WIENER DECONVOLUTION ===\n",
    "    try:\n",
    "        # Create motion blur PSF\n",
    "        kernel_size = 11\n",
    "        psf = np.zeros((kernel_size, kernel_size))\n",
    "        psf[kernel_size // 2, :] = 1.0\n",
    "        psf = psf / psf.sum()\n",
    "        \n",
    "        # Apply Wiener deconvolution\n",
    "        deconvolved = np.zeros_like(image)\n",
    "        for c in range(3):\n",
    "            deconv_channel = restoration.wiener(image[:, :, c], psf, balance=0.05)\n",
    "            deconvolved[:, :, c] = np.clip(deconv_channel, 0, 1)\n",
    "        \n",
    "        deconv_uint8 = (deconvolved * 255).astype(np.uint8)\n",
    "    except:\n",
    "        deconv_uint8 = img_uint8\n",
    "    \n",
    "    # === METHOD 2: RICHARDSON-LUCY DECONVOLUTION ===\n",
    "    try:\n",
    "        rl_deconvolved = np.zeros_like(image)\n",
    "        for c in range(3):\n",
    "            rl_channel = restoration.richardson_lucy(image[:, :, c], psf, num_iter=15)\n",
    "            rl_deconvolved[:, :, c] = np.clip(rl_channel, 0, 1)\n",
    "        \n",
    "        rl_uint8 = (rl_deconvolved * 255).astype(np.uint8)\n",
    "    except:\n",
    "        rl_uint8 = img_uint8\n",
    "    \n",
    "    # === METHOD 3: ENHANCED UNSHARP MASKING ===\n",
    "    gaussian_blur = cv2.GaussianBlur(img_uint8, (9, 9), 2.0)\n",
    "    if strength == 'high':\n",
    "        unsharp = cv2.addWeighted(img_uint8, 2.0, gaussian_blur, -1.0, 0)\n",
    "    else:\n",
    "        unsharp = cv2.addWeighted(img_uint8, 1.8, gaussian_blur, -0.8, 0)\n",
    "    unsharp = np.clip(unsharp, 0, 255)\n",
    "    \n",
    "    # === METHOD 4: EDGE ENHANCEMENT ===\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Sobel edge detection\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    edges = np.sqrt(sobelx**2 + sobely**2)\n",
    "    edges = np.clip(edges, 0, 255).astype(np.uint8)\n",
    "    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    edge_enhanced = cv2.addWeighted(img_uint8, 1.0, edges_colored, 0.3, 0)\n",
    "    edge_enhanced = np.clip(edge_enhanced, 0, 255)\n",
    "    \n",
    "    # === METHOD 5: ADAPTIVE CLAHE ===\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    clahe_enhanced = np.zeros_like(img_uint8)\n",
    "    for c in range(3):\n",
    "        clahe_enhanced[:, :, c] = clahe.apply(img_uint8[:, :, c])\n",
    "    \n",
    "    # === BLEND ALL METHODS WITH NORMALIZED WEIGHTS ===\n",
    "    # Weights: Wiener (20%) + RL (15%) + Unsharp (35%) + Edge (15%) + CLAHE (15%) = 100%\n",
    "    result = (deconv_uint8.astype(np.float32) * 0.20 + \n",
    "              rl_uint8.astype(np.float32) * 0.15 + \n",
    "              unsharp.astype(np.float32) * 0.35 + \n",
    "              edge_enhanced.astype(np.float32) * 0.15 + \n",
    "              clahe_enhanced.astype(np.float32) * 0.15)\n",
    "    \n",
    "    result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # === FINAL CONTRAST ENHANCEMENT ===\n",
    "    # Apply adaptive histogram equalization to boost contrast\n",
    "    final_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    result_enhanced = np.zeros_like(result)\n",
    "    for c in range(3):\n",
    "        result_enhanced[:, :, c] = final_clahe.apply(result[:, :, c])\n",
    "    \n",
    "    return result_enhanced.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def calculate_quality_metrics(image):\n",
    "    \"\"\"Calculate image quality metrics with safe division\"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Sharpness (Laplacian variance)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    \n",
    "    # Contrast (standard deviation)\n",
    "    contrast = np.std(image)\n",
    "    \n",
    "    # Brightness\n",
    "    brightness = np.mean(image)\n",
    "    \n",
    "    # Edge density with minimum threshold to prevent divide by zero\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = max(np.sum(edges > 0) / edges.size, 1e-6)  # Minimum 1e-6 to prevent inf\n",
    "    \n",
    "    return {\n",
    "        'sharpness': laplacian_var,\n",
    "        'contrast': contrast,\n",
    "        'brightness': brightness,\n",
    "        'edge_density': edge_density\n",
    "    }\n",
    "\n",
    "print(\"Advanced preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply advanced preprocessing to both pre-event and post-event images\n",
    "print(\"Applying advanced preprocessing to BOTH pre-event and post-event images...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pre-event processing\n",
    "print(\"\\n[PRE-EVENT IMAGE]\")\n",
    "pre_degraded = create_synthetic_degraded_image(sample_data['pre_image'])\n",
    "pre_cloud_removed, pre_cloud_mask = advanced_cloud_removal(pre_degraded, aggressive=True)\n",
    "pre_enhanced = advanced_deblurring(pre_cloud_removed, strength='high')\n",
    "pre_cloud_cov = np.mean(pre_cloud_mask) * 100\n",
    "\n",
    "# Calculate metrics\n",
    "pre_orig_metrics = calculate_quality_metrics(sample_data['pre_image'])\n",
    "pre_deg_metrics = calculate_quality_metrics(pre_degraded)\n",
    "pre_enh_metrics = calculate_quality_metrics(pre_enhanced)\n",
    "\n",
    "print(f\"  Cloud coverage detected: {pre_cloud_cov:.1f}%\")\n",
    "print(f\"  Sharpness improvement: {((pre_enh_metrics['sharpness']/pre_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"  Contrast improvement:  {((pre_enh_metrics['contrast']/pre_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "\n",
    "# Post-event processing\n",
    "print(\"\\n[POST-EVENT IMAGE]\")\n",
    "post_degraded = create_synthetic_degraded_image(sample_data['post_image'])\n",
    "post_cloud_removed, post_cloud_mask = advanced_cloud_removal(post_degraded, aggressive=True)\n",
    "post_enhanced = advanced_deblurring(post_cloud_removed, strength='high')\n",
    "post_cloud_cov = np.mean(post_cloud_mask) * 100\n",
    "\n",
    "# Calculate metrics\n",
    "post_orig_metrics = calculate_quality_metrics(sample_data['post_image'])\n",
    "post_deg_metrics = calculate_quality_metrics(post_degraded)\n",
    "post_enh_metrics = calculate_quality_metrics(post_enhanced)\n",
    "\n",
    "print(f\"  Cloud coverage detected: {post_cloud_cov:.1f}%\")\n",
    "print(f\"  Sharpness improvement: {((post_enh_metrics['sharpness']/post_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"  Contrast improvement:  {((post_enh_metrics['contrast']/post_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Preprocessing complete for both images\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metrics for visualization\n",
    "pre_metrics_dict = {\n",
    "    'orig': pre_orig_metrics,\n",
    "    'degraded': pre_deg_metrics,\n",
    "    'enhanced': pre_enh_metrics\n",
    "}\n",
    "\n",
    "post_metrics_dict = {\n",
    "    'orig': post_orig_metrics,\n",
    "    'degraded': post_deg_metrics,\n",
    "    'enhanced': post_enh_metrics\n",
    "}\n",
    "\n",
    "fig = plot_advanced_preprocessing(\n",
    "    sample_data, pre_degraded, pre_enhanced, pre_cloud_removed, pre_cloud_mask,\n",
    "    post_degraded, post_enhanced, post_cloud_removed, post_cloud_mask,\n",
    "    pre_metrics_dict, post_metrics_dict\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary with safe percentage calculations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING EFFECTIVENESS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Safe percentage calculation function\n",
    "def safe_improvement(enhanced, degraded):\n",
    "    \"\"\"Calculate percentage improvement, handling zero/near-zero denominators\"\"\"\n",
    "    if degraded < 1e-6:\n",
    "        return 0.0\n",
    "    return ((enhanced / degraded - 1) * 100)\n",
    "\n",
    "pre_cloud_cov = np.mean(pre_cloud_mask) * 100\n",
    "post_cloud_cov = np.mean(post_cloud_mask) * 100\n",
    "\n",
    "print(\"\\nPRE-EVENT IMAGE:\")\n",
    "print(f\"    Sharpness improvement:    {safe_improvement(pre_enh_metrics['sharpness'], pre_deg_metrics['sharpness']):+.1f}%\")\n",
    "print(f\"    Contrast improvement:     {safe_improvement(pre_enh_metrics['contrast'], pre_deg_metrics['contrast']):+.1f}%\")\n",
    "print(f\"    Edge density improvement: {safe_improvement(pre_enh_metrics['edge_density'], pre_deg_metrics['edge_density']):+.1f}%\")\n",
    "print(f\"    Cloud coverage removed:   {pre_cloud_cov:.1f}%\")\n",
    "\n",
    "print(\"\\nPOST-EVENT IMAGE:\")\n",
    "print(f\"    Sharpness improvement:    {safe_improvement(post_enh_metrics['sharpness'], post_deg_metrics['sharpness']):+.1f}%\")\n",
    "print(f\"    Contrast improvement:     {safe_improvement(post_enh_metrics['contrast'], post_deg_metrics['contrast']):+.1f}%\")\n",
    "print(f\"    Edge density improvement: {safe_improvement(post_enh_metrics['edge_density'], post_deg_metrics['edge_density']):+.1f}%\")\n",
    "print(f\"    Cloud coverage removed:   {post_cloud_cov:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TECHNIQUES APPLIED:\")\n",
    "print(\"  - Multi-stage cloud detection (brightness + blue excess + texture + saturation + value)\")\n",
    "print(\"  - Morphological refinement (remove_small_objects + remove_small_holes + dilation)\")\n",
    "print(\"  - Dual inpainting (Navier-Stokes 60% + Telea 40%)\")\n",
    "print(\"  - Wiener deconvolution (20% weight)\")\n",
    "print(\"  - Richardson-Lucy deconvolution (15% weight)\")\n",
    "print(\"  - Enhanced unsharp masking (35% weight)\")\n",
    "print(\"  - Sobel edge enhancement (15% weight)\")\n",
    "print(\"  - Adaptive CLAHE (15% weight, clip=3.0)\")\n",
    "print(\"  - Final CLAHE enhancement (clip=2.0)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image quality\n",
    "quality_pre = preprocessor.check_image_quality(sample_data['pre_image'])\n",
    "quality_post = preprocessor.check_image_quality(sample_data['post_image'])\n",
    "\n",
    "print(\"Pre-Event Quality Metrics:\")\n",
    "for key, value in quality_pre.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nPost-Event Quality Metrics:\")\n",
    "for key, value in quality_post.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Visualize quality metrics\n",
    "metrics = ['valid_ratio', 'cloud_ratio', 'dark_ratio', 'mean_intensity', 'std_intensity']\n",
    "pre_values = [quality_pre[m] for m in metrics]\n",
    "post_values = [quality_post[m] for m in metrics]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, pre_values, width, label='Pre-Event', color='#3498db')\n",
    "ax.bar(x + width/2, post_values, width, label='Post-Event', color='#e74c3c')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Image Quality Metrics Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize patch extractor with updated threshold\n",
    "patch_extractor = PatchExtractor(\n",
    "    patch_size=PATCH_SIZE,\n",
    "    overlap=PATCH_OVERLAP,\n",
    "    min_flood_pixels=MIN_FLOOD_PIXELS  # Now 2621 pixels (~1% of patch)\n",
    ")\n",
    "\n",
    "print(f\"Patch extractor configuration:\")\n",
    "print(f\"  Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"  Overlap: {PATCH_OVERLAP}\")\n",
    "print(f\"  Min flood pixels: {MIN_FLOOD_PIXELS} ({(MIN_FLOOD_PIXELS/(PATCH_SIZE**2))*100:.2f}% of patch)\")\n",
    "\n",
    "# Concatenate pre and post images\n",
    "combined_image = np.concatenate([pre_enhanced, post_enhanced], axis=2)\n",
    "print(f\"\\nCombined image shape: {combined_image.shape} (6 channels: 3 pre + 3 post)\")\n",
    "\n",
    "# Extract patches (without oversampling for demonstration)\n",
    "patches = patch_extractor.extract_patches(\n",
    "    combined_image,\n",
    "    mask=sample_data['mask'],\n",
    "    oversample_flood=False  # Disabled to show true distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(patches)} patches\")\n",
    "\n",
    "# Count flood-positive patches\n",
    "flood_positive = [p for p in patches if p['is_flood_positive']]\n",
    "print(f\"  Flood-positive patches: {len(flood_positive)}\")\n",
    "print(f\"  Non-flood patches: {len(patches) - len(flood_positive)}\")\n",
    "print(f\"  Flood ratio: {len(flood_positive)/len(patches)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6371619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mask shape:\", sample_data['mask'].shape)\n",
    "print(\"Unique classes in mask:\", np.unique(sample_data['mask']))\n",
    "print(\"Mask value counts:\")\n",
    "unique, counts = np.unique(sample_data['mask'], return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    pct = (count / sample_data['mask'].size) * 100\n",
    "    print(f\"  Class {cls} ({CLASS_NAMES.get(cls, 'unknown')}): {count:,} pixels ({pct:.2f}%)\")\n",
    "\n",
    "# Check if mask has any flood-related classes (2, 3, 4, 5)\n",
    "flood_related_pixels = np.sum(sample_data['mask'] > 1)\n",
    "print(f\"\\nTotal flood-related pixels (class > 1): {flood_related_pixels:,}\")\n",
    "print(f\"Percentage: {(flood_related_pixels / sample_data['mask'].size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading different tiles to find one with mixed flood/non-flood patches\n",
    "print(\"Testing different tiles to find varied flood distribution:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx in range(min(5, len(germany_loader.get_tile_list()))):\n",
    "    tile_name = germany_loader.get_tile_list()[idx]\n",
    "    tile_data = load_tile_data(GERMANY_TRAIN, tile_name, 'Germany')\n",
    "    \n",
    "    # Calculate flood percentage\n",
    "    flood_px = np.sum((tile_data['mask'] == 2) | (tile_data['mask'] == 3) | (tile_data['mask'] == 4))\n",
    "    total_px = tile_data['mask'].size\n",
    "    flood_pct = (flood_px / total_px) * 100\n",
    "    \n",
    "    print(f\"\\nTile {idx}: {tile_name}\")\n",
    "    print(f\"  Flood pixels: {flood_px:,} ({flood_pct:.2f}%)\")\n",
    "    print(f\"  Classes present: {np.unique(tile_data['mask'])}\")\n",
    "    \n",
    "    # If this tile has moderate flooding (2-15%), use it for demo\n",
    "    if 2.0 <= flood_pct <= 15.0:\n",
    "        print(f\" Good candidate for mixed flood/non-flood patches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673aeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing patch extraction with different thresholds:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for threshold in [100, 2621, 5000, 10000, 20000]:\n",
    "    test_extractor = PatchExtractor(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        overlap=PATCH_OVERLAP,\n",
    "        min_flood_pixels=threshold\n",
    "    )\n",
    "    \n",
    "    test_patches = test_extractor.extract_patches(\n",
    "        combined_image,\n",
    "        mask=sample_data['mask'],\n",
    "        oversample_flood=True  # ENABLED: Production setting for balanced training\n",
    "    )\n",
    "    \n",
    "    flood_count = sum(1 for p in test_patches if p['is_flood_positive'])\n",
    "    non_flood_count = len(test_patches) - flood_count\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold} pixels ({(threshold/(PATCH_SIZE**2))*100:.2f}% of patch)\")\n",
    "    print(f\"  Total patches: {len(test_patches)}\")\n",
    "    print(f\"  Flood-positive: {flood_count}\")\n",
    "    print(f\"  Non-flood: {non_flood_count}\")\n",
    "    print(f\"  Flood ratio: {(flood_count/len(test_patches))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124835c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPatch-level flood analysis:\")\n",
    "for i, patch in enumerate(patches):\n",
    "    flood_px = patch['flood_pixels']\n",
    "    total_px = PATCH_SIZE * PATCH_SIZE\n",
    "    flood_pct = (flood_px / total_px) * 100\n",
    "    print(f\"  Patch {i}: {flood_px} flood pixels ({flood_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nPatch size: {PATCH_SIZE}x{PATCH_SIZE} = {PATCH_SIZE*PATCH_SIZE:,} pixels\")\n",
    "print(f\"Min flood pixels threshold: {patch_extractor.min_flood_pixels}\")\n",
    "print(f\"Min flood percentage needed: {(patch_extractor.min_flood_pixels / (PATCH_SIZE*PATCH_SIZE)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_patch_samples(patches, n_samples=8, patch_size=PATCH_SIZE)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3743bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_totals = {i: 0 for i in range(7)}\n",
    "\n",
    "for patch in patches:\n",
    "    for cls, count in patch.get('class_distribution', {}).items():\n",
    "        class_totals[int(cls)] += count\n",
    "\n",
    "fig = plot_class_distribution(patches, CLASS_NAMES, CLASS_COLORS)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = get_training_augmentation(image_size=PATCH_SIZE)\n",
    "\n",
    "demo_patch = flood_positive[0] if len(flood_positive) > 0 else patches[0]\n",
    "demo_image = demo_patch['image'][:, :, :3]\n",
    "demo_mask = demo_patch['mask']\n",
    "\n",
    "fig = plot_augmentation_samples(demo_image, demo_mask, train_aug, n_samples=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77977da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run full preprocessing pipeline\n",
    "print(\"Starting preprocessing pipeline...\")\n",
    "\n",
    "# Clean up previous preprocessing output before running\n",
    "from preprocessing import cleanup_processed_data\n",
    "import config\n",
    "\n",
    "cleanup_processed_data(config.PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the preprocessing script\n",
    "# This will now:\n",
    "# 1. Process training data (Germany + Louisiana-East)\n",
    "# 2. Create train/val split (85%/15%)\n",
    "# 3. Process test data (Louisiana-West_Test_Public)\n",
    "\n",
    "if IS_COLAB:\n",
    "    %run src/run_preprocessing.py\n",
    "else:    \n",
    "    %run ../src/run_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88614c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data\n",
    "if IS_COLAB:\n",
    "    %run src/process_test_data.py\n",
    "else:    \n",
    "    %run ../src/process_test_data.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate class balance after preprocessing\n",
    "from preprocessing import validate_class_balance\n",
    "import config\n",
    "\n",
    "validate_class_balance(config.PROCESSED_DIR, config.NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f416b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path('../dataset/processed')\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    split_dir = base_dir / split\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    \n",
    "    images_dir = split_dir / 'images'\n",
    "    masks_dir = split_dir / 'masks'\n",
    "    if images_dir.exists() and masks_dir.exists():\n",
    "        n_images = len(list(images_dir.glob('*.npy')))\n",
    "        n_masks = len(list(masks_dir.glob('*.npy')))\n",
    "        print(f\"  Patches: {n_images} images, {n_masks} masks\")\n",
    "    \n",
    "    processed_images_dir = split_dir / 'processed_images'\n",
    "    if processed_images_dir.exists():\n",
    "        regions = [d.name for d in processed_images_dir.iterdir() if d.is_dir()]\n",
    "        for region in regions:\n",
    "            pre_count = len(list((processed_images_dir / region / 'PRE-event').glob('*.tif')))\n",
    "            post_count = len(list((processed_images_dir / region / 'POST-event').glob('*.tif')))\n",
    "            print(f\"  {region}: {pre_count} PRE, {post_count} POST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metadata export script\n",
    "if IS_COLAB:\n",
    "    %run src/export_metadata.py\n",
    "else:    \n",
    "    %run ../src/export_metadata.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESSED_TRAIN_DIR.exists():\n",
    "    print(\"Processed data directory exists\")\n",
    "    \n",
    "    train_images = list((PROCESSED_TRAIN_DIR / 'images').glob('*.npy'))\n",
    "    train_masks = list((PROCESSED_TRAIN_DIR / 'masks').glob('*.npy'))\n",
    "    \n",
    "    print(f\"\\nTraining set:\")\n",
    "    print(f\"  Images: {len(train_images)}\")\n",
    "    print(f\"  Masks: {len(train_masks)}\")\n",
    "    \n",
    "    metadata_path = PROCESSED_TRAIN_DIR / 'metadata' / 'train_metadata.json'\n",
    "    if metadata_path.exists():\n",
    "        import json\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"  Metadata entries: {len(metadata)}\")\n",
    "        \n",
    "        flood_count = sum(1 for m in metadata if m['is_flood_positive'])\n",
    "        print(f\"  Flood-positive patches: {flood_count} ({flood_count/len(metadata)*100:.1f}%)\")\n",
    "    \n",
    "    if len(train_images) > 0:\n",
    "        sample_img = np.load(train_images[0])\n",
    "        sample_mask = np.load(train_masks[0])\n",
    "        \n",
    "        print(f\"\\nSample patch:\")\n",
    "        print(f\"  Image shape: {sample_img.shape}\")\n",
    "        print(f\"  Mask shape: {sample_mask.shape}\")\n",
    "        print(f\"  Image range: [{sample_img.min():.3f}, {sample_img.max():.3f}]\")\n",
    "        print(f\"  Mask classes: {np.unique(sample_mask)}\")\n",
    "        \n",
    "        fig = plot_processed_sample(sample_img, sample_mask)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Processed data not found. Run preprocessing first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Check if processed full-resolution images exist\n",
    "if IS_COLAB:\n",
    "    processed_base = Path('dataset/processed')\n",
    "else:\n",
    "    processed_base = Path('../dataset/processed')\n",
    "train_processed_images = processed_base / 'train' / 'processed_images'\n",
    "\n",
    "if not train_processed_images.exists():\n",
    "    print(\"Processed full-resolution images not found.\")\n",
    "    print(\"Run preprocessing pipeline first to generate processed images.\")\n",
    "else:\n",
    "    print(\"Processed images directory found\")\n",
    "    \n",
    "    # Get available regions\n",
    "    available_regions = [d.name for d in train_processed_images.iterdir() if d.is_dir()]\n",
    "    print(f\"Available regions: {available_regions}\")\n",
    "    \n",
    "    # Map to raw data directories (use actual directory names)\n",
    "    region_mapping = {\n",
    "        'Germany_Training_Public': GERMANY_TRAIN,\n",
    "        'Louisiana-East_Training_Public': LOUISIANA_EAST_TRAIN\n",
    "    }\n",
    "    \n",
    "    # Select 2 random tiles from each region\n",
    "    comparison_samples = []\n",
    "    \n",
    "    for region in available_regions:\n",
    "        if region in region_mapping:\n",
    "            raw_dir = region_mapping[region]\n",
    "            \n",
    "            # Load CSV mapping file\n",
    "            csv_name = f\"{region}_label_image_mapping.csv\"\n",
    "            csv_path = raw_dir / csv_name\n",
    "            \n",
    "            if not csv_path.exists():\n",
    "                print(f\"CSV mapping not found: {csv_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Read CSV to get pre/post image mappings\n",
    "            mapping_df = pd.read_csv(csv_path)\n",
    "            print(f\"\\nLoaded {len(mapping_df)} mappings from {csv_name}\")\n",
    "            \n",
    "            # Get list of processed PRE-event images\n",
    "            pre_processed_dir = train_processed_images / region / 'PRE-event'\n",
    "            if not pre_processed_dir.exists():\n",
    "                print(f\"No PRE-event processed images for {region}\")\n",
    "                continue\n",
    "            \n",
    "            processed_pre_files = list(pre_processed_dir.glob('*.tif'))\n",
    "            \n",
    "            if len(processed_pre_files) == 0:\n",
    "                print(f\"No TIF files found for {region}\")\n",
    "                continue\n",
    "            \n",
    "            # Select 2 random samples\n",
    "            n_samples = min(2, len(processed_pre_files))\n",
    "            selected_files = random.sample(processed_pre_files, n_samples)\n",
    "            \n",
    "            for pre_tif in selected_files:\n",
    "                # Processed files are named after the pre-event image\n",
    "                pre_image_name = pre_tif.name  # e.g., \"10500500C4DD7000_0_41_59.tif\"\n",
    "                \n",
    "                # Find matching row in CSV\n",
    "                matching_row = mapping_df[mapping_df['pre-event image'] == pre_image_name]\n",
    "                \n",
    "                if matching_row.empty:\n",
    "                    print(f\"No CSV mapping found for: {pre_image_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get post-event image name from CSV\n",
    "                post_image_name = matching_row.iloc[0]['post-event image 1']\n",
    "                \n",
    "                # Paths\n",
    "                raw_pre_path = raw_dir / 'PRE-event' / pre_image_name\n",
    "                raw_post_path = raw_dir / 'POST-event' / post_image_name\n",
    "                # Processed POST-event images are saved with their original POST-event filenames\n",
    "                processed_post_path = train_processed_images / region / 'POST-event' / post_image_name\n",
    "                \n",
    "                if all([p.exists() for p in [raw_pre_path, raw_post_path, processed_post_path]]):\n",
    "                    comparison_samples.append({\n",
    "                        'region': region,\n",
    "                        'tile': pre_tif.stem,\n",
    "                        'raw_pre': raw_pre_path,\n",
    "                        'raw_post': raw_post_path,\n",
    "                        'processed_pre': pre_tif,\n",
    "                        'processed_post': processed_post_path\n",
    "                    })\n",
    "                    print(f\"Found complete set: {pre_tif.stem}\")\n",
    "                else:\n",
    "                    missing = []\n",
    "                    if not raw_pre_path.exists(): missing.append(\"raw_pre\")\n",
    "                    if not raw_post_path.exists(): missing.append(\"raw_post\")\n",
    "                    if not processed_post_path.exists(): missing.append(\"processed_post\")\n",
    "                    print(f\"Missing files for {pre_tif.stem}: {', '.join(missing)}\")\n",
    "\n",
    "    print(f\"\\nFound {len(comparison_samples)} complete sample sets for comparison\")\n",
    "    for sample in comparison_samples:\n",
    "        print(f\"  - {sample['region']}: {sample['tile']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc682c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tif_image(path):\n",
    "    \"\"\"Load TIF image and convert from uint16 to float32\"\"\"\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {path}\")\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert uint16 [0, 65535] to float32 [0, 1]\n",
    "    img_float = img.astype(np.float32) / 65535.0\n",
    "    return img_float\n",
    "\n",
    "\n",
    "def load_raw_png_image(path):\n",
    "    \"\"\"Load raw PNG image\"\"\"\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {path}\")\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to float32 [0, 1]\n",
    "    img_float = img.astype(np.float32) / 255.0\n",
    "    return img_float\n",
    "\n",
    "\n",
    "# Visualize all comparison samples\n",
    "if len(comparison_samples) > 0:\n",
    "    n_samples = len(comparison_samples)\n",
    "    \n",
    "    # Create figure with subplots: 4 columns (Raw Pre, Processed Pre, Raw Post, Processed Post) x n_samples rows\n",
    "    fig, axes = plt.subplots(n_samples, 4, figsize=(20, 5*n_samples))\n",
    "    \n",
    "    # Handle single sample case (axes won't be 2D)\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, sample in enumerate(comparison_samples):\n",
    "        try:\n",
    "            # Load images\n",
    "            raw_pre = load_raw_png_image(sample['raw_pre'])\n",
    "            raw_post = load_raw_png_image(sample['raw_post'])\n",
    "            processed_pre = load_tif_image(sample['processed_pre'])\n",
    "            processed_post = load_tif_image(sample['processed_post'])\n",
    "            \n",
    "            # Display images\n",
    "            axes[idx, 0].imshow(raw_pre)\n",
    "            axes[idx, 0].set_title(f\"{sample['region']}\\n{sample['tile']}\\nRaw PRE-event\", \n",
    "                                   fontsize=11, fontweight='bold')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            axes[idx, 1].imshow(processed_pre)\n",
    "            axes[idx, 1].set_title(f\"Processed PRE-event\\n(CLAHE + Cloud Removal + Deblur)\", \n",
    "                                   fontsize=11, fontweight='bold', color='darkgreen')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            axes[idx, 2].imshow(raw_post)\n",
    "            axes[idx, 2].set_title(f\"Raw POST-event\", \n",
    "                                   fontsize=11, fontweight='bold')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            axes[idx, 3].imshow(processed_post)\n",
    "            axes[idx, 3].set_title(f\"Processed POST-event\\n(CLAHE + Cloud Removal + Deblur)\", \n",
    "                                   fontsize=11, fontweight='bold', color='darkgreen')\n",
    "            axes[idx, 3].axis('off')\n",
    "            \n",
    "            # Calculate quality improvements\n",
    "            pre_raw_metrics = calculate_quality_metrics(raw_pre)\n",
    "            pre_proc_metrics = calculate_quality_metrics(processed_pre)\n",
    "            post_raw_metrics = calculate_quality_metrics(raw_post)\n",
    "            post_proc_metrics = calculate_quality_metrics(processed_post)\n",
    "            \n",
    "            print(f\"\\n{sample['region']} - {sample['tile']}:\")\n",
    "            print(f\"  PRE-event improvements:\")\n",
    "            print(f\"    Sharpness: {pre_raw_metrics['sharpness']:.1f}  {pre_proc_metrics['sharpness']:.1f} \"\n",
    "                  f\"({((pre_proc_metrics['sharpness']/pre_raw_metrics['sharpness']-1)*100):+.1f}%)\")\n",
    "            print(f\"    Contrast:  {pre_raw_metrics['contrast']:.3f}  {pre_proc_metrics['contrast']:.3f} \"\n",
    "                  f\"({((pre_proc_metrics['contrast']/pre_raw_metrics['contrast']-1)*100):+.1f}%)\")\n",
    "            \n",
    "            print(f\"  POST-event improvements:\")\n",
    "            print(f\"    Sharpness: {post_raw_metrics['sharpness']:.1f}  {post_proc_metrics['sharpness']:.1f} \"\n",
    "                  f\"({((post_proc_metrics['sharpness']/post_raw_metrics['sharpness']-1)*100):+.1f}%)\")\n",
    "            print(f\"    Contrast:  {post_raw_metrics['contrast']:.3f}  {post_proc_metrics['contrast']:.3f} \"\n",
    "                  f\"({((post_proc_metrics['contrast']/post_raw_metrics['contrast']-1)*100):+.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {sample['tile']}: {e}\")\n",
    "            for col in range(4):\n",
    "                axes[idx, col].text(0.5, 0.5, 'Error loading image', \n",
    "                                    ha='center', va='center', color='red')\n",
    "                axes[idx, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('Raw vs Processed Full-Resolution Images Comparison\\n(Germany & Louisiana-East Datasets)', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPROCESSING EFFECTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"CLAHE Enhancement: Improved local contrast and visibility\")\n",
    "    print(\"Cloud Removal: Multi-stage detection and advanced inpainting\")\n",
    "    print(\"Deblurring: Wiener + Richardson-Lucy + Unsharp masking + Edge enhancement\")\n",
    "    print(\"Format: Saved as TIF (uint16) for quality preservation\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n No comparison samples available. Run preprocessing pipeline first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
