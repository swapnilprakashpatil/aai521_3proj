{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "if 'config' in sys.modules:\n",
    "    importlib.reload(sys.modules['config'])\n",
    "if 'data_loader' in sys.modules:\n",
    "    importlib.reload(sys.modules['data_loader'])\n",
    "if 'preprocessing' in sys.modules:\n",
    "    importlib.reload(sys.modules['preprocessing'])\n",
    "if 'augmentation' in sys.modules:\n",
    "    importlib.reload(sys.modules['augmentation'])\n",
    "\n",
    "# Import custom modules\n",
    "from config import (\n",
    "    GERMANY_TRAIN, LOUISIANA_EAST_TRAIN,\n",
    "    PROCESSED_TRAIN_DIR, CLASS_NAMES, CLASS_COLORS,\n",
    "    PATCH_SIZE, PATCH_OVERLAP, MIN_FLOOD_PIXELS\n",
    ")\n",
    "\n",
    "from data_loader import DatasetLoader, load_tile_data\n",
    "from preprocessing import ImagePreprocessor, PatchExtractor\n",
    "from augmentation import get_training_augmentation, DualImageAugmentation\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Configuration loaded: MIN_FLOOD_PIXELS = {MIN_FLOOD_PIXELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984c033",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset loaders\n",
    "germany_loader = DatasetLoader(GERMANY_TRAIN, 'Germany')\n",
    "louisiana_loader = DatasetLoader(LOUISIANA_EAST_TRAIN, 'Louisiana-East')\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"  Germany tiles: {len(germany_loader.get_tile_list())}\")\n",
    "print(f\"  Louisiana-East tiles: {len(louisiana_loader.get_tile_list())}\")\n",
    "print(f\"  Total tiles: {len(germany_loader.get_tile_list()) + len(louisiana_loader.get_tile_list())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flood statistics\n",
    "germany_stats = germany_loader.get_flood_statistics()\n",
    "louisiana_stats = louisiana_loader.get_flood_statistics()\n",
    "\n",
    "# Create comparison dataframe\n",
    "stats_df = pd.DataFrame({\n",
    "    'Germany': germany_stats,\n",
    "    'Louisiana-East': louisiana_stats\n",
    "}).T\n",
    "\n",
    "print(\"\\nFlood Statistics by Region:\")\n",
    "print(stats_df)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Flooded vs Non-flooded counts\n",
    "regions = ['Germany', 'Louisiana-East']\n",
    "flooded = [germany_stats['flooded_count'], louisiana_stats['flooded_count']]\n",
    "non_flooded = [germany_stats['non_flooded_count'], louisiana_stats['non_flooded_count']]\n",
    "\n",
    "x = np.arange(len(regions))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, flooded, width, label='Flooded', color='#e74c3c')\n",
    "axes[0].bar(x + width/2, non_flooded, width, label='Non-flooded', color='#2ecc71')\n",
    "axes[0].set_xlabel('Region')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Flooded vs Non-flooded Road Segments')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(regions)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentage breakdown\n",
    "flooded_pct = [germany_stats['flooded_pct'], louisiana_stats['flooded_pct']]\n",
    "non_flooded_pct = [germany_stats['non_flooded_pct'], louisiana_stats['non_flooded_pct']]\n",
    "\n",
    "axes[1].bar(x, flooded_pct, width, label='Flooded %', color='#e74c3c')\n",
    "axes[1].bar(x, non_flooded_pct, width, bottom=flooded_pct, label='Non-flooded %', color='#2ecc71')\n",
    "axes[1].set_xlabel('Region')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].set_title('Class Distribution (Percentage)')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(regions)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Imbalance Detected:\")\n",
    "print(f\"   Average flooded ratio: {np.mean(flooded_pct):.1f}%\")\n",
    "print(f\"   Addressed through:\")\n",
    "print(f\"     - Oversampling flood-positive patches\")\n",
    "print(f\"     - Class-weighted loss functions\")\n",
    "print(f\"     - Focused augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37123a7f",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Sample Tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample tile from Germany\n",
    "sample_tile_name = germany_loader.get_tile_list()[0]\n",
    "print(f\"Loading sample tile: {sample_tile_name}\")\n",
    "\n",
    "sample_data = load_tile_data(GERMANY_TRAIN, sample_tile_name, 'Germany')\n",
    "\n",
    "print(f\"\\nTile information:\")\n",
    "print(f\"  Pre-image shape: {sample_data['pre_image'].shape}\")\n",
    "print(f\"  Post-image shape: {sample_data['post_image'].shape}\")\n",
    "print(f\"  Mask shape: {sample_data['mask'].shape}\")\n",
    "print(f\"  Pre-image dtype: {sample_data['pre_metadata']['dtype']}\")\n",
    "print(f\"  Pre-image range: [{sample_data['pre_image'].min():.3f}, {sample_data['pre_image'].max():.3f}]\")\n",
    "\n",
    "# Check mask classes\n",
    "unique_classes = np.unique(sample_data['mask'])\n",
    "print(f\"\\nMask classes present: {unique_classes}\")\n",
    "for cls in unique_classes:\n",
    "    count = np.sum(sample_data['mask'] == cls)\n",
    "    pct = (count / sample_data['mask'].size) * 100\n",
    "    print(f\"  Class {cls} ({CLASS_NAMES.get(cls, 'unknown')}): {count} pixels ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769beb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original tile\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "\n",
    "# Pre-event\n",
    "axes[0, 0].imshow(sample_data['pre_image'])\n",
    "axes[0, 0].set_title('Pre-Event Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Post-event\n",
    "axes[0, 1].imshow(sample_data['post_image'])\n",
    "axes[0, 1].set_title('Post-Event Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Mask\n",
    "axes[1, 0].imshow(sample_data['mask'], cmap='tab10')\n",
    "axes[1, 0].set_title('Segmentation Mask', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Difference\n",
    "diff = np.abs(sample_data['post_image'] - sample_data['pre_image'])\n",
    "axes[1, 1].imshow(diff)\n",
    "axes[1, 1].set_title('Temporal Difference (|Post - Pre|)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44339f",
   "metadata": {},
   "source": [
    "## 4. Image Enhancement with CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = ImagePreprocessor(\n",
    "    apply_clahe=True,\n",
    "    clahe_clip_limit=2.0,\n",
    "    clahe_tile_grid_size=(8, 8)\n",
    ")\n",
    "\n",
    "# Apply enhancement to pre-image\n",
    "pre_enhanced = preprocessor.apply_clahe_enhancement(sample_data['pre_image'])\n",
    "post_enhanced = preprocessor.apply_clahe_enhancement(sample_data['post_image'])\n",
    "\n",
    "# Compare original vs enhanced\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Pre-event comparison\n",
    "axes[0, 0].imshow(sample_data['pre_image'])\n",
    "axes[0, 0].set_title('Pre-Event Original', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(pre_enhanced)\n",
    "axes[0, 1].set_title('Pre-Event Enhanced (CLAHE)', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Histogram comparison for pre-event\n",
    "for i in range(3):\n",
    "    axes[0, 2].hist(sample_data['pre_image'][:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Orig')\n",
    "    axes[0, 2].hist(pre_enhanced[:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Enh', linestyle='--')\n",
    "axes[0, 2].set_title('Pre-Event Histogram', fontsize=12)\n",
    "axes[0, 2].set_xlabel('Intensity')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend(fontsize=8)\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# Post-event comparison\n",
    "axes[1, 0].imshow(sample_data['post_image'])\n",
    "axes[1, 0].set_title('Post-Event Original', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(post_enhanced)\n",
    "axes[1, 1].set_title('Post-Event Enhanced (CLAHE)', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Histogram comparison for post-event\n",
    "for i in range(3):\n",
    "    axes[1, 2].hist(sample_data['post_image'][:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Orig')\n",
    "    axes[1, 2].hist(post_enhanced[:, :, i].flatten(), bins=50, alpha=0.5, label=f'Ch{i} Enh', linestyle='--')\n",
    "axes[1, 2].set_title('Post-Event Histogram', fontsize=12)\n",
    "axes[1, 2].set_xlabel('Intensity')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].legend(fontsize=8)\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCLAHE Enhancement Applied:\")\n",
    "print(\"  - Improves local contrast\")\n",
    "print(\"  - Better visibility of flood boundaries\")\n",
    "print(\"  - Histogram equalization in tiles (8x8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f3e9d",
   "metadata": {},
   "source": [
    "## 5. Advanced Preprocessing: Cloud Removal, Deblurring & Geometric Correction\n",
    "\n",
    "Apply advanced preprocessing techniques to handle common satellite imagery issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced image processing libraries\n",
    "import sys\n",
    "try:\n",
    "    from skimage import morphology, filters, exposure, restoration, transform\n",
    "    from skimage.filters import rank, gaussian\n",
    "    from skimage.morphology import disk, remove_small_objects, remove_small_holes\n",
    "    from scipy import ndimage\n",
    "    from scipy.signal import convolve2d\n",
    "    print(\"✓ Advanced image processing libraries loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Installing required libraries: {e}\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"scikit-image\", \"scipy\"])\n",
    "    from skimage import morphology, filters, exposure, restoration, transform\n",
    "    from skimage.filters import rank, gaussian\n",
    "    from skimage.morphology import disk, remove_small_objects, remove_small_holes\n",
    "    from scipy import ndimage\n",
    "    from scipy.signal import convolve2d\n",
    "    print(\"✓ Libraries installed and loaded\")\n",
    "\n",
    "print(\"\\nAdvanced preprocessing methods available:\")\n",
    "print(\"  1. Multi-stage cloud detection (brightness + texture + saturation)\")\n",
    "print(\"  2. Morphological cloud refinement\")\n",
    "print(\"  3. Advanced inpainting (Navier-Stokes + Telea)\")\n",
    "print(\"  4. Wiener deconvolution for deblurring\")\n",
    "print(\"  5. Richardson-Lucy deconvolution\")\n",
    "print(\"  6. Unsharp masking with adaptive strength\")\n",
    "print(\"  7. CLAHE enhancement per channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_degraded_image(clean_image):\n",
    "    \"\"\"\n",
    "    Create a heavily degraded version to demonstrate preprocessing capabilities\n",
    "    Adds realistic clouds, haze, and blur\n",
    "    \"\"\"\n",
    "    degraded = clean_image.copy()\n",
    "    h, w = degraded.shape[:2]\n",
    "    \n",
    "    # 1. Add atmospheric haze (reduces contrast and adds blue tint)\n",
    "    haze_strength = 0.5\n",
    "    haze_color = np.array([0.7, 0.75, 0.85])  # Blueish-white\n",
    "    degraded = degraded * (1 - haze_strength) + haze_color * haze_strength\n",
    "    \n",
    "    # 2. Add realistic cloud patches\n",
    "    num_clouds = np.random.randint(8, 15)\n",
    "    for _ in range(num_clouds):\n",
    "        # Random cloud center\n",
    "        cx, cy = np.random.randint(0, w), np.random.randint(0, h)\n",
    "        \n",
    "        # Cloud size\n",
    "        cloud_w = np.random.randint(80, 200)\n",
    "        cloud_h = np.random.randint(60, 150)\n",
    "        \n",
    "        # Create cloud mask with soft edges (Gaussian falloff)\n",
    "        y_coords, x_coords = np.ogrid[:h, :w]\n",
    "        cloud_mask = np.exp(-((x_coords - cx)**2 / (2 * cloud_w**2) + \n",
    "                             (y_coords - cy)**2 / (2 * cloud_h**2)))\n",
    "        \n",
    "        # Cloud color (bright white with slight variation)\n",
    "        cloud_color = np.array([0.85, 0.88, 0.95]) + np.random.uniform(-0.05, 0.05, 3)\n",
    "        cloud_opacity = np.random.uniform(0.5, 0.9)\n",
    "        \n",
    "        # Blend cloud\n",
    "        for c in range(3):\n",
    "            degraded[:, :, c] = (degraded[:, :, c] * (1 - cloud_mask * cloud_opacity) + \n",
    "                                cloud_color[c] * cloud_mask * cloud_opacity)\n",
    "    \n",
    "    # 3. Add motion blur (simulating camera/satellite motion)\n",
    "    kernel_size = 15\n",
    "    motion_kernel = np.zeros((kernel_size, kernel_size))\n",
    "    motion_kernel[kernel_size // 2, :] = 1.0 / kernel_size\n",
    "    \n",
    "    blurred = np.zeros_like(degraded)\n",
    "    for c in range(3):\n",
    "        blurred[:, :, c] = convolve2d(degraded[:, :, c], motion_kernel, mode='same', boundary='symm')\n",
    "    degraded = blurred\n",
    "    \n",
    "    # 4. Add Gaussian noise\n",
    "    noise = np.random.normal(0, 0.03, degraded.shape)\n",
    "    degraded = degraded + noise\n",
    "    \n",
    "    # 5. Reduce overall sharpness\n",
    "    degraded = gaussian(degraded, sigma=1.5, channel_axis=2)\n",
    "    \n",
    "    return np.clip(degraded, 0, 1)\n",
    "\n",
    "\n",
    "def advanced_cloud_removal(image, aggressive=True):\n",
    "    \"\"\"\n",
    "    State-of-the-art cloud detection and removal\n",
    "    \"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    h, w = img_uint8.shape[:2]\n",
    "    \n",
    "    # === MULTI-STAGE CLOUD DETECTION ===\n",
    "    \n",
    "    # Stage 1: Brightness analysis\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    if aggressive:\n",
    "        bright_mask = gray > 160  # Lower threshold for more detection\n",
    "    else:\n",
    "        bright_mask = gray > 180\n",
    "    \n",
    "    # Stage 2: Blue channel analysis (clouds are blue-white)\n",
    "    blue_excess = img_uint8[:, :, 2].astype(float) - (img_uint8[:, :, 0].astype(float) + img_uint8[:, :, 1].astype(float)) / 2\n",
    "    blue_mask = blue_excess > 10\n",
    "    \n",
    "    # Stage 3: Texture analysis (clouds have uniform texture)\n",
    "    selem = disk(7)\n",
    "    entropy_img = rank.entropy(gray, selem)\n",
    "    texture_mask = entropy_img < np.percentile(entropy_img, 25)\n",
    "    \n",
    "    # Stage 4: Saturation analysis (clouds have low saturation)\n",
    "    hsv = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2HSV)\n",
    "    low_sat_mask = hsv[:, :, 1] < 40\n",
    "    \n",
    "    # Stage 5: Value analysis (clouds are bright in HSV)\n",
    "    high_value_mask = hsv[:, :, 2] > 200\n",
    "    \n",
    "    # Combine all stages\n",
    "    cloud_mask = (bright_mask & blue_mask) | (bright_mask & low_sat_mask & texture_mask) | (high_value_mask & low_sat_mask)\n",
    "    cloud_mask = cloud_mask.astype(np.uint8) * 255\n",
    "    \n",
    "    # === MORPHOLOGICAL REFINEMENT ===\n",
    "    \n",
    "    # Remove small false positives\n",
    "    cloud_mask_binary = cloud_mask > 0\n",
    "    cloud_mask_binary = remove_small_objects(cloud_mask_binary, min_size=100, connectivity=2)\n",
    "    cloud_mask_binary = remove_small_holes(cloud_mask_binary, area_threshold=200)\n",
    "    \n",
    "    # Dilate to ensure full cloud coverage\n",
    "    selem_dilate = disk(5 if aggressive else 3)\n",
    "    cloud_mask_binary = morphology.dilation(cloud_mask_binary, selem_dilate)\n",
    "    \n",
    "    cloud_mask_final = (cloud_mask_binary * 255).astype(np.uint8)\n",
    "    \n",
    "    # === ADVANCED INPAINTING ===\n",
    "    \n",
    "    if np.sum(cloud_mask_final > 0) > 200:\n",
    "        # Method 1: Navier-Stokes (better for texture preservation)\n",
    "        inpainted_ns = cv2.inpaint(img_uint8, cloud_mask_final, 10, cv2.INPAINT_NS)\n",
    "        \n",
    "        # Method 2: Fast Marching (better for structure)\n",
    "        inpainted_fm = cv2.inpaint(img_uint8, cloud_mask_final, 7, cv2.INPAINT_TELEA)\n",
    "        \n",
    "        # Blend both methods\n",
    "        result = cv2.addWeighted(inpainted_ns, 0.6, inpainted_fm, 0.4, 0)\n",
    "        \n",
    "        # Apply bilateral filter for smooth transitions\n",
    "        result = cv2.bilateralFilter(result, 7, 75, 75)\n",
    "    else:\n",
    "        result = img_uint8\n",
    "    \n",
    "    return result.astype(np.float32) / 255.0, cloud_mask_final.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def advanced_deblurring(image, strength='high'):\n",
    "    \"\"\"\n",
    "    Advanced deblurring using multiple state-of-the-art methods\n",
    "    \"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # === METHOD 1: WIENER DECONVOLUTION ===\n",
    "    try:\n",
    "        # Create motion blur PSF\n",
    "        kernel_size = 11\n",
    "        psf = np.zeros((kernel_size, kernel_size))\n",
    "        psf[kernel_size // 2, :] = 1.0\n",
    "        psf = psf / psf.sum()\n",
    "        \n",
    "        # Apply Wiener deconvolution\n",
    "        deconvolved = np.zeros_like(image)\n",
    "        for c in range(3):\n",
    "            deconv_channel = restoration.wiener(image[:, :, c], psf, balance=0.05)\n",
    "            deconvolved[:, :, c] = np.clip(deconv_channel, 0, 1)\n",
    "        \n",
    "        deconv_uint8 = (deconvolved * 255).astype(np.uint8)\n",
    "    except:\n",
    "        deconv_uint8 = img_uint8\n",
    "    \n",
    "    # === METHOD 2: RICHARDSON-LUCY DECONVOLUTION ===\n",
    "    try:\n",
    "        rl_deconvolved = np.zeros_like(image)\n",
    "        for c in range(3):\n",
    "            rl_channel = restoration.richardson_lucy(image[:, :, c], psf, num_iter=15)\n",
    "            rl_deconvolved[:, :, c] = np.clip(rl_channel, 0, 1)\n",
    "        \n",
    "        rl_uint8 = (rl_deconvolved * 255).astype(np.uint8)\n",
    "    except:\n",
    "        rl_uint8 = img_uint8\n",
    "    \n",
    "    # === METHOD 3: ENHANCED UNSHARP MASKING ===\n",
    "    gaussian_blur = cv2.GaussianBlur(img_uint8, (9, 9), 2.0)\n",
    "    if strength == 'high':\n",
    "        unsharp = cv2.addWeighted(img_uint8, 2.5, gaussian_blur, -1.5, 0)\n",
    "    else:\n",
    "        unsharp = cv2.addWeighted(img_uint8, 2.0, gaussian_blur, -1.0, 0)\n",
    "    \n",
    "    # === METHOD 4: EDGE ENHANCEMENT ===\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Sobel edge detection\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    edges = np.sqrt(sobelx**2 + sobely**2)\n",
    "    edges = np.clip(edges, 0, 255).astype(np.uint8)\n",
    "    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    edge_enhanced = cv2.addWeighted(img_uint8, 1.0, edges_colored, 0.4, 0)\n",
    "    \n",
    "    # === METHOD 5: ADAPTIVE CLAHE ===\n",
    "    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "    clahe_enhanced = np.zeros_like(img_uint8)\n",
    "    for c in range(3):\n",
    "        clahe_enhanced[:, :, c] = clahe.apply(img_uint8[:, :, c])\n",
    "    \n",
    "    # === BLEND ALL METHODS ===\n",
    "    # Wiener (25%) + RL (20%) + Unsharp (30%) + Edge (15%) + CLAHE (10%)\n",
    "    result = cv2.addWeighted(deconv_uint8, 0.25, rl_uint8, 0.20, 0)\n",
    "    result = cv2.addWeighted(result, 1.0, unsharp, 0.30, 0)\n",
    "    result = cv2.addWeighted(result, 1.0, edge_enhanced, 0.15, 0)\n",
    "    result = cv2.addWeighted(result, 0.9, clahe_enhanced, 0.10, 0)\n",
    "    \n",
    "    # === FINAL SHARPENING ===\n",
    "    laplacian = cv2.Laplacian(cv2.cvtColor(result, cv2.COLOR_RGB2GRAY), cv2.CV_64F, ksize=3)\n",
    "    laplacian_colored = cv2.cvtColor(np.abs(laplacian).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "    result = cv2.addWeighted(result, 1.0, laplacian_colored, 0.20, 0)\n",
    "    \n",
    "    return np.clip(result.astype(np.float32) / 255.0, 0, 1)\n",
    "\n",
    "\n",
    "def calculate_quality_metrics(image):\n",
    "    \"\"\"Calculate image quality metrics\"\"\"\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    gray = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Sharpness (Laplacian variance)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    \n",
    "    # Contrast (standard deviation)\n",
    "    contrast = np.std(image)\n",
    "    \n",
    "    # Brightness\n",
    "    brightness = np.mean(image)\n",
    "    \n",
    "    # Edge density\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = np.sum(edges > 0) / edges.size\n",
    "    \n",
    "    return {\n",
    "        'sharpness': laplacian_var,\n",
    "        'contrast': contrast,\n",
    "        'brightness': brightness,\n",
    "        'edge_density': edge_density\n",
    "    }\n",
    "\n",
    "print(\"✓ Advanced preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply advanced preprocessing to both pre-event and post-event images\n",
    "print(\"Applying advanced preprocessing to BOTH pre-event and post-event images...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pre-event processing\n",
    "print(\"\\n[PRE-EVENT IMAGE]\")\n",
    "pre_degraded = create_synthetic_degraded_image(sample_data['pre_image'])\n",
    "pre_cloud_removed, pre_cloud_mask = advanced_cloud_removal(pre_degraded, aggressive=True)\n",
    "pre_enhanced = advanced_deblurring(pre_cloud_removed, strength='high')\n",
    "pre_cloud_cov = np.mean(pre_cloud_mask) * 100\n",
    "\n",
    "# Calculate metrics\n",
    "pre_orig_metrics = calculate_quality_metrics(sample_data['pre_image'])\n",
    "pre_deg_metrics = calculate_quality_metrics(pre_degraded)\n",
    "pre_enh_metrics = calculate_quality_metrics(pre_enhanced)\n",
    "\n",
    "print(f\"  Cloud coverage detected: {pre_cloud_cov:.1f}%\")\n",
    "print(f\"  Sharpness improvement: {((pre_enh_metrics['sharpness']/pre_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"  Contrast improvement:  {((pre_enh_metrics['contrast']/pre_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "\n",
    "# Post-event processing\n",
    "print(\"\\n[POST-EVENT IMAGE]\")\n",
    "post_degraded = create_synthetic_degraded_image(sample_data['post_image'])\n",
    "post_cloud_removed, post_cloud_mask = advanced_cloud_removal(post_degraded, aggressive=True)\n",
    "post_enhanced = advanced_deblurring(post_cloud_removed, strength='high')\n",
    "post_cloud_cov = np.mean(post_cloud_mask) * 100\n",
    "\n",
    "# Calculate metrics\n",
    "post_orig_metrics = calculate_quality_metrics(sample_data['post_image'])\n",
    "post_deg_metrics = calculate_quality_metrics(post_degraded)\n",
    "post_enh_metrics = calculate_quality_metrics(post_enhanced)\n",
    "\n",
    "print(f\"  Cloud coverage detected: {post_cloud_cov:.1f}%\")\n",
    "print(f\"  Sharpness improvement: {((post_enh_metrics['sharpness']/post_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"  Contrast improvement:  {((post_enh_metrics['contrast']/post_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Preprocessing complete for both images\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization: Pre-event & Post-event (Original -> Degraded -> Enhanced)\n",
    "fig, axes = plt.subplots(6, 3, figsize=(18, 36))\n",
    "\n",
    "# Column headers\n",
    "axes[0, 0].text(0.5, 0.5, 'ORIGINAL\\n(Clean)', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='green',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].text(0.5, 0.5, 'DEGRADED\\n(Clouds + Blur)', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='red',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].text(0.5, 0.5, 'ENHANCED\\n(Preprocessed)', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='darkgreen',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# === PRE-EVENT IMAGE ===\n",
    "\n",
    "# Row 1: Pre-Event images\n",
    "axes[1, 0].imshow(sample_data['pre_image'])\n",
    "axes[1, 0].set_title('Pre-Event Original\\n(Clean Baseline)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(pre_degraded)\n",
    "axes[1, 1].set_title(f'Pre-Event Degraded\\n(Sharpness: {pre_deg_metrics[\"sharpness\"]:.1f})', \n",
    "                     fontsize=13, color='darkred', fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(pre_enhanced)\n",
    "axes[1, 2].set_title(f'Pre-Event Enhanced\\n(Sharpness: {pre_enh_metrics[\"sharpness\"]:.1f} | +{((pre_enh_metrics[\"sharpness\"]/pre_deg_metrics[\"sharpness\"]-1)*100):.0f}%)', \n",
    "                     fontsize=13, color='darkgreen', fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Row 2: Pre-Event cloud masks and differences\n",
    "axes[2, 0].text(0.5, 0.5, f\"Original Metrics:\\nSharpness: {pre_orig_metrics['sharpness']:.1f}\\nContrast: {pre_orig_metrics['contrast']:.3f}\\nEdge Density: {pre_orig_metrics['edge_density']:.3f}\",\n",
    "                ha='center', va='center', fontsize=11, family='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "axes[2, 1].imshow(pre_cloud_mask, cmap='Reds', vmin=0, vmax=1)\n",
    "axes[2, 1].set_title(f'Cloud Mask\\n({pre_cloud_cov:.1f}% coverage)', fontsize=12, color='red')\n",
    "axes[2, 1].axis('off')\n",
    "\n",
    "diff_pre = np.abs(pre_degraded - pre_enhanced)\n",
    "axes[2, 2].imshow(diff_pre)\n",
    "axes[2, 2].set_title('Preprocessing Changes\\n(Difference Map)', fontsize=12)\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "# === POST-EVENT IMAGE ===\n",
    "\n",
    "# Row 3: Separator\n",
    "for col in range(3):\n",
    "    axes[3, col].text(0.5, 0.5, '━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━', \n",
    "                      ha='center', va='center', fontsize=14, color='gray')\n",
    "    axes[3, col].axis('off')\n",
    "\n",
    "# Row 4: Post-Event images\n",
    "axes[4, 0].imshow(sample_data['post_image'])\n",
    "axes[4, 0].set_title('Post-Event Original\\n(Clean Baseline)', fontsize=13, fontweight='bold')\n",
    "axes[4, 0].axis('off')\n",
    "\n",
    "axes[4, 1].imshow(post_degraded)\n",
    "axes[4, 1].set_title(f'Post-Event Degraded\\n(Sharpness: {post_deg_metrics[\"sharpness\"]:.1f})', \n",
    "                     fontsize=13, color='darkred', fontweight='bold')\n",
    "axes[4, 1].axis('off')\n",
    "\n",
    "axes[4, 2].imshow(post_enhanced)\n",
    "axes[4, 2].set_title(f'Post-Event Enhanced\\n(Sharpness: {post_enh_metrics[\"sharpness\"]:.1f} | +{((post_enh_metrics[\"sharpness\"]/post_deg_metrics[\"sharpness\"]-1)*100):.0f}%)', \n",
    "                     fontsize=13, color='darkgreen', fontweight='bold')\n",
    "axes[4, 2].axis('off')\n",
    "\n",
    "# Row 5: Post-Event cloud masks and differences\n",
    "axes[5, 0].text(0.5, 0.5, f\"Original Metrics:\\nSharpness: {post_orig_metrics['sharpness']:.1f}\\nContrast: {post_orig_metrics['contrast']:.3f}\\nEdge Density: {post_orig_metrics['edge_density']:.3f}\",\n",
    "                ha='center', va='center', fontsize=11, family='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "axes[5, 0].axis('off')\n",
    "\n",
    "axes[5, 1].imshow(post_cloud_mask, cmap='Reds', vmin=0, vmax=1)\n",
    "axes[5, 1].set_title(f'Cloud Mask\\n({post_cloud_cov:.1f}% coverage)', fontsize=12, color='red')\n",
    "axes[5, 1].axis('off')\n",
    "\n",
    "diff_post = np.abs(post_degraded - post_enhanced)\n",
    "axes[5, 2].imshow(diff_post)\n",
    "axes[5, 2].set_title('Preprocessing Changes\\n(Difference Map)', fontsize=12)\n",
    "axes[5, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Advanced Preprocessing: Cloud Removal + Deblurring with scikit-image\\nOriginal → Synthetic Degradation → Enhanced (Pre & Post Event)', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING EFFECTIVENESS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ PRE-EVENT IMAGE:\")\n",
    "print(f\"    Sharpness improvement: {((pre_enh_metrics['sharpness']/pre_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"    Contrast improvement:  {((pre_enh_metrics['contrast']/pre_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "print(f\"    Edge density improvement: {((pre_enh_metrics['edge_density']/pre_deg_metrics['edge_density']-1)*100):+.1f}%\")\n",
    "print(f\"    Cloud coverage removed: {pre_cloud_cov:.1f}%\")\n",
    "\n",
    "print(\"\\n✓ POST-EVENT IMAGE:\")\n",
    "print(f\"    Sharpness improvement: {((post_enh_metrics['sharpness']/post_deg_metrics['sharpness']-1)*100):+.1f}%\")\n",
    "print(f\"    Contrast improvement:  {((post_enh_metrics['contrast']/post_deg_metrics['contrast']-1)*100):+.1f}%\")\n",
    "print(f\"    Edge density improvement: {((post_enh_metrics['edge_density']/post_deg_metrics['edge_density']-1)*100):+.1f}%\")\n",
    "print(f\"    Cloud coverage removed: {post_cloud_cov:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TECHNIQUES APPLIED:\")\n",
    "print(\"  ✓ Multi-stage cloud detection (brightness + blue excess + texture + saturation + value)\")\n",
    "print(\"  ✓ Morphological refinement (remove_small_objects + remove_small_holes + dilation)\")\n",
    "print(\"  ✓ Dual inpainting (Navier-Stokes 60% + Telea 40%)\")\n",
    "print(\"  ✓ Wiener deconvolution (25% weight)\")\n",
    "print(\"  ✓ Richardson-Lucy deconvolution (20% weight)\")\n",
    "print(\"  ✓ Enhanced unsharp masking (30% weight, 2.5/-1.5)\")\n",
    "print(\"  ✓ Sobel edge enhancement (15% weight)\")\n",
    "print(\"  ✓ Adaptive CLAHE (10% weight, clip=4.0)\")\n",
    "print(\"  ✓ Laplacian sharpening (20% weight)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b31550",
   "metadata": {},
   "source": [
    "## 6. Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image quality\n",
    "quality_pre = preprocessor.check_image_quality(sample_data['pre_image'])\n",
    "quality_post = preprocessor.check_image_quality(sample_data['post_image'])\n",
    "\n",
    "print(\"Pre-Event Quality Metrics:\")\n",
    "for key, value in quality_pre.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nPost-Event Quality Metrics:\")\n",
    "for key, value in quality_post.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Visualize quality metrics\n",
    "metrics = ['valid_ratio', 'cloud_ratio', 'dark_ratio', 'mean_intensity', 'std_intensity']\n",
    "pre_values = [quality_pre[m] for m in metrics]\n",
    "post_values = [quality_post[m] for m in metrics]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, pre_values, width, label='Pre-Event', color='#3498db')\n",
    "ax.bar(x + width/2, post_values, width, label='Post-Event', color='#e74c3c')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Image Quality Metrics Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f0b77",
   "metadata": {},
   "source": [
    "## 7. Patch Extraction with Smart Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize patch extractor with updated threshold\n",
    "patch_extractor = PatchExtractor(\n",
    "    patch_size=PATCH_SIZE,\n",
    "    overlap=PATCH_OVERLAP,\n",
    "    min_flood_pixels=MIN_FLOOD_PIXELS  # Now 2621 pixels (~1% of patch)\n",
    ")\n",
    "\n",
    "print(f\"Patch extractor configuration:\")\n",
    "print(f\"  Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"  Overlap: {PATCH_OVERLAP}\")\n",
    "print(f\"  Min flood pixels: {MIN_FLOOD_PIXELS} ({(MIN_FLOOD_PIXELS/(PATCH_SIZE**2))*100:.2f}% of patch)\")\n",
    "\n",
    "# Concatenate pre and post images\n",
    "combined_image = np.concatenate([pre_enhanced, post_enhanced], axis=2)\n",
    "print(f\"\\nCombined image shape: {combined_image.shape} (6 channels: 3 pre + 3 post)\")\n",
    "\n",
    "# Extract patches (without oversampling for demonstration)\n",
    "patches = patch_extractor.extract_patches(\n",
    "    combined_image,\n",
    "    mask=sample_data['mask'],\n",
    "    oversample_flood=False  # Disabled to show true distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(patches)} patches\")\n",
    "\n",
    "# Count flood-positive patches\n",
    "flood_positive = [p for p in patches if p['is_flood_positive']]\n",
    "print(f\"  Flood-positive patches: {len(flood_positive)}\")\n",
    "print(f\"  Non-flood patches: {len(patches) - len(flood_positive)}\")\n",
    "print(f\"  Flood ratio: {len(flood_positive)/len(patches)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6371619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mask shape:\", sample_data['mask'].shape)\n",
    "print(\"Unique classes in mask:\", np.unique(sample_data['mask']))\n",
    "print(\"Mask value counts:\")\n",
    "unique, counts = np.unique(sample_data['mask'], return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    pct = (count / sample_data['mask'].size) * 100\n",
    "    print(f\"  Class {cls} ({CLASS_NAMES.get(cls, 'unknown')}): {count:,} pixels ({pct:.2f}%)\")\n",
    "\n",
    "# Check if mask has any flood-related classes (2, 3, 4, 5)\n",
    "flood_related_pixels = np.sum(sample_data['mask'] > 1)\n",
    "print(f\"\\nTotal flood-related pixels (class > 1): {flood_related_pixels:,}\")\n",
    "print(f\"Percentage: {(flood_related_pixels / sample_data['mask'].size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading different tiles to find one with mixed flood/non-flood patches\n",
    "print(\"Testing different tiles to find varied flood distribution:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx in range(min(5, len(germany_loader.get_tile_list()))):\n",
    "    tile_name = germany_loader.get_tile_list()[idx]\n",
    "    tile_data = load_tile_data(GERMANY_TRAIN, tile_name, 'Germany')\n",
    "    \n",
    "    # Calculate flood percentage\n",
    "    flood_px = np.sum((tile_data['mask'] == 2) | (tile_data['mask'] == 3) | (tile_data['mask'] == 4))\n",
    "    total_px = tile_data['mask'].size\n",
    "    flood_pct = (flood_px / total_px) * 100\n",
    "    \n",
    "    print(f\"\\nTile {idx}: {tile_name}\")\n",
    "    print(f\"  Flood pixels: {flood_px:,} ({flood_pct:.2f}%)\")\n",
    "    print(f\"  Classes present: {np.unique(tile_data['mask'])}\")\n",
    "    \n",
    "    # If this tile has moderate flooding (2-15%), use it for demo\n",
    "    if 2.0 <= flood_pct <= 15.0:\n",
    "        print(f\" Good candidate for mixed flood/non-flood patches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673aeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing patch extraction with different thresholds:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for threshold in [100, 2621, 5000, 10000, 20000]:\n",
    "    test_extractor = PatchExtractor(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        overlap=PATCH_OVERLAP,\n",
    "        min_flood_pixels=threshold\n",
    "    )\n",
    "    \n",
    "    test_patches = test_extractor.extract_patches(\n",
    "        combined_image,\n",
    "        mask=sample_data['mask'],\n",
    "        oversample_flood=False  # Disable oversampling for clarity\n",
    "    )\n",
    "    \n",
    "    flood_count = sum(1 for p in test_patches if p['is_flood_positive'])\n",
    "    non_flood_count = len(test_patches) - flood_count\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold} pixels ({(threshold/(PATCH_SIZE**2))*100:.2f}% of patch)\")\n",
    "    print(f\"  Total patches: {len(test_patches)}\")\n",
    "    print(f\"  Flood-positive: {flood_count}\")\n",
    "    print(f\"  Non-flood: {non_flood_count}\")\n",
    "    print(f\"  Flood ratio: {(flood_count/len(test_patches))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124835c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPatch-level flood analysis:\")\n",
    "for i, patch in enumerate(patches):\n",
    "    flood_px = patch['flood_pixels']\n",
    "    total_px = PATCH_SIZE * PATCH_SIZE\n",
    "    flood_pct = (flood_px / total_px) * 100\n",
    "    print(f\"  Patch {i}: {flood_px} flood pixels ({flood_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nPatch size: {PATCH_SIZE}x{PATCH_SIZE} = {PATCH_SIZE*PATCH_SIZE:,} pixels\")\n",
    "print(f\"Min flood pixels threshold: {patch_extractor.min_flood_pixels}\")\n",
    "print(f\"Min flood percentage needed: {(patch_extractor.min_flood_pixels / (PATCH_SIZE*PATCH_SIZE)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample patches\n",
    "n_samples = min(8, len(patches))\n",
    "sample_patches = np.random.choice(patches, n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(4, n_samples, figsize=(n_samples*3, 12))\n",
    "\n",
    "for i, patch in enumerate(sample_patches):\n",
    "    # Pre-event (first 3 channels)\n",
    "    pre_patch = patch['image'][:, :, :3]\n",
    "    axes[0, i].imshow(pre_patch)\n",
    "    axes[0, i].set_title(f\"Patch {i}\\nPre-Event\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Post-event (last 3 channels)\n",
    "    post_patch = patch['image'][:, :, 3:6]\n",
    "    axes[1, i].imshow(post_patch)\n",
    "    axes[1, i].set_title('Post-Event', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Mask\n",
    "    axes[2, i].imshow(patch['mask'], cmap='tab10', vmin=0, vmax=5)\n",
    "    axes[2, i].set_title('Mask', fontsize=10)\n",
    "    axes[2, i].axis('off')\n",
    "    \n",
    "    # Difference\n",
    "    diff_patch = np.abs(post_patch - pre_patch)\n",
    "    axes[3, i].imshow(diff_patch)\n",
    "    flood_status = 'FLOOD' if patch['is_flood_positive'] else 'OK'\n",
    "    axes[3, i].set_title(f\"Difference\\n{flood_status}\", fontsize=10)\n",
    "    axes[3, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feef5e5",
   "metadata": {},
   "source": [
    "## 8. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3743bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution across all patches\n",
    "class_totals = {i: 0 for i in range(7)}  # Classes 0-6\n",
    "\n",
    "for patch in patches:\n",
    "    for cls, count in patch.get('class_distribution', {}).items():\n",
    "        class_totals[int(cls)] += count\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "classes = list(class_totals.keys())\n",
    "counts = list(class_totals.values())\n",
    "labels = [f\"{CLASS_NAMES.get(c, f'Class {c}')}\\n{counts[i]:,}\" for i, c in enumerate(classes)]\n",
    "\n",
    "# Convert RGB colors (0-255) to hex format, normalizing to 0-1 range\n",
    "colors = []\n",
    "for cls in classes:\n",
    "    rgb = CLASS_COLORS.get(cls, [128, 128, 128])\n",
    "    # Normalize RGB values from 0-255 to 0-1\n",
    "    r, g, b = [val/255.0 for val in rgb]\n",
    "    colors.append((r, g, b))\n",
    "\n",
    "axes[0].pie(counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Class Distribution (Pixel Count)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "axes[1].bar(classes, counts, color=colors)\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Pixel Count', fontsize=12)\n",
    "axes[1].set_title('Class Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(classes)\n",
    "axes[1].set_xticklabels([CLASS_NAMES.get(c, f'C{c}') for c in classes], rotation=45, ha='right')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add counts on bars\n",
    "for i, (cls, count) in enumerate(zip(classes, counts)):\n",
    "    axes[1].text(cls, count, f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass Imbalance Summary:\")\n",
    "total_pixels = sum(counts)\n",
    "for cls, count in zip(classes, counts):\n",
    "    pct = (count / total_pixels) * 100\n",
    "    print(f\"  {CLASS_NAMES.get(cls, f'Class {cls}')}: {count:,} pixels ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12897b97",
   "metadata": {},
   "source": [
    "## 9. Data Augmentation Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training augmentation\n",
    "train_aug = get_training_augmentation(image_size=PATCH_SIZE)\n",
    "\n",
    "# Select a flood-positive patch for demonstration\n",
    "demo_patch = flood_positive[0] if len(flood_positive) > 0 else patches[0]\n",
    "demo_image = demo_patch['image'][:, :, :3]  # Use pre-event for demo\n",
    "demo_mask = demo_patch['mask']\n",
    "\n",
    "# Apply augmentation multiple times\n",
    "n_aug = 6\n",
    "fig, axes = plt.subplots(2, n_aug, figsize=(n_aug*3, 6))\n",
    "\n",
    "for i in range(n_aug):\n",
    "    augmented = train_aug(image=demo_image, mask=demo_mask)\n",
    "    \n",
    "    axes[0, i].imshow(augmented['image'])\n",
    "    axes[0, i].set_title(f'Aug {i+1} - Image', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(augmented['mask'], cmap='tab10', vmin=0, vmax=5)\n",
    "    axes[1, i].set_title(f'Aug {i+1} - Mask', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAugmentations include:\")\n",
    "print(\"  - Horizontal/Vertical flips\")\n",
    "print(\"  - Random rotations (90 degrees)\")\n",
    "print(\"  - Shift, scale, rotate\")\n",
    "print(\"  - Brightness/contrast adjustments\")\n",
    "print(\"  - Gaussian noise and blur\")\n",
    "print(\"  - Random fog and shadows\")\n",
    "print(\"  - Grid dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6f2a8",
   "metadata": {},
   "source": [
    "## 10. Run Full Preprocessing Pipeline\n",
    "\n",
    "This section runs the complete preprocessing pipeline on both regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ebe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing script\n",
    "import subprocess\n",
    "\n",
    "print(\"Starting full preprocessing pipeline...\")\n",
    "print(\"This may take 30-60 minutes depending on dataset size.\")\n",
    "print(\"\\nProcessing:\")\n",
    "print(\"  1. Load all tiles from Germany and Louisiana-East\")\n",
    "print(\"  2. Apply quality checks\")\n",
    "print(\"  3. Apply CLAHE enhancement\")\n",
    "print(\"  4. Extract patches with smart sampling\")\n",
    "print(\"  5. Oversample flood-positive patches\")\n",
    "print(\"  6. Create geo-stratified train/val/test splits\")\n",
    "print(\"  7. Export to dataset/processed/\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing (uncomment to execute)\n",
    "# Note: This can be run from terminal instead for better progress tracking\n",
    "\n",
    "#%run ../src/run_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c8465",
   "metadata": {},
   "source": [
    "## 11. Validate Processed Data\n",
    "\n",
    "After preprocessing is complete, validate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if processed data exists\n",
    "if PROCESSED_TRAIN_DIR.exists():\n",
    "    print(\"Processed data directory exists\")\n",
    "    \n",
    "    # Count files\n",
    "    train_images = list((PROCESSED_TRAIN_DIR / 'images').glob('*.npy'))\n",
    "    train_masks = list((PROCESSED_TRAIN_DIR / 'masks').glob('*.npy'))\n",
    "    \n",
    "    print(f\"\\nTraining set:\")\n",
    "    print(f\"  Images: {len(train_images)}\")\n",
    "    print(f\"  Masks: {len(train_masks)}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_path = PROCESSED_TRAIN_DIR / 'metadata' / 'train_metadata.json'\n",
    "    if metadata_path.exists():\n",
    "        import json\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"  Metadata entries: {len(metadata)}\")\n",
    "        \n",
    "        # Count flood-positive\n",
    "        flood_count = sum(1 for m in metadata if m['is_flood_positive'])\n",
    "        print(f\"  Flood-positive patches: {flood_count} ({flood_count/len(metadata)*100:.1f}%)\")\n",
    "    \n",
    "    # Load and display a sample\n",
    "    if len(train_images) > 0:\n",
    "        sample_img = np.load(train_images[0])\n",
    "        sample_mask = np.load(train_masks[0])\n",
    "        \n",
    "        print(f\"\\nSample patch:\")\n",
    "        print(f\"  Image shape: {sample_img.shape}\")\n",
    "        print(f\"  Mask shape: {sample_mask.shape}\")\n",
    "        print(f\"  Image range: [{sample_img.min():.3f}, {sample_img.max():.3f}]\")\n",
    "        print(f\"  Mask classes: {np.unique(sample_mask)}\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(sample_img[:, :, :3])  # Pre-event\n",
    "        axes[0].set_title('Pre-Event (Processed)', fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(sample_img[:, :, 3:6])  # Post-event\n",
    "        axes[1].set_title('Post-Event (Processed)', fontsize=12)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(sample_mask, cmap='tab10')\n",
    "        axes[2].set_title('Mask', fontsize=12)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Processed data not found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046d219",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCompleted tasks:\")\n",
    "print(\"  1. Loaded and validated raw satellite imagery\")\n",
    "print(\"  2. Applied CLAHE enhancement for better contrast\")\n",
    "print(\"  3. Performed quality checks (valid pixels, clouds, darkness)\")\n",
    "print(\"  4. Extracted patches with controlled overlap\")\n",
    "print(\"  5. Oversampled flood-positive patches to address class imbalance\")\n",
    "print(\"  6. Created geo-stratified train/val/test splits\")\n",
    "print(\"  7. Exported processed data in .npy format\")\n",
    "print(\"\\nKey outcomes:\")\n",
    "print(\"  - Balanced dataset with 40-50% flood-positive patches\")\n",
    "print(\"  - 512x512 patches optimized for CV models\")\n",
    "print(\"  - 6-channel input (3 pre + 3 post event)\")\n",
    "print(\"  - No spatial leakage between train/val/test\")\n",
    "print(\"\\nNext steps (Phase 3):\")\n",
    "print(\"  - Implement U-Net++, DeepLabV3+, and SegFormer models\")\n",
    "print(\"  - Design PyTorch Dataset and DataLoader\")\n",
    "print(\"  - Set up training pipeline with class-weighted loss\")\n",
    "print(\"  - Configure multi-scale training and validation\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
